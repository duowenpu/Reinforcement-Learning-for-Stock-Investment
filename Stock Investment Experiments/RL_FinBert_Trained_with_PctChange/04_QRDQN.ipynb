{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "time_period = 2\n",
    "sys.path.append('./')\n",
    "from utlis import get_data, Stock_Env\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import deque\n",
    "    \n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, N, hidden=[64, 64]):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size*N)\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.N = N\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state #(batch_size, state_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #(batch_size, action_size*N)\n",
    "        x = x.view(-1, self.action_size, self.N) #(batch_size, action_size, N)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:00<00:00, 708.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 695.02it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train, stock_df_test, stock_df_train_, stock_df_test_, codes = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, N, kappa, device, visual=False):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.tau = torch.linspace(0, 1, N+1)\n",
    "        self.tau = (self.tau[:-1] + self.tau[1:]) / 2\n",
    "        self.tau_hat = self.tau.to(device).unsqueeze(0) #(1, N)\n",
    "        self.tau = tau\n",
    "        self.kappa = kappa\n",
    "\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size, N).to(self.device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size, N).to(self.device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state)\n",
    "                action_values = action_values.mean(dim=-1, keepdim=False).view(-1)\n",
    "            return np.argmax(action_values.cpu().numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        quantiles_local = self.Q_local(states) #(batch_size, action_size, N)\n",
    "        quantiles_local = torch.gather(input=quantiles_local, dim=1, index=actions.unsqueeze(1).repeat(1, 1, self.N)) #(batch_size, 1, N)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            quantiles_target = self.Q_target(next_states) #(batch_size, action_size, N)\n",
    "            next_actions = torch.max(quantiles_target.sum(dim=2, keepdim=True), dim=1, keepdim=True)[1] #(batch_size, 1, 1)\n",
    "            quantiles_target = torch.gather(input=quantiles_target, index=next_actions.repeat(1,1,self.N), dim=1) #(batch_size, 1, N)\n",
    "            quantiles_target = rewards.unsqueeze(1).repeat(1, 1, self.N) + self.gamma * (1 - dones.unsqueeze(1).repeat(1, 1, self.N)) * quantiles_target #(batch_size, 1, N)\n",
    "            quantiles_target = quantiles_target.permute(0, 2, 1) #(batch_size, N, 1)\n",
    "\n",
    "        diff = quantiles_target - quantiles_local #(batch_size, N, N)\n",
    "        tau = self.tau_hat.unsqueeze(0).repeat(diff.size(0), 1, 1) #(batch_size, 1, N)\n",
    "        loss = (tau - (diff<0).float()).abs() * self.huber(diff) #(batch_size, N, N)\n",
    "        loss = loss.mean(dim=2, keepdim=False).sum(dim=1, keepdim=False) #(batch_size,)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.tau)\n",
    "\n",
    "    def huber(self, u):\n",
    "        if self.kappa > 0:\n",
    "            flag = (u.abs()<self.kappa).float()\n",
    "            loss = 0.5*u.pow(2) * flag + self.kappa*(u.abs()-0.5*self.kappa) * (1-flag)\n",
    "        else:\n",
    "            loss = u.abs()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    state = env.reset()\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        next_state = frame\n",
    "        state = next_state.copy()\n",
    "        episodic_reward += reward\n",
    "        rewards_log.append(reward)\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "        \n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.099, Average Reward 0.049, valReward 0.167, val Average Reward 0.283, Asset 1100630.91, Validation Asset 1171086.45, Average Validation Sharpe 2.04\n",
      "Episode 200, Reward 0.065, Average Reward 0.120, valReward 0.240, val Average Reward 0.264, Asset 1065364.33, Validation Asset 1259746.33, Average Validation Sharpe 1.92\n",
      "Episode 300, Reward 0.348, Average Reward 0.171, valReward 0.271, val Average Reward 0.263, Asset 1408346.91, Validation Asset 1300960.05, Average Validation Sharpe 1.92\n",
      "Episode 400, Reward 0.234, Average Reward 0.206, valReward 0.375, val Average Reward 0.263, Asset 1260115.13, Validation Asset 1438181.53, Average Validation Sharpe 1.89\n",
      "Episode 500, Reward 0.309, Average Reward 0.203, valReward 0.319, val Average Reward 0.259, Asset 1354376.62, Validation Asset 1361442.19, Average Validation Sharpe 1.86\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.251, Average Reward 0.071, valReward 0.017, val Average Reward -0.005, Asset 1268195.41, Validation Asset 1010667.99, Average Validation Sharpe -0.04\n",
      "Episode 200, Reward 0.124, Average Reward 0.132, valReward -0.041, val Average Reward -0.003, Asset 1128047.99, Validation Asset 952824.51, Average Validation Sharpe -0.02\n",
      "Episode 300, Reward 0.349, Average Reward 0.171, valReward 0.121, val Average Reward 0.004, Asset 1404424.58, Validation Asset 1118424.69, Average Validation Sharpe 0.0401\n",
      "Episode 400, Reward 0.343, Average Reward 0.237, valReward -0.023, val Average Reward -0.002, Asset 1400198.22, Validation Asset 970512.85, Average Validation Sharpe -0.01\n",
      "Episode 500, Reward 0.423, Average Reward 0.252, valReward -0.041, val Average Reward -0.009, Asset 1515178.85, Validation Asset 949972.49, Average Validation Sharpe -0.06\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.391, Average Reward 0.103, valReward -0.002, val Average Reward -0.034, Asset 1462972.00, Validation Asset 976862.05, Average Validation Sharpe -0.17\n",
      "Episode 200, Reward 0.459, Average Reward 0.210, valReward 0.082, val Average Reward -0.012, Asset 1560189.21, Validation Asset 1068666.52, Average Validation Sharpe -0.06\n",
      "Episode 300, Reward 0.091, Average Reward 0.266, valReward 0.076, val Average Reward -0.033, Asset 1094325.39, Validation Asset 1059700.39, Average Validation Sharpe -0.17\n",
      "Episode 400, Reward 0.623, Average Reward 0.314, valReward 0.164, val Average Reward -0.017, Asset 1841010.26, Validation Asset 1162113.14, Average Validation Sharpe -0.08\n",
      "Episode 500, Reward 0.428, Average Reward 0.338, valReward -0.064, val Average Reward -0.018, Asset 1521570.17, Validation Asset 921114.42, Average Validation Sharpe -0.08\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.070, Average Reward 0.084, valReward 0.206, val Average Reward 0.154, Asset 1071182.47, Validation Asset 1215034.24, Average Validation Sharpe 1.04\n",
      "Episode 200, Reward 0.124, Average Reward 0.104, valReward 0.066, val Average Reward 0.152, Asset 1127875.86, Validation Asset 1054352.71, Average Validation Sharpe 1.03\n",
      "Episode 300, Reward 0.103, Average Reward 0.141, valReward 0.072, val Average Reward 0.157, Asset 1106626.72, Validation Asset 1063829.25, Average Validation Sharpe 1.06\n",
      "Episode 400, Reward 0.233, Average Reward 0.156, valReward 0.088, val Average Reward 0.156, Asset 1259071.79, Validation Asset 1077721.17, Average Validation Sharpe 1.03\n",
      "Episode 500, Reward 0.160, Average Reward 0.150, valReward 0.246, val Average Reward 0.147, Asset 1169736.80, Validation Asset 1259645.13, Average Validation Sharpe 1.00\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.124, Average Reward 0.037, valReward 0.154, val Average Reward 0.055, Asset 1128627.09, Validation Asset 1149300.88, Average Validation Sharpe 0.44\n",
      "Episode 200, Reward 0.104, Average Reward 0.084, valReward 0.107, val Average Reward 0.052, Asset 1107739.99, Validation Asset 1103292.88, Average Validation Sharpe 0.40\n",
      "Episode 300, Reward 0.100, Average Reward 0.106, valReward -0.033, val Average Reward 0.051, Asset 1103353.15, Validation Asset 960289.06, Average Validation Sharpe 0.41\n",
      "Episode 400, Reward 0.259, Average Reward 0.141, valReward 0.069, val Average Reward 0.060, Asset 1291606.96, Validation Asset 1064014.26, Average Validation Sharpe 0.49\n",
      "Episode 500, Reward 0.058, Average Reward 0.145, valReward 0.019, val Average Reward 0.056, Asset 1059291.61, Validation Asset 1011313.54, Average Validation Sharpe 0.44\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, codes_dict=codes_dict, train=True, code=code, time_period = time_period, codes=codes)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, codes_dict=codes_dict, train=False, code=code, time_period = time_period,  codes=codes)\n",
    "    agent = Agent(2*3, env.action_space, 64, 0.001, 0.001, 0.99, 51, 1, 'cuda', True)\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
