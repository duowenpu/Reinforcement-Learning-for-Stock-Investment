{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "time_period = 2\n",
    "sys.path.append('./')\n",
    "from utlis import get_data, Stock_Env\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, N, Vmin, Vmax, hidden=[64, 64]):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size*N)\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.N = N\n",
    "        self.values = torch.linspace(Vmin, Vmax, N).view(1, 1, -1).to('cuda') #(1, 1, N)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state #(batch_size, state_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #(batch_size, action_size*N)\n",
    "        x = x.view(-1, self.action_size, self.N) #(batch_size, action_size, N)\n",
    "        log_probs = F.log_softmax(x, dim=2) #(batch_size, action_size, N)\n",
    "        Q_values = log_probs.exp() * self.values #(batch_size, action_size, N)\n",
    "        Q_values = Q_values.sum(dim=2, keepdims=False) #(batch_size, action_size)\n",
    "\n",
    "        return log_probs, Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:00<00:00, 629.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 617.50it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train, stock_df_test, stock_df_train_, stock_df_test_, codes = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Neg</th>\n",
       "      <th>Neu</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2017-01-03</th>\n",
       "      <th>AMZN</th>\n",
       "      <td>-0.237922</td>\n",
       "      <td>0.577502</td>\n",
       "      <td>-0.533748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>-0.886930</td>\n",
       "      <td>1.145552</td>\n",
       "      <td>-0.133128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>0.317281</td>\n",
       "      <td>-1.234364</td>\n",
       "      <td>1.125079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2017-01-04</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.256717</td>\n",
       "      <td>0.481192</td>\n",
       "      <td>-0.343052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>-0.268459</td>\n",
       "      <td>0.338432</td>\n",
       "      <td>-0.097422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <th>NFLX</th>\n",
       "      <td>-0.203152</td>\n",
       "      <td>-0.033138</td>\n",
       "      <td>0.427762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2018-12-31</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.145201</td>\n",
       "      <td>0.163496</td>\n",
       "      <td>0.090220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>0.173823</td>\n",
       "      <td>-0.029492</td>\n",
       "      <td>-0.302544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>-0.112033</td>\n",
       "      <td>0.099431</td>\n",
       "      <td>0.135030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>0.292497</td>\n",
       "      <td>-0.490054</td>\n",
       "      <td>0.304973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2086 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Neg       Neu       Pos\n",
       "date       symbol                              \n",
       "2017-01-03 AMZN   -0.237922  0.577502 -0.533748\n",
       "           GOOGL  -0.886930  1.145552 -0.133128\n",
       "           NFLX    0.317281 -1.234364  1.125079\n",
       "2017-01-04 AAPL   -0.256717  0.481192 -0.343052\n",
       "           AMZN   -0.268459  0.338432 -0.097422\n",
       "...                     ...       ...       ...\n",
       "2018-12-28 NFLX   -0.203152 -0.033138  0.427762\n",
       "2018-12-31 AAPL   -0.145201  0.163496  0.090220\n",
       "           AMZN    0.173823 -0.029492 -0.302544\n",
       "           GOOGL  -0.112033  0.099431  0.135030\n",
       "           NFLX    0.292497 -0.490054  0.304973\n",
       "\n",
       "[2086 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df_train[['Neg','Neu','Pos']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from networks import *\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, N, Vmin, Vmax, device, visual=False, personality=1):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.Vmin = Vmin\n",
    "        self.Vmax = Vmax\n",
    "        self.vals = torch.linspace(Vmin, Vmax, N).to(device)\n",
    "        self.unit = (Vmax - Vmin) / (N - 1)\n",
    "        self.personality=personality\n",
    "\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size, N, Vmin, Vmax).to(self.device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size, N, Vmin, Vmax).to(self.device)\n",
    "\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                _, action_values = self.Q_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "        rewards[rewards<0] = rewards[rewards<0]*self.personality\n",
    "        # print(states)\n",
    "        # print(self.Q_local)\n",
    "        log_probs, _ = self.Q_local(states) #(batch_size, action_size, N)\n",
    "        log_probs = torch.gather(input=log_probs, dim=1, index=actions.unsqueeze(1).repeat(1, 1, self.N)) #(batch_size, 1, N)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            log_probs_targets, Q_targets = self.Q_target(next_states)\n",
    "            _, actions_target = torch.max(input=Q_targets, dim=1, keepdim=True)#(batch_size, 1) the same size as actions\n",
    "            log_probs_targets = torch.gather(input=log_probs_targets, dim=1, index=actions_target.unsqueeze(1).repeat(1, 1, self.N))\n",
    "            target_distribution = self.update_distribution(log_probs_targets.exp(), rewards, dones) #(batch_size, 1, N)\n",
    "\n",
    "        loss = -target_distribution*log_probs #D_KL(target||local)\n",
    "        #loss = -log_probs.exp()*((target_distribution+1e-9).log() - log_probs) #D_KL(local||target)\n",
    "\n",
    "        loss = loss.sum(dim=2, keepdims=False).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_distribution(self, old_distribution, reward, dones):\n",
    "        with torch.no_grad():\n",
    "            reward = reward.view(-1, 1)\n",
    "            batch_size = reward.size(0)\n",
    "            assert old_distribution.size(0) == batch_size\n",
    "            new_vals = self.vals.view(1, -1) * self.gamma * (1-dones) + reward\n",
    "            new_vals = torch.clamp(new_vals, self.Vmin, self.Vmax)\n",
    "            lower = torch.floor((new_vals - self.Vmin) / self.unit).long().to(self.device)\n",
    "            upper = torch.min(lower + 1, other=torch.tensor(self.N - 1)).to(self.device)\n",
    "            lower_vals = self.vals[lower]\n",
    "            lower_probs = 1 - torch.min((new_vals - lower_vals) / self.unit, other=torch.tensor(1, dtype=torch.float32)).to(self.device)\n",
    "            transit = torch.zeros((batch_size, self.N, self.N)).to(self.device)\n",
    "            first_dim = torch.tensor(range(batch_size), dtype=torch.long).view(-1, 1).repeat(1, self.N).view(-1).to(self.device)\n",
    "            second_dim = torch.tensor(range(self.N), dtype=torch.long).repeat(batch_size).to(self.device)\n",
    "            transit[first_dim, second_dim, lower.view(-1)] += lower_probs.view(-1)\n",
    "            transit[first_dim, second_dim, upper.view(-1)] += 1 - lower_probs.view(-1)\n",
    "            if len(old_distribution.size()) == 2:\n",
    "                old_distribution = old_distribution.unsqueeze(1)\n",
    "            return torch.bmm(old_distribution, transit)\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    state = env.reset()\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        next_state = frame\n",
    "        state = next_state.copy()\n",
    "        episodic_reward += reward\n",
    "        rewards_log.append(reward)\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "        \n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes_dict = dict(zip(codes, range(len(codes))))\n",
    "# train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward -0.002, Average Reward 0.051, valReward 0.287, val Average Reward 0.270, Asset 997758.59, Validation Asset 1321508.30, Average Validation Sharpe 1.94\n",
      "Episode 200, Reward 0.310, Average Reward 0.135, valReward 0.305, val Average Reward 0.273, Asset 1357132.59, Validation Asset 1342672.82, Average Validation Sharpe 1.99\n",
      "Episode 300, Reward 0.358, Average Reward 0.216, valReward 0.275, val Average Reward 0.274, Asset 1422417.67, Validation Asset 1305330.45, Average Validation Sharpe 1.99\n",
      "Episode 400, Reward -0.006, Average Reward 0.241, valReward 0.328, val Average Reward 0.266, Asset 993389.40, Validation Asset 1372749.99, Average Validation Sharpe 1.95\n",
      "Episode 500, Reward 0.178, Average Reward 0.254, valReward 0.184, val Average Reward 0.269, Asset 1191808.66, Validation Asset 1190065.31, Average Validation Sharpe 1.97\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.013, Average Reward 0.094, valReward 0.019, val Average Reward 0.003, Asset 1013033.61, Validation Asset 1011137.85, Average Validation Sharpe 0.02\n",
      "Episode 200, Reward 0.185, Average Reward 0.203, valReward -0.072, val Average Reward -0.004, Asset 1199419.49, Validation Asset 924658.98, Average Validation Sharpe -0.03\n",
      "Episode 300, Reward 0.258, Average Reward 0.245, valReward -0.037, val Average Reward -0.015, Asset 1289983.90, Validation Asset 957006.05, Average Validation Sharpe -0.11\n",
      "Episode 400, Reward 0.170, Average Reward 0.287, valReward -0.103, val Average Reward -0.001, Asset 1183681.75, Validation Asset 894187.32, Average Validation Sharpe -0.01\n",
      "Episode 500, Reward 0.247, Average Reward 0.311, valReward -0.114, val Average Reward -0.017, Asset 1278649.42, Validation Asset 883575.02, Average Validation Sharpe -0.13\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.008, Average Reward 0.097, valReward -0.114, val Average Reward -0.029, Asset 1006492.09, Validation Asset 873705.51, Average Validation Sharpe -0.15\n",
      "Episode 200, Reward 0.156, Average Reward 0.229, valReward -0.070, val Average Reward -0.045, Asset 1166591.85, Validation Asset 916455.28, Average Validation Sharpe -0.23\n",
      "Episode 300, Reward 0.521, Average Reward 0.303, valReward -0.036, val Average Reward -0.035, Asset 1664935.37, Validation Asset 948496.85, Average Validation Sharpe -0.18\n",
      "Episode 400, Reward 0.111, Average Reward 0.389, valReward 0.012, val Average Reward -0.012, Asset 1115052.24, Validation Asset 994765.03, Average Validation Sharpe -0.066\n",
      "Episode 500, Reward 0.494, Average Reward 0.400, valReward 0.108, val Average Reward -0.041, Asset 1627163.37, Validation Asset 1096020.13, Average Validation Sharpe -0.20\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.111, Average Reward 0.091, valReward 0.011, val Average Reward 0.134, Asset 1114401.01, Validation Asset 1000300.59, Average Validation Sharpe 0.91\n",
      "Episode 200, Reward 0.182, Average Reward 0.131, valReward 0.067, val Average Reward 0.167, Asset 1196830.23, Validation Asset 1058768.22, Average Validation Sharpe 1.10\n",
      "Episode 300, Reward 0.116, Average Reward 0.134, valReward 0.008, val Average Reward 0.150, Asset 1121356.88, Validation Asset 995106.67, Average Validation Sharpe 1.012\n",
      "Episode 400, Reward 0.079, Average Reward 0.148, valReward 0.301, val Average Reward 0.138, Asset 1080526.40, Validation Asset 1327030.87, Average Validation Sharpe 0.92\n",
      "Episode 500, Reward 0.012, Average Reward 0.151, valReward 0.102, val Average Reward 0.149, Asset 1011798.22, Validation Asset 1088647.56, Average Validation Sharpe 0.98\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.027, Average Reward 0.042, valReward 0.186, val Average Reward 0.055, Asset 1026101.23, Validation Asset 1187888.78, Average Validation Sharpe 0.43\n",
      "Episode 200, Reward 0.141, Average Reward 0.090, valReward 0.107, val Average Reward 0.039, Asset 1149074.00, Validation Asset 1098727.82, Average Validation Sharpe 0.30\n",
      "Episode 300, Reward 0.078, Average Reward 0.132, valReward -0.069, val Average Reward 0.038, Asset 1079764.05, Validation Asset 928509.90, Average Validation Sharpe 0.31\n",
      "Episode 400, Reward 0.180, Average Reward 0.167, valReward -0.018, val Average Reward 0.055, Asset 1195804.47, Validation Asset 972388.88, Average Validation Sharpe 0.44\n",
      "Episode 500, Reward 0.178, Average Reward 0.180, valReward -0.049, val Average Reward 0.050, Asset 1193096.33, Validation Asset 939703.34, Average Validation Sharpe 0.40\n"
     ]
    }
   ],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))\n",
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, codes_dict=codes_dict, train=True, code=code, time_period = time_period, codes=codes)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, codes_dict=codes_dict, train=False, code=code, time_period = time_period,  codes=codes)\n",
    "    agent = Agent(2*3, env.action_space, 64, 0.001, 0.001, 0.99, 51, -0.1, 0.1, 'cuda', True,personality = 1)\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
