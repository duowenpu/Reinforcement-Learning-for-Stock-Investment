{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "time_period = 2\n",
    "sys.path.append('./')\n",
    "from utlis import get_data, Stock_Env\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import deque\n",
    "\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden=[64, 64]):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:00<00:00, 543.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 575.03it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train, stock_df_test, stock_df_train_, stock_df_test_, codes = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = None\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# from networks import *\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, device):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size).to(device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size).to(device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.tst = None\n",
    "        self.mu = [0]\n",
    "        self.last_action = 0\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state).reshape(-1)\n",
    "            if (action_values).max() > np.max(self.mu):\n",
    "                # self.mu = 0.95*self.mu + 0.05*action_values.max()\n",
    "\n",
    "                self.mu.append(action_values.max().cpu().data.numpy())                \n",
    "                if len(self.mu) > 10:\n",
    "                    self.mu = self.mu[-10:]\n",
    "                self.last_action = np.argmax(action_values.cpu().data.numpy())\n",
    "                return self.last_action\n",
    "            else:\n",
    "                return self.last_action\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "            self.last_action = action\n",
    "            return action\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "        self.tst = states\n",
    "        Q_values = self.Q_local(states).reshape(-1,11)\n",
    "        Q_values = torch.gather(input=Q_values, dim=-1, index=actions)\n",
    "        with torch.no_grad():\n",
    "            Q_targets = self.Q_target(next_states)\n",
    "            Q_targets, _ = torch.max(input=Q_targets, dim=-1, keepdim=True)\n",
    "            Q_targets = rewards + self.gamma * (1 - dones) * Q_targets\n",
    "\n",
    "        loss = (Q_values - Q_targets).pow(2).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 1000\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    # agent.mu=0\n",
    "    env.mu=[0]\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    state = frame\n",
    "    t = 0\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        rewards_log.append(reward)\n",
    "        episodic_reward += reward\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    # global rewards_log, average_log, state_history, action_history, done_history, reward_history\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "    for i in range(1, 1 + num_episode):\n",
    "        env.mu=[0]\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "\n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        # print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}'.format(i, episodic_reward, average_log[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.030, Average Reward 0.038, valReward 0.366, val Average Reward 0.292, Asset 1027468.31, Validation Asset 1426516.61, Average Validation Sharpe 2.13\n",
      "Episode 200, Reward 0.008, Average Reward 0.045, valReward 0.246, val Average Reward 0.289, Asset 1003938.16, Validation Asset 1268518.35, Average Validation Sharpe 2.09\n",
      "Episode 300, Reward -0.003, Average Reward 0.051, valReward 0.176, val Average Reward 0.276, Asset 997163.73, Validation Asset 1182843.36, Average Validation Sharpe 1.99\n",
      "Episode 400, Reward 0.089, Average Reward 0.053, valReward 0.397, val Average Reward 0.273, Asset 1090460.49, Validation Asset 1470266.80, Average Validation Sharpe 1.99\n",
      "Episode 500, Reward 0.056, Average Reward 0.062, valReward 0.336, val Average Reward 0.275, Asset 1054243.11, Validation Asset 1388755.45, Average Validation Sharpe 2.01\n",
      "Episode 600, Reward 0.010, Average Reward 0.072, valReward 0.281, val Average Reward 0.278, Asset 1008883.83, Validation Asset 1315106.42, Average Validation Sharpe 2.03\n",
      "Episode 700, Reward -0.020, Average Reward 0.071, valReward 0.327, val Average Reward 0.273, Asset 976826.52, Validation Asset 1377261.57, Average Validation Sharpe 1.98\n",
      "Episode 800, Reward 0.040, Average Reward 0.085, valReward 0.415, val Average Reward 0.283, Asset 1037087.30, Validation Asset 1501265.41, Average Validation Sharpe 2.03\n",
      "Episode 900, Reward 0.098, Average Reward 0.076, valReward 0.137, val Average Reward 0.276, Asset 1099825.88, Validation Asset 1137587.20, Average Validation Sharpe 2.00\n",
      "Episode 1000, Reward 0.106, Average Reward 0.062, valReward 0.248, val Average Reward 0.273, Asset 1107355.30, Validation Asset 1271647.80, Average Validation Sharpe 1.99\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.034, Average Reward 0.067, valReward -0.155, val Average Reward -0.041, Asset 1028208.10, Validation Asset 841609.64, Average Validation Sharpe -0.21\n",
      "Episode 200, Reward 0.063, Average Reward 0.098, valReward 0.011, val Average Reward -0.035, Asset 1056430.08, Validation Asset 990720.35, Average Validation Sharpe -0.189\n",
      "Episode 300, Reward 0.068, Average Reward 0.100, valReward 0.065, val Average Reward -0.045, Asset 1065644.87, Validation Asset 1047208.71, Average Validation Sharpe -0.23\n",
      "Episode 400, Reward 0.039, Average Reward 0.094, valReward -0.149, val Average Reward -0.042, Asset 1036888.16, Validation Asset 848281.91, Average Validation Sharpe -0.22\n",
      "Episode 500, Reward 0.088, Average Reward 0.102, valReward 0.008, val Average Reward -0.042, Asset 1085549.31, Validation Asset 990273.77, Average Validation Sharpe -0.212\n",
      "Episode 600, Reward 0.050, Average Reward 0.114, valReward -0.143, val Average Reward -0.046, Asset 1050263.73, Validation Asset 851421.36, Average Validation Sharpe -0.23\n",
      "Episode 700, Reward 0.264, Average Reward 0.107, valReward 0.148, val Average Reward -0.051, Asset 1289962.01, Validation Asset 1138591.18, Average Validation Sharpe -0.26\n",
      "Episode 800, Reward 0.150, Average Reward 0.134, valReward -0.063, val Average Reward -0.057, Asset 1158948.00, Validation Asset 925642.44, Average Validation Sharpe -0.30\n",
      "Episode 900, Reward 0.132, Average Reward 0.096, valReward 0.110, val Average Reward -0.045, Asset 1128584.39, Validation Asset 1096630.91, Average Validation Sharpe -0.23\n",
      "Episode 1000, Reward 0.172, Average Reward 0.117, valReward -0.050, val Average Reward -0.030, Asset 1176164.56, Validation Asset 930697.04, Average Validation Sharpe -0.15\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.101, Average Reward 0.081, valReward -0.111, val Average Reward -0.012, Asset 1095615.90, Validation Asset 888199.11, Average Validation Sharpe -0.09\n",
      "Episode 200, Reward 0.177, Average Reward 0.095, valReward -0.088, val Average Reward -0.012, Asset 1178325.33, Validation Asset 907661.31, Average Validation Sharpe -0.10\n",
      "Episode 300, Reward 0.143, Average Reward 0.112, valReward -0.059, val Average Reward -0.015, Asset 1147163.71, Validation Asset 936891.37, Average Validation Sharpe -0.11\n",
      "Episode 400, Reward 0.088, Average Reward 0.106, valReward 0.068, val Average Reward -0.008, Asset 1083158.54, Validation Asset 1059537.44, Average Validation Sharpe -0.06\n",
      "Episode 500, Reward 0.204, Average Reward 0.114, valReward -0.095, val Average Reward -0.020, Asset 1209551.81, Validation Asset 900807.72, Average Validation Sharpe -0.14\n",
      "Episode 600, Reward 0.061, Average Reward 0.119, valReward -0.044, val Average Reward -0.021, Asset 1056086.01, Validation Asset 951102.95, Average Validation Sharpe -0.16\n",
      "Episode 700, Reward 0.287, Average Reward 0.103, valReward 0.052, val Average Reward -0.008, Asset 1317056.48, Validation Asset 1046600.04, Average Validation Sharpe -0.06\n",
      "Episode 800, Reward 0.263, Average Reward 0.117, valReward -0.043, val Average Reward -0.015, Asset 1291165.23, Validation Asset 950970.03, Average Validation Sharpe -0.11\n",
      "Episode 900, Reward 0.201, Average Reward 0.112, valReward 0.019, val Average Reward -0.017, Asset 1206937.72, Validation Asset 1010475.24, Average Validation Sharpe -0.13\n",
      "Episode 1000, Reward 0.281, Average Reward 0.133, valReward 0.065, val Average Reward -0.013, Asset 1304092.35, Validation Asset 1058157.88, Average Validation Sharpe -0.10\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.108, Average Reward 0.073, valReward 0.101, val Average Reward 0.126, Asset 1112956.97, Validation Asset 1093393.55, Average Validation Sharpe 0.91\n",
      "Episode 200, Reward 0.152, Average Reward 0.078, valReward 0.275, val Average Reward 0.141, Asset 1160609.33, Validation Asset 1295771.18, Average Validation Sharpe 0.99\n",
      "Episode 300, Reward -0.014, Average Reward 0.078, valReward -0.081, val Average Reward 0.128, Asset 985538.64, Validation Asset 912787.13, Average Validation Sharpe 0.93\n",
      "Episode 400, Reward 0.107, Average Reward 0.090, valReward 0.241, val Average Reward 0.154, Asset 1110946.49, Validation Asset 1253794.28, Average Validation Sharpe 1.08\n",
      "Episode 500, Reward 0.130, Average Reward 0.081, valReward 0.143, val Average Reward 0.136, Asset 1138013.11, Validation Asset 1141417.16, Average Validation Sharpe 0.97\n",
      "Episode 600, Reward 0.035, Average Reward 0.086, valReward 0.070, val Average Reward 0.137, Asset 1035251.47, Validation Asset 1063657.46, Average Validation Sharpe 0.98\n",
      "Episode 700, Reward 0.134, Average Reward 0.084, valReward 0.133, val Average Reward 0.143, Asset 1141248.05, Validation Asset 1128533.01, Average Validation Sharpe 1.01\n",
      "Episode 800, Reward 0.138, Average Reward 0.092, valReward 0.059, val Average Reward 0.139, Asset 1143631.15, Validation Asset 1047890.51, Average Validation Sharpe 0.99\n",
      "Episode 900, Reward 0.167, Average Reward 0.092, valReward -0.011, val Average Reward 0.125, Asset 1178641.19, Validation Asset 972580.63, Average Validation Sharpe 0.89\n",
      "Episode 1000, Reward 0.174, Average Reward 0.084, valReward 0.128, val Average Reward 0.154, Asset 1186761.55, Validation Asset 1124271.61, Average Validation Sharpe 1.08\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.119, Average Reward 0.040, valReward 0.055, val Average Reward 0.067, Asset 1120658.37, Validation Asset 1046424.72, Average Validation Sharpe 0.59\n",
      "Episode 200, Reward -0.009, Average Reward 0.047, valReward -0.031, val Average Reward 0.049, Asset 987543.57, Validation Asset 960692.95, Average Validation Sharpe 0.42\n",
      "Episode 300, Reward 0.097, Average Reward 0.051, valReward 0.020, val Average Reward 0.058, Asset 1098030.99, Validation Asset 1008624.80, Average Validation Sharpe 0.49\n",
      "Episode 400, Reward 0.035, Average Reward 0.054, valReward 0.054, val Average Reward 0.067, Asset 1033291.14, Validation Asset 1045799.25, Average Validation Sharpe 0.59\n",
      "Episode 500, Reward 0.023, Average Reward 0.052, valReward 0.090, val Average Reward 0.049, Asset 1020674.41, Validation Asset 1080092.78, Average Validation Sharpe 0.42\n",
      "Episode 600, Reward 0.033, Average Reward 0.056, valReward 0.173, val Average Reward 0.060, Asset 1033089.92, Validation Asset 1174412.55, Average Validation Sharpe 0.52\n",
      "Episode 700, Reward 0.039, Average Reward 0.065, valReward 0.024, val Average Reward 0.058, Asset 1039040.64, Validation Asset 1020749.50, Average Validation Sharpe 0.49\n",
      "Episode 800, Reward 0.098, Average Reward 0.071, valReward 0.073, val Average Reward 0.063, Asset 1101017.31, Validation Asset 1068730.35, Average Validation Sharpe 0.54\n",
      "Episode 900, Reward 0.133, Average Reward 0.063, valReward 0.196, val Average Reward 0.050, Asset 1139060.10, Validation Asset 1203027.36, Average Validation Sharpe 0.42\n",
      "Episode 1000, Reward 0.163, Average Reward 0.071, valReward 0.107, val Average Reward 0.059, Asset 1172401.86, Validation Asset 1104848.44, Average Validation Sharpe 0.51\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, codes_dict=codes_dict, train=True, code=code, time_period = time_period, codes=codes)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, codes_dict=codes_dict, train=False, code=code, time_period = time_period,  codes=codes)\n",
    "    agent = Agent(2*3, 11, 64, 0.001, 0.001, 0.99, 'cuda')\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
