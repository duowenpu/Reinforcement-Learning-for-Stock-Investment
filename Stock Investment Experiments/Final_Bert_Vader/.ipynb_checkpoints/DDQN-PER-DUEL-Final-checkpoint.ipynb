{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "time_period = 2\n",
    "sys.path.append('./')\n",
    "from utlis import get_data, Stock_Env\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import deque\n",
    "\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden=[64, 64], duel=False):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size)\n",
    "        self.duel = duel\n",
    "        if self.duel:\n",
    "            self.fc4 = nn.Linear(hidden[1], 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if self.duel:\n",
    "            x1 = self.fc3(x)\n",
    "            x1 = x1 - torch.max(x1, dim=1, keepdim=True)[0] # set the max to be 0\n",
    "            x2 = self.fc4(x)\n",
    "            # print(x1.shape, x2.shape)\n",
    "            return x1 + x2\n",
    "        else:\n",
    "            x = self.fc3(x)\n",
    "            # print(x.shape)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:00<00:00, 611.81it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 594.54it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train, stock_df_test, stock_df_train_, stock_df_test_, codes = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        # the first capacity-1 positions are not leaves\n",
    "        self.vals = [0 for _ in range(2*capacity - 1)] # think about why if you are not familiar with this\n",
    "        \n",
    "    def retrive(self, num):\n",
    "        '''\n",
    "        This function find the first index whose cumsum is no smaller than num\n",
    "        '''\n",
    "        ind = 0 # search from root\n",
    "        while ind < self.capacity-1: # not a leaf\n",
    "            left = 2*ind + 1\n",
    "            right = left + 1\n",
    "            if num > self.vals[left]: # the sum of the whole left tree is not large enouth\n",
    "                num -= self.vals[left] # think about why?\n",
    "                ind = right\n",
    "            else: # search in the left tree\n",
    "                ind = left\n",
    "        return ind - self.capacity + 1\n",
    "    \n",
    "    def update(self, delta, ind):\n",
    "        '''\n",
    "        Change the value at ind by delta, and update the tree\n",
    "        Notice that this ind should be the index in real memory part, instead of the ind in self.vals\n",
    "        '''\n",
    "        ind += self.capacity - 1\n",
    "        while True:\n",
    "            self.vals[ind] += delta\n",
    "            if ind == 0:\n",
    "                break\n",
    "            ind -= 1\n",
    "            ind //= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import deque\n",
    "\n",
    "# test = deque(maxlen=5)\n",
    "# for i in range(10):\n",
    "#     test.append(i)\n",
    "#     print(test)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import bisect\n",
    "import torch\n",
    "\n",
    "ALPHA = 0.5\n",
    "EPSILON = 0.05\n",
    "TD_INIT = 1\n",
    "\n",
    "class Replay_Buffer:\n",
    "    '''\n",
    "    Vanilla replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=None):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.memory = [None for _ in range(capacity)] # save tuples (state, action, reward, next_state, done)\n",
    "        self.ind_max = 0 # how many transitions have been stored\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        ind = self.ind_max % self.capacity\n",
    "        self.memory[ind] = (state, action, reward, next_state, done)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, k):\n",
    "        '''\n",
    "        return sampled transitions. Make sure that there are at least k transitions stored before calling this method \n",
    "        '''\n",
    "        index_set = random.sample(list(range(len(self))), k)\n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.ind_max, self.capacity)\n",
    "        \n",
    "class Rank_Replay_Buffer:\n",
    "    '''\n",
    "    Rank-based replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=64):\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = ALPHA\n",
    "        self.memory = [None for _ in range(capacity)]\n",
    "        self.segments = [-1] + [None for _ in range(batch_size)] # the ith index will be in [segments[i-1]+1, segments[i]]\n",
    "        \n",
    "        self.errors = [] # saves (-TD_error, index of transition), sorted\n",
    "        self.memory_to_rank = [None for _ in range(capacity)]\n",
    "        \n",
    "        self.ind_max = 0 # how many transitions have been stored\n",
    "        self.total_weights = 0 # sum of p_i\n",
    "        self.cumulated_weights = []\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        index = self.ind_max % self.capacity\n",
    "        if self.ind_max >= self.capacity: # memory is full, need to pop\n",
    "            self.pop(index)\n",
    "        else: # memory is not full, need to adjust weights and find segment points\n",
    "            self.total_weights += (1/(1+self.ind_max))**self.alpha # memory is not full, calculate new weights\n",
    "            self.cumulated_weights.append(self.total_weights)\n",
    "            self.update_segments()\n",
    "        \n",
    "        max_error = -self.errors[0][0] if self.errors else 0\n",
    "        self.insert(max_error, index)\n",
    "        self.memory[index] = (state, action, reward, next_state, done)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, batch_size=None): # notive that batch_size is not used. It's just to unify the calling form\n",
    "        index_set = [random.randint(self.segments[i]+1, self.segments[i+1]) for i in range(self.batch_size)]\n",
    "        probs = torch.from_numpy(np.vstack([(1/(1+ind))**self.alpha/self.total_weights for ind in index_set])).float()\n",
    "        \n",
    "        index_set = [self.errors[ind][1] for ind in index_set]\n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "        for ind in index_set:\n",
    "            self.pop(ind)\n",
    "        \n",
    "        return index_set, states, actions, rewards, next_states, dones, probs\n",
    "    \n",
    "    def insert(self, error, index):\n",
    "        '''\n",
    "        Input : \n",
    "            error : the TD-error of this transition\n",
    "            index : the location of this transition\n",
    "        insert error into self.errors, update self.memory_to_rank and self.rank_to_memory accordingly\n",
    "        '''\n",
    "        ind = bisect.bisect(self.errors, (-error, index))\n",
    "        self.memory_to_rank[index] = ind\n",
    "        self.errors.insert(ind, (-error, index))\n",
    "        for i in range(ind+1, len(self.errors)):\n",
    "            self.memory_to_rank[self.errors[i][1]] += 1\n",
    "        \n",
    "    def pop(self, index):\n",
    "        '''\n",
    "        Input :\n",
    "            index : the location of a transition\n",
    "        remove this transition, update self.memory_to_rank and self.rank_to_memory accordingly\n",
    "        '''\n",
    "        ind = self.memory_to_rank[index]\n",
    "        self.memory_to_rank[index] = None\n",
    "        self.errors.pop(ind)\n",
    "        for i in range(ind, len(self.errors)):\n",
    "            self.memory_to_rank[self.errors[i][1]] -= 1\n",
    "        \n",
    "    def update_segments(self):\n",
    "        '''\n",
    "        Update the segment points.\n",
    "        '''\n",
    "        if self.ind_max+1 < self.batch_size: # if there is no enough transitions\n",
    "            return None\n",
    "        for i in range(self.batch_size):\n",
    "            ind = bisect.bisect_left(self.cumulated_weights, self.total_weights*((i+1)/self.batch_size))\n",
    "            self.segments[i+1] = max(ind, self.segments[i]+1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return min(self.capacity, self.ind_max)\n",
    "    \n",
    "\n",
    "class Proportion_Replay_Buffer:\n",
    "    '''\n",
    "    Proportion-based replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=None):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = ALPHA\n",
    "        self.memory = [None for _ in range(capacity)]\n",
    "        self.weights = SumTree(self.capacity)\n",
    "        self.default = TD_INIT\n",
    "        self.ind_max = 0\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        index = self.ind_max % self.capacity\n",
    "        self.memory[index] = (state, action, reward, next_state, done)\n",
    "        delta = self.default+EPSILON - self.weights.vals[index+self.capacity-1]\n",
    "        self.weights.update(delta, index)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        index_set = [self.weights.retrive(self.weights.vals[0]*random.random()) for _ in range(batch_size)]\n",
    "        #print(index_set)\n",
    "        probs = torch.from_numpy(np.vstack([self.weights.vals[ind+self.capacity-1]/self.weights.vals[0] for ind in index_set])).float()                     \n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "\n",
    "        return index_set, states, actions, rewards, next_states, dones, probs\n",
    "                                 \n",
    "    def insert(self, error, index):\n",
    "        delta = error+EPSILON - self.weights.vals[index+self.capacity-1]\n",
    "        self.weights.update(delta, index)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return min(self.capacity, self.ind_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = None\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# from networks import *\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, device, visual=False):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size,duel=False).to(device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size,duel=False).to(device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = Proportion_Replay_Buffer(int(1e5), bs)\n",
    "        self.tst = None\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        index_set, states, actions, rewards, next_states, dones, probs = self.memory.sample(self.bs)\n",
    "        w = 1/len(self.memory)/probs\n",
    "        w = w/torch.max(w)\n",
    "        w = w.to(self.device)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        # print(states.shape)\n",
    "        Q_values = self.Q_local(states)\n",
    "        # print(actions.shape)\n",
    "        Q_values = torch.gather(input=Q_values, dim=-1, index=actions)\n",
    "        with torch.no_grad():\n",
    "            Q_targets = self.Q_target(next_states)\n",
    "            Q_targets, _ = torch.max(input=Q_targets, dim=-1, keepdim=True)\n",
    "            Q_targets = rewards + self.gamma * (1 - dones) * Q_targets\n",
    "    \n",
    "        deltas = Q_values - Q_targets\n",
    "        loss = (w*deltas.pow(2)).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        deltas = np.abs(deltas.detach().cpu().numpy().reshape(-1))\n",
    "        for i in range(self.bs):\n",
    "            self.memory.insert(deltas[i], index_set[i])\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.995\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    # agent.mu=0\n",
    "    env.mu=[0]\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    state = frame.reshape(-1)\n",
    "    t = 0\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        rewards_log.append(reward)\n",
    "        episodic_reward += reward\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset().reshape(-1)\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        # state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            frame = frame.reshape(-1)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            # next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "\n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.063, Average Reward 0.037, valReward 0.248, val Average Reward 0.278, Asset 1061638.68, Validation Asset 1268293.66, Average Validation Sharpe 2.01\n",
      "Episode 200, Reward 0.008, Average Reward 0.061, valReward 0.308, val Average Reward 0.285, Asset 1007066.68, Validation Asset 1347247.72, Average Validation Sharpe 2.06\n",
      "Episode 300, Reward 0.036, Average Reward 0.093, valReward 0.347, val Average Reward 0.282, Asset 1035439.88, Validation Asset 1398966.43, Average Validation Sharpe 2.03\n",
      "Episode 400, Reward 0.002, Average Reward 0.135, valReward 0.130, val Average Reward 0.275, Asset 1001711.34, Validation Asset 1127726.17, Average Validation Sharpe 1.99\n",
      "Episode 500, Reward 0.061, Average Reward 0.148, valReward 0.282, val Average Reward 0.296, Asset 1061736.17, Validation Asset 1315477.85, Average Validation Sharpe 2.14\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.183, Average Reward 0.091, valReward -0.094, val Average Reward -0.070, Asset 1196069.43, Validation Asset 896666.64, Average Validation Sharpe -0.37\n",
      "Episode 200, Reward 0.063, Average Reward 0.188, valReward -0.009, val Average Reward -0.038, Asset 1064177.17, Validation Asset 974938.60, Average Validation Sharpe -0.19\n",
      "Episode 300, Reward 0.438, Average Reward 0.314, valReward -0.055, val Average Reward -0.033, Asset 1527422.32, Validation Asset 931380.80, Average Validation Sharpe -0.17\n",
      "Episode 400, Reward 0.105, Average Reward 0.407, valReward -0.033, val Average Reward -0.051, Asset 1109093.47, Validation Asset 953271.46, Average Validation Sharpe -0.26\n",
      "Episode 500, Reward 0.385, Average Reward 0.424, valReward -0.078, val Average Reward -0.041, Asset 1461785.26, Validation Asset 908455.40, Average Validation Sharpe -0.21\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.114, Average Reward 0.076, valReward -0.099, val Average Reward -0.020, Asset 1117194.46, Validation Asset 896947.04, Average Validation Sharpe -0.16\n",
      "Episode 200, Reward 0.153, Average Reward 0.118, valReward -0.078, val Average Reward -0.013, Asset 1161161.79, Validation Asset 916399.57, Average Validation Sharpe -0.09\n",
      "Episode 300, Reward 0.096, Average Reward 0.128, valReward 0.144, val Average Reward -0.009, Asset 1098147.96, Validation Asset 1146903.71, Average Validation Sharpe -0.07\n",
      "Episode 400, Reward 0.145, Average Reward 0.150, valReward 0.011, val Average Reward -0.016, Asset 1153166.65, Validation Asset 1001482.72, Average Validation Sharpe -0.12\n",
      "Episode 500, Reward 0.252, Average Reward 0.166, valReward 0.017, val Average Reward -0.017, Asset 1279401.20, Validation Asset 1008906.77, Average Validation Sharpe -0.13\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward -0.020, Average Reward 0.072, valReward 0.052, val Average Reward 0.133, Asset 980399.18, Validation Asset 1044721.87, Average Validation Sharpe 0.94\n",
      "Episode 200, Reward 0.029, Average Reward 0.116, valReward 0.160, val Average Reward 0.144, Asset 1029677.39, Validation Asset 1160154.13, Average Validation Sharpe 1.02\n",
      "Episode 300, Reward 0.218, Average Reward 0.140, valReward 0.125, val Average Reward 0.136, Asset 1241075.22, Validation Asset 1122518.62, Average Validation Sharpe 1.00\n",
      "Episode 400, Reward 0.190, Average Reward 0.172, valReward 0.049, val Average Reward 0.135, Asset 1206038.57, Validation Asset 1043501.73, Average Validation Sharpe 0.96\n",
      "Episode 500, Reward 0.242, Average Reward 0.191, valReward 0.063, val Average Reward 0.148, Asset 1269028.14, Validation Asset 1051421.83, Average Validation Sharpe 1.04\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.064, Average Reward 0.046, valReward 0.049, val Average Reward 0.053, Asset 1064369.57, Validation Asset 1038551.29, Average Validation Sharpe 0.45\n",
      "Episode 200, Reward 0.048, Average Reward 0.079, valReward -0.031, val Average Reward 0.070, Asset 1047675.40, Validation Asset 962732.96, Average Validation Sharpe 0.60\n",
      "Episode 300, Reward 0.107, Average Reward 0.090, valReward 0.014, val Average Reward 0.062, Asset 1110349.22, Validation Asset 1005052.67, Average Validation Sharpe 0.52\n",
      "Episode 400, Reward 0.132, Average Reward 0.108, valReward 0.046, val Average Reward 0.057, Asset 1138896.66, Validation Asset 1038761.25, Average Validation Sharpe 0.49\n",
      "Episode 500, Reward 0.183, Average Reward 0.144, valReward 0.157, val Average Reward 0.058, Asset 1197156.30, Validation Asset 1156062.11, Average Validation Sharpe 0.49\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, codes_dict=codes_dict, train=True, code=code, time_period = time_period, codes=codes)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, codes_dict=codes_dict, train=False, code=code, time_period = time_period,  codes=codes)\n",
    "    agent = Agent(2*3, 11, 64, 0.001, 0.001, 0.99, 'cuda', True)\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
