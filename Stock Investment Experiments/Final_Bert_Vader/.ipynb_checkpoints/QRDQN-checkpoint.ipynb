{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "time_period = 2\n",
    "sys.path.append('./')\n",
    "from utlis import get_data, Stock_Env\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import deque\n",
    "    \n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, N, hidden=[64, 64]):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size*N)\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.N = N\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state #(batch_size, state_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #(batch_size, action_size*N)\n",
    "        x = x.view(-1, self.action_size, self.N) #(batch_size, action_size, N)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:01<00:00, 482.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 559.38it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train, stock_df_test, stock_df_train_, stock_df_test_, codes = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, N, kappa, device, visual=False):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.tau = torch.linspace(0, 1, N+1)\n",
    "        self.tau = (self.tau[:-1] + self.tau[1:]) / 2\n",
    "        self.tau_hat = self.tau.to(device).unsqueeze(0) #(1, N)\n",
    "        self.tau = tau\n",
    "        self.kappa = kappa\n",
    "\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size, N).to(self.device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size, N).to(self.device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state)\n",
    "                action_values = action_values.mean(dim=-1, keepdim=False).view(-1)\n",
    "            return np.argmax(action_values.cpu().numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        quantiles_local = self.Q_local(states) #(batch_size, action_size, N)\n",
    "        quantiles_local = torch.gather(input=quantiles_local, dim=1, index=actions.unsqueeze(1).repeat(1, 1, self.N)) #(batch_size, 1, N)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            quantiles_target = self.Q_target(next_states) #(batch_size, action_size, N)\n",
    "            next_actions = torch.max(quantiles_target.sum(dim=2, keepdim=True), dim=1, keepdim=True)[1] #(batch_size, 1, 1)\n",
    "            quantiles_target = torch.gather(input=quantiles_target, index=next_actions.repeat(1,1,self.N), dim=1) #(batch_size, 1, N)\n",
    "            quantiles_target = rewards.unsqueeze(1).repeat(1, 1, self.N) + self.gamma * (1 - dones.unsqueeze(1).repeat(1, 1, self.N)) * quantiles_target #(batch_size, 1, N)\n",
    "            quantiles_target = quantiles_target.permute(0, 2, 1) #(batch_size, N, 1)\n",
    "\n",
    "        diff = quantiles_target - quantiles_local #(batch_size, N, N)\n",
    "        tau = self.tau_hat.unsqueeze(0).repeat(diff.size(0), 1, 1) #(batch_size, 1, N)\n",
    "        loss = (tau - (diff<0).float()).abs() * self.huber(diff) #(batch_size, N, N)\n",
    "        loss = loss.mean(dim=2, keepdim=False).sum(dim=1, keepdim=False) #(batch_size,)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.tau)\n",
    "\n",
    "    def huber(self, u):\n",
    "        if self.kappa > 0:\n",
    "            flag = (u.abs()<self.kappa).float()\n",
    "            loss = 0.5*u.pow(2) * flag + self.kappa*(u.abs()-0.5*self.kappa) * (1-flag)\n",
    "        else:\n",
    "            loss = u.abs()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    state = env.reset()\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        next_state = frame\n",
    "        state = next_state.copy()\n",
    "        episodic_reward += reward\n",
    "        rewards_log.append(reward)\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "        \n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.014, Average Reward 0.037, valReward 0.228, val Average Reward 0.280, Asset 1012817.74, Validation Asset 1242371.48, Average Validation Sharpe 2.02\n",
      "Episode 200, Reward 0.035, Average Reward 0.098, valReward 0.263, val Average Reward 0.285, Asset 1033788.74, Validation Asset 1289214.67, Average Validation Sharpe 2.08\n",
      "Episode 300, Reward 0.299, Average Reward 0.131, valReward 0.115, val Average Reward 0.269, Asset 1341079.58, Validation Asset 1109416.68, Average Validation Sharpe 1.96\n",
      "Episode 400, Reward 0.208, Average Reward 0.166, valReward 0.174, val Average Reward 0.272, Asset 1226993.14, Validation Asset 1180081.91, Average Validation Sharpe 1.98\n",
      "Episode 500, Reward 0.222, Average Reward 0.178, valReward 0.281, val Average Reward 0.284, Asset 1244851.19, Validation Asset 1312490.45, Average Validation Sharpe 2.07\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.190, Average Reward 0.116, valReward -0.192, val Average Reward -0.044, Asset 1197619.65, Validation Asset 805491.84, Average Validation Sharpe -0.22\n",
      "Episode 200, Reward 0.386, Average Reward 0.200, valReward -0.009, val Average Reward -0.022, Asset 1447754.48, Validation Asset 977639.91, Average Validation Sharpe -0.11\n",
      "Episode 300, Reward 0.422, Average Reward 0.259, valReward -0.332, val Average Reward -0.044, Asset 1509282.03, Validation Asset 700954.51, Average Validation Sharpe -0.22\n",
      "Episode 400, Reward 0.581, Average Reward 0.358, valReward -0.082, val Average Reward -0.072, Asset 1765326.58, Validation Asset 904794.29, Average Validation Sharpe -0.37\n",
      "Episode 500, Reward 0.534, Average Reward 0.420, valReward -0.084, val Average Reward -0.050, Asset 1684610.98, Validation Asset 902747.30, Average Validation Sharpe -0.26\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.111, Average Reward 0.083, valReward -0.136, val Average Reward -0.029, Asset 1107816.96, Validation Asset 865445.72, Average Validation Sharpe -0.22\n",
      "Episode 200, Reward 0.142, Average Reward 0.128, valReward -0.141, val Average Reward -0.007, Asset 1144899.50, Validation Asset 860521.44, Average Validation Sharpe -0.06\n",
      "Episode 300, Reward 0.311, Average Reward 0.174, valReward 0.055, val Average Reward -0.025, Asset 1357421.03, Validation Asset 1048549.23, Average Validation Sharpe -0.20\n",
      "Episode 400, Reward 0.223, Average Reward 0.187, valReward 0.018, val Average Reward -0.014, Asset 1245196.61, Validation Asset 1007012.84, Average Validation Sharpe -0.11\n",
      "Episode 500, Reward 0.208, Average Reward 0.231, valReward -0.022, val Average Reward -0.012, Asset 1228185.20, Validation Asset 968437.88, Average Validation Sharpe -0.09\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.110, Average Reward 0.076, valReward 0.205, val Average Reward 0.148, Asset 1114174.70, Validation Asset 1211664.59, Average Validation Sharpe 1.02\n",
      "Episode 200, Reward 0.260, Average Reward 0.115, valReward 0.119, val Average Reward 0.156, Asset 1291759.56, Validation Asset 1112721.30, Average Validation Sharpe 1.10\n",
      "Episode 300, Reward 0.270, Average Reward 0.168, valReward 0.062, val Average Reward 0.132, Asset 1304550.84, Validation Asset 1045663.50, Average Validation Sharpe 0.94\n",
      "Episode 400, Reward 0.312, Average Reward 0.174, valReward 0.065, val Average Reward 0.144, Asset 1361731.70, Validation Asset 1053521.33, Average Validation Sharpe 1.02\n",
      "Episode 500, Reward 0.000, Average Reward 0.186, valReward 0.138, val Average Reward 0.141, Asset 1000000.00, Validation Asset 1136661.60, Average Validation Sharpe 0.99\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.061, Average Reward 0.047, valReward 0.000, val Average Reward 0.054, Asset 1059724.83, Validation Asset 992202.17, Average Validation Sharpe 0.46\n",
      "Episode 200, Reward 0.121, Average Reward 0.090, valReward 0.057, val Average Reward 0.061, Asset 1125284.55, Validation Asset 1051033.51, Average Validation Sharpe 0.52\n",
      "Episode 300, Reward 0.143, Average Reward 0.117, valReward 0.016, val Average Reward 0.046, Asset 1151833.02, Validation Asset 1010410.81, Average Validation Sharpe 0.39\n",
      "Episode 400, Reward 0.065, Average Reward 0.133, valReward 0.034, val Average Reward 0.065, Asset 1066642.44, Validation Asset 1025736.84, Average Validation Sharpe 0.55\n",
      "Episode 500, Reward 0.146, Average Reward 0.137, valReward 0.134, val Average Reward 0.050, Asset 1155700.31, Validation Asset 1133681.84, Average Validation Sharpe 0.44\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, codes_dict=codes_dict, train=True, code=code, time_period = time_period, codes=codes)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, codes_dict=codes_dict, train=False, code=code, time_period = time_period,  codes=codes)\n",
    "    agent = Agent(2*3, env.action_space, 64, 0.001, 0.001, 0.99, 51, 1, 'cuda', True)\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
