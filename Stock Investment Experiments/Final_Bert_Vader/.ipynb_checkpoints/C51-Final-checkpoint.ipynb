{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "time_period = 2\n",
    "sys.path.append('./')\n",
    "from utlis import get_data, Stock_Env\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, N, Vmin, Vmax, hidden=[64, 64]):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size*N)\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.N = N\n",
    "        self.values = torch.linspace(Vmin, Vmax, N).view(1, 1, -1).to('cuda') #(1, 1, N)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state #(batch_size, state_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #(batch_size, action_size*N)\n",
    "        x = x.view(-1, self.action_size, self.N) #(batch_size, action_size, N)\n",
    "        log_probs = F.log_softmax(x, dim=2) #(batch_size, action_size, N)\n",
    "        Q_values = log_probs.exp() * self.values #(batch_size, action_size, N)\n",
    "        Q_values = Q_values.sum(dim=2, keepdims=False) #(batch_size, action_size)\n",
    "\n",
    "        return log_probs, Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:00<00:00, 710.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 749.78it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train, stock_df_test, stock_df_train_, stock_df_test_, codes = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Neg</th>\n",
       "      <th>Neu</th>\n",
       "      <th>Pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2017-01-03</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.453046</td>\n",
       "      <td>0.599609</td>\n",
       "      <td>-0.216138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>-0.030042</td>\n",
       "      <td>0.274498</td>\n",
       "      <td>-0.421923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>-0.296605</td>\n",
       "      <td>0.551949</td>\n",
       "      <td>-0.372512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>0.173656</td>\n",
       "      <td>-0.614835</td>\n",
       "      <td>0.628548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-04</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.501430</td>\n",
       "      <td>0.630189</td>\n",
       "      <td>-0.153039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <th>NFLX</th>\n",
       "      <td>0.080448</td>\n",
       "      <td>-0.288111</td>\n",
       "      <td>0.360529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2018-12-31</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>-0.337546</td>\n",
       "      <td>0.418065</td>\n",
       "      <td>-0.119941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMZN</th>\n",
       "      <td>-0.076472</td>\n",
       "      <td>0.181307</td>\n",
       "      <td>-0.191764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOGL</th>\n",
       "      <td>-0.148977</td>\n",
       "      <td>0.328610</td>\n",
       "      <td>-0.296503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>0.078409</td>\n",
       "      <td>-0.252359</td>\n",
       "      <td>0.310931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2091 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Neg       Neu       Pos\n",
       "date       symbol                              \n",
       "2017-01-03 AAPL   -0.453046  0.599609 -0.216138\n",
       "           AMZN   -0.030042  0.274498 -0.421923\n",
       "           GOOGL  -0.296605  0.551949 -0.372512\n",
       "           NFLX    0.173656 -0.614835  0.628548\n",
       "2017-01-04 AAPL   -0.501430  0.630189 -0.153039\n",
       "...                     ...       ...       ...\n",
       "2018-12-28 NFLX    0.080448 -0.288111  0.360529\n",
       "2018-12-31 AAPL   -0.337546  0.418065 -0.119941\n",
       "           AMZN   -0.076472  0.181307 -0.191764\n",
       "           GOOGL  -0.148977  0.328610 -0.296503\n",
       "           NFLX    0.078409 -0.252359  0.310931\n",
       "\n",
       "[2091 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df_train[['Neg','Neu','Pos']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from networks import *\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, N, Vmin, Vmax, device, visual=False, personality=1):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.Vmin = Vmin\n",
    "        self.Vmax = Vmax\n",
    "        self.vals = torch.linspace(Vmin, Vmax, N).to(device)\n",
    "        self.unit = (Vmax - Vmin) / (N - 1)\n",
    "        self.personality=personality\n",
    "\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size, N, Vmin, Vmax).to(self.device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size, N, Vmin, Vmax).to(self.device)\n",
    "\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                _, action_values = self.Q_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "        rewards[rewards<0] = rewards[rewards<0]*self.personality\n",
    "        # print(states)\n",
    "        # print(self.Q_local)\n",
    "        log_probs, _ = self.Q_local(states) #(batch_size, action_size, N)\n",
    "        log_probs = torch.gather(input=log_probs, dim=1, index=actions.unsqueeze(1).repeat(1, 1, self.N)) #(batch_size, 1, N)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            log_probs_targets, Q_targets = self.Q_target(next_states)\n",
    "            _, actions_target = torch.max(input=Q_targets, dim=1, keepdim=True)#(batch_size, 1) the same size as actions\n",
    "            log_probs_targets = torch.gather(input=log_probs_targets, dim=1, index=actions_target.unsqueeze(1).repeat(1, 1, self.N))\n",
    "            target_distribution = self.update_distribution(log_probs_targets.exp(), rewards, dones) #(batch_size, 1, N)\n",
    "\n",
    "        loss = -target_distribution*log_probs #D_KL(target||local)\n",
    "        #loss = -log_probs.exp()*((target_distribution+1e-9).log() - log_probs) #D_KL(local||target)\n",
    "\n",
    "        loss = loss.sum(dim=2, keepdims=False).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_distribution(self, old_distribution, reward, dones):\n",
    "        with torch.no_grad():\n",
    "            reward = reward.view(-1, 1)\n",
    "            batch_size = reward.size(0)\n",
    "            assert old_distribution.size(0) == batch_size\n",
    "            new_vals = self.vals.view(1, -1) * self.gamma * (1-dones) + reward\n",
    "            new_vals = torch.clamp(new_vals, self.Vmin, self.Vmax)\n",
    "            lower = torch.floor((new_vals - self.Vmin) / self.unit).long().to(self.device)\n",
    "            upper = torch.min(lower + 1, other=torch.tensor(self.N - 1)).to(self.device)\n",
    "            lower_vals = self.vals[lower]\n",
    "            lower_probs = 1 - torch.min((new_vals - lower_vals) / self.unit, other=torch.tensor(1, dtype=torch.float32)).to(self.device)\n",
    "            transit = torch.zeros((batch_size, self.N, self.N)).to(self.device)\n",
    "            first_dim = torch.tensor(range(batch_size), dtype=torch.long).view(-1, 1).repeat(1, self.N).view(-1).to(self.device)\n",
    "            second_dim = torch.tensor(range(self.N), dtype=torch.long).repeat(batch_size).to(self.device)\n",
    "            transit[first_dim, second_dim, lower.view(-1)] += lower_probs.view(-1)\n",
    "            transit[first_dim, second_dim, upper.view(-1)] += 1 - lower_probs.view(-1)\n",
    "            if len(old_distribution.size()) == 2:\n",
    "                old_distribution = old_distribution.unsqueeze(1)\n",
    "            return torch.bmm(old_distribution, transit)\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    state = env.reset()\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        next_state = frame\n",
    "        state = next_state.copy()\n",
    "        episodic_reward += reward\n",
    "        rewards_log.append(reward)\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "        \n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes_dict = dict(zip(codes, range(len(codes))))\n",
    "# train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.132, Average Reward 0.063, valReward 0.260, val Average Reward 0.274, Asset 1134568.34, Validation Asset 1285999.31, Average Validation Sharpe 2.02\n",
      "Episode 200, Reward 0.261, Average Reward 0.132, valReward 0.265, val Average Reward 0.277, Asset 1291705.64, Validation Asset 1293492.70, Average Validation Sharpe 2.00\n",
      "Episode 300, Reward 0.300, Average Reward 0.204, valReward 0.191, val Average Reward 0.286, Asset 1343484.27, Validation Asset 1199874.01, Average Validation Sharpe 2.07\n",
      "Episode 400, Reward 0.393, Average Reward 0.269, valReward 0.199, val Average Reward 0.280, Asset 1475135.46, Validation Asset 1209208.47, Average Validation Sharpe 2.02\n",
      "Episode 500, Reward 0.164, Average Reward 0.295, valReward 0.185, val Average Reward 0.278, Asset 1176268.53, Validation Asset 1191977.64, Average Validation Sharpe 2.03\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.200, Average Reward 0.124, valReward -0.126, val Average Reward -0.043, Asset 1210365.17, Validation Asset 865366.22, Average Validation Sharpe -0.23\n",
      "Episode 200, Reward 0.154, Average Reward 0.228, valReward -0.247, val Average Reward -0.048, Asset 1163042.81, Validation Asset 766489.94, Average Validation Sharpe -0.25\n",
      "Episode 300, Reward 0.370, Average Reward 0.327, valReward -0.169, val Average Reward -0.050, Asset 1431589.59, Validation Asset 827359.26, Average Validation Sharpe -0.26\n",
      "Episode 400, Reward 0.627, Average Reward 0.389, valReward 0.019, val Average Reward -0.033, Asset 1843603.57, Validation Asset 1003708.72, Average Validation Sharpe -0.17\n",
      "Episode 500, Reward 0.136, Average Reward 0.435, valReward 0.036, val Average Reward -0.056, Asset 1143518.74, Validation Asset 1021851.03, Average Validation Sharpe -0.29\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.204, Average Reward 0.101, valReward -0.042, val Average Reward -0.015, Asset 1218696.97, Validation Asset 951110.62, Average Validation Sharpe -0.12\n",
      "Episode 200, Reward 0.124, Average Reward 0.158, valReward -0.025, val Average Reward -0.025, Asset 1128356.33, Validation Asset 966500.65, Average Validation Sharpe -0.19\n",
      "Episode 300, Reward 0.224, Average Reward 0.220, valReward 0.001, val Average Reward -0.015, Asset 1245924.32, Validation Asset 991276.36, Average Validation Sharpe -0.111\n",
      "Episode 400, Reward 0.491, Average Reward 0.283, valReward -0.102, val Average Reward -0.015, Asset 1623765.55, Validation Asset 894986.80, Average Validation Sharpe -0.11\n",
      "Episode 500, Reward 0.174, Average Reward 0.315, valReward -0.023, val Average Reward -0.010, Asset 1187460.47, Validation Asset 969954.46, Average Validation Sharpe -0.08\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.116, Average Reward 0.081, valReward 0.081, val Average Reward 0.140, Asset 1121014.04, Validation Asset 1072852.94, Average Validation Sharpe 1.00\n",
      "Episode 200, Reward 0.153, Average Reward 0.119, valReward 0.160, val Average Reward 0.118, Asset 1161457.76, Validation Asset 1156134.69, Average Validation Sharpe 0.83\n",
      "Episode 300, Reward 0.120, Average Reward 0.145, valReward 0.036, val Average Reward 0.146, Asset 1126025.97, Validation Asset 1026594.72, Average Validation Sharpe 1.04\n",
      "Episode 400, Reward 0.231, Average Reward 0.162, valReward 0.215, val Average Reward 0.127, Asset 1256094.52, Validation Asset 1222066.52, Average Validation Sharpe 0.88\n",
      "Episode 500, Reward 0.206, Average Reward 0.187, valReward 0.012, val Average Reward 0.140, Asset 1226988.12, Validation Asset 1002284.44, Average Validation Sharpe 0.97\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.032, Average Reward 0.058, valReward 0.149, val Average Reward 0.062, Asset 1029713.20, Validation Asset 1154132.93, Average Validation Sharpe 0.54\n",
      "Episode 200, Reward 0.028, Average Reward 0.093, valReward 0.117, val Average Reward 0.052, Asset 1028404.05, Validation Asset 1115434.46, Average Validation Sharpe 0.46\n",
      "Episode 300, Reward 0.254, Average Reward 0.138, valReward 0.046, val Average Reward 0.058, Asset 1285839.16, Validation Asset 1038197.13, Average Validation Sharpe 0.50\n",
      "Episode 400, Reward 0.186, Average Reward 0.148, valReward -0.022, val Average Reward 0.057, Asset 1203079.04, Validation Asset 972482.22, Average Validation Sharpe 0.49\n",
      "Episode 500, Reward 0.329, Average Reward 0.179, valReward 0.043, val Average Reward 0.052, Asset 1384572.65, Validation Asset 1032942.53, Average Validation Sharpe 0.45\n"
     ]
    }
   ],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))\n",
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, codes_dict=codes_dict, train=True, code=code, time_period = time_period, codes=codes)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, codes_dict=codes_dict, train=False, code=code, time_period = time_period,  codes=codes)\n",
    "    agent = Agent(2*3, env.action_space, 64, 0.001, 0.001, 0.99, 51, -0.1, 0.1, 'cuda', True,personality = 1)\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
