{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "time_period = 2\n",
    "sys.path.append('./')\n",
    "from utlis import get_data, Stock_Env\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import deque\n",
    "    \n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, N, hidden=[64, 64]):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size*N)\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.N = N\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state #(batch_size, state_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #(batch_size, action_size*N)\n",
    "        x = x.view(-1, self.action_size, self.N) #(batch_size, action_size, N)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:01<00:00, 434.43it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 251/251 [00:00<00:00, 488.46it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train, stock_df_test, stock_df_train_, stock_df_test_, codes = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, N, kappa, device, visual=False):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.tau = torch.linspace(0, 1, N+1)\n",
    "        self.tau = (self.tau[:-1] + self.tau[1:]) / 2\n",
    "        self.tau_hat = self.tau.to(device).unsqueeze(0) #(1, N)\n",
    "        self.tau = tau\n",
    "        self.kappa = kappa\n",
    "\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size, N).to(self.device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size, N).to(self.device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state)\n",
    "                action_values = action_values.mean(dim=-1, keepdim=False).view(-1)\n",
    "            return np.argmax(action_values.cpu().numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        quantiles_local = self.Q_local(states) #(batch_size, action_size, N)\n",
    "        quantiles_local = torch.gather(input=quantiles_local, dim=1, index=actions.unsqueeze(1).repeat(1, 1, self.N)) #(batch_size, 1, N)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            quantiles_target = self.Q_target(next_states) #(batch_size, action_size, N)\n",
    "            next_actions = torch.max(quantiles_target.sum(dim=2, keepdim=True), dim=1, keepdim=True)[1] #(batch_size, 1, 1)\n",
    "            quantiles_target = torch.gather(input=quantiles_target, index=next_actions.repeat(1,1,self.N), dim=1) #(batch_size, 1, N)\n",
    "            quantiles_target = rewards.unsqueeze(1).repeat(1, 1, self.N) + self.gamma * (1 - dones.unsqueeze(1).repeat(1, 1, self.N)) * quantiles_target #(batch_size, 1, N)\n",
    "            quantiles_target = quantiles_target.permute(0, 2, 1) #(batch_size, N, 1)\n",
    "\n",
    "        diff = quantiles_target - quantiles_local #(batch_size, N, N)\n",
    "        tau = self.tau_hat.unsqueeze(0).repeat(diff.size(0), 1, 1) #(batch_size, 1, N)\n",
    "        loss = (tau - (diff<0).float()).abs() * self.huber(diff) #(batch_size, N, N)\n",
    "        loss = loss.mean(dim=2, keepdim=False).sum(dim=1, keepdim=False) #(batch_size,)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.tau)\n",
    "\n",
    "    def huber(self, u):\n",
    "        if self.kappa > 0:\n",
    "            flag = (u.abs()<self.kappa).float()\n",
    "            loss = 0.5*u.pow(2) * flag + self.kappa*(u.abs()-0.5*self.kappa) * (1-flag)\n",
    "        else:\n",
    "            loss = u.abs()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    state = env.reset()\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        next_state = frame\n",
    "        state = next_state.copy()\n",
    "        episodic_reward += reward\n",
    "        rewards_log.append(reward)\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "        \n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.221, Average Reward 0.033, valReward 0.419, val Average Reward 0.271, Asset 1240190.76, Validation Asset 1502312.59, Average Validation Sharpe 1.95\n",
      "Episode 200, Reward 0.118, Average Reward 0.086, valReward 0.254, val Average Reward 0.280, Asset 1121304.46, Validation Asset 1275956.42, Average Validation Sharpe 2.05\n",
      "Episode 300, Reward 0.181, Average Reward 0.124, valReward 0.367, val Average Reward 0.262, Asset 1190947.60, Validation Asset 1427712.22, Average Validation Sharpe 1.90\n",
      "Episode 400, Reward 0.312, Average Reward 0.149, valReward 0.280, val Average Reward 0.264, Asset 1357634.08, Validation Asset 1308648.77, Average Validation Sharpe 1.91\n",
      "Episode 500, Reward 0.126, Average Reward 0.153, valReward 0.303, val Average Reward 0.275, Asset 1131579.34, Validation Asset 1341796.58, Average Validation Sharpe 2.02\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.007, Average Reward 0.071, valReward 0.122, val Average Reward -0.017, Asset 1007099.57, Validation Asset 1102209.38, Average Validation Sharpe -0.09\n",
      "Episode 200, Reward 0.108, Average Reward 0.151, valReward -0.047, val Average Reward -0.023, Asset 1107261.82, Validation Asset 939155.29, Average Validation Sharpe -0.11\n",
      "Episode 300, Reward 0.327, Average Reward 0.219, valReward -0.083, val Average Reward -0.022, Asset 1373336.97, Validation Asset 903955.93, Average Validation Sharpe -0.11\n",
      "Episode 400, Reward 0.038, Average Reward 0.264, valReward 0.085, val Average Reward -0.019, Asset 1038334.46, Validation Asset 1070168.55, Average Validation Sharpe -0.10\n",
      "Episode 500, Reward 0.203, Average Reward 0.321, valReward -0.033, val Average Reward -0.050, Asset 1220518.49, Validation Asset 948654.03, Average Validation Sharpe -0.26\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.026, Average Reward 0.021, valReward 0.018, val Average Reward 0.053, Asset 1023811.02, Validation Asset 1011942.18, Average Validation Sharpe 0.43\n",
      "Episode 200, Reward 0.091, Average Reward 0.058, valReward -0.009, val Average Reward 0.044, Asset 1093198.52, Validation Asset 983311.52, Average Validation Sharpe 0.35\n",
      "Episode 300, Reward 0.111, Average Reward 0.080, valReward 0.066, val Average Reward 0.046, Asset 1114737.46, Validation Asset 1051788.08, Average Validation Sharpe 0.38\n",
      "Episode 400, Reward 0.046, Average Reward 0.095, valReward 0.168, val Average Reward 0.046, Asset 1046878.64, Validation Asset 1168721.55, Average Validation Sharpe 0.36\n",
      "Episode 500, Reward 0.269, Average Reward 0.123, valReward 0.177, val Average Reward 0.049, Asset 1302577.98, Validation Asset 1184967.85, Average Validation Sharpe 0.40\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.111, Average Reward 0.081, valReward -0.116, val Average Reward -0.019, Asset 1112152.95, Validation Asset 881347.39, Average Validation Sharpe -0.14\n",
      "Episode 200, Reward 0.094, Average Reward 0.125, valReward -0.005, val Average Reward -0.004, Asset 1095732.94, Validation Asset 988387.74, Average Validation Sharpe -0.02\n",
      "Episode 300, Reward 0.006, Average Reward 0.172, valReward -0.010, val Average Reward -0.009, Asset 1005821.58, Validation Asset 982428.04, Average Validation Sharpe -0.07\n",
      "Episode 400, Reward 0.152, Average Reward 0.195, valReward 0.082, val Average Reward -0.008, Asset 1156337.48, Validation Asset 1074956.20, Average Validation Sharpe -0.06\n",
      "Episode 500, Reward 0.228, Average Reward 0.230, valReward 0.036, val Average Reward -0.015, Asset 1250383.32, Validation Asset 1029931.93, Average Validation Sharpe -0.12\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.103, Average Reward 0.089, valReward 0.137, val Average Reward 0.151, Asset 1106135.25, Validation Asset 1131252.60, Average Validation Sharpe 1.01\n",
      "Episode 200, Reward 0.163, Average Reward 0.120, valReward 0.164, val Average Reward 0.130, Asset 1174380.87, Validation Asset 1158597.59, Average Validation Sharpe 0.88\n",
      "Episode 300, Reward 0.270, Average Reward 0.131, valReward 0.122, val Average Reward 0.144, Asset 1305105.76, Validation Asset 1115850.96, Average Validation Sharpe 0.96\n",
      "Episode 400, Reward 0.056, Average Reward 0.156, valReward 0.068, val Average Reward 0.144, Asset 1056376.81, Validation Asset 1055642.57, Average Validation Sharpe 0.97\n",
      "Episode 500, Reward 0.014, Average Reward 0.137, valReward 0.080, val Average Reward 0.147, Asset 1014172.45, Validation Asset 1066335.76, Average Validation Sharpe 0.99\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, codes_dict=codes_dict, train=True, code=code, time_period = time_period, codes=codes)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, codes_dict=codes_dict, train=False, code=code, time_period = time_period,  codes=codes)\n",
    "    agent = Agent(2*5, env.action_space, 64, 0.001, 0.001, 0.99, 51, 1, 'cuda', True)\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
