{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "time_period = 2\n",
    "sys.path.append('./')\n",
    "from utlis import get_data, Stock_Env\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, N, Vmin, Vmax, hidden=[64, 64]):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size*N)\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.N = N\n",
    "        self.values = torch.linspace(Vmin, Vmax, N).view(1, 1, -1).to('cuda') #(1, 1, N)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state #(batch_size, state_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #(batch_size, action_size*N)\n",
    "        x = x.view(-1, self.action_size, self.N) #(batch_size, action_size, N)\n",
    "        log_probs = F.log_softmax(x, dim=2) #(batch_size, action_size, N)\n",
    "        Q_values = log_probs.exp() * self.values #(batch_size, action_size, N)\n",
    "        Q_values = Q_values.sum(dim=2, keepdims=False) #(batch_size, action_size)\n",
    "\n",
    "        return log_probs, Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:01<00:00, 436.32it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 251/251 [00:00<00:00, 492.28it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train, stock_df_test, stock_df_train_, stock_df_test_, codes = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from networks import *\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, N, Vmin, Vmax, device, visual=False, personality=1):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.Vmin = Vmin\n",
    "        self.Vmax = Vmax\n",
    "        self.vals = torch.linspace(Vmin, Vmax, N).to(device)\n",
    "        self.unit = (Vmax - Vmin) / (N - 1)\n",
    "        self.personality=personality\n",
    "\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size, N, Vmin, Vmax).to(self.device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size, N, Vmin, Vmax).to(self.device)\n",
    "\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                _, action_values = self.Q_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "        rewards[rewards<0] = rewards[rewards<0]*self.personality\n",
    "        # print(states)\n",
    "        # print(self.Q_local)\n",
    "        log_probs, _ = self.Q_local(states) #(batch_size, action_size, N)\n",
    "        log_probs = torch.gather(input=log_probs, dim=1, index=actions.unsqueeze(1).repeat(1, 1, self.N)) #(batch_size, 1, N)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            log_probs_targets, Q_targets = self.Q_target(next_states)\n",
    "            _, actions_target = torch.max(input=Q_targets, dim=1, keepdim=True)#(batch_size, 1) the same size as actions\n",
    "            log_probs_targets = torch.gather(input=log_probs_targets, dim=1, index=actions_target.unsqueeze(1).repeat(1, 1, self.N))\n",
    "            target_distribution = self.update_distribution(log_probs_targets.exp(), rewards, dones) #(batch_size, 1, N)\n",
    "\n",
    "        loss = -target_distribution*log_probs #D_KL(target||local)\n",
    "        #loss = -log_probs.exp()*((target_distribution+1e-9).log() - log_probs) #D_KL(local||target)\n",
    "\n",
    "        loss = loss.sum(dim=2, keepdims=False).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_distribution(self, old_distribution, reward, dones):\n",
    "        with torch.no_grad():\n",
    "            reward = reward.view(-1, 1)\n",
    "            batch_size = reward.size(0)\n",
    "            assert old_distribution.size(0) == batch_size\n",
    "            new_vals = self.vals.view(1, -1) * self.gamma * (1-dones) + reward\n",
    "            new_vals = torch.clamp(new_vals, self.Vmin, self.Vmax)\n",
    "            lower = torch.floor((new_vals - self.Vmin) / self.unit).long().to(self.device)\n",
    "            upper = torch.min(lower + 1, other=torch.tensor(self.N - 1)).to(self.device)\n",
    "            lower_vals = self.vals[lower]\n",
    "            lower_probs = 1 - torch.min((new_vals - lower_vals) / self.unit, other=torch.tensor(1, dtype=torch.float32)).to(self.device)\n",
    "            transit = torch.zeros((batch_size, self.N, self.N)).to(self.device)\n",
    "            first_dim = torch.tensor(range(batch_size), dtype=torch.long).view(-1, 1).repeat(1, self.N).view(-1).to(self.device)\n",
    "            second_dim = torch.tensor(range(self.N), dtype=torch.long).repeat(batch_size).to(self.device)\n",
    "            transit[first_dim, second_dim, lower.view(-1)] += lower_probs.view(-1)\n",
    "            transit[first_dim, second_dim, upper.view(-1)] += 1 - lower_probs.view(-1)\n",
    "            if len(old_distribution.size()) == 2:\n",
    "                old_distribution = old_distribution.unsqueeze(1)\n",
    "            return torch.bmm(old_distribution, transit)\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    state = env.reset()\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        next_state = frame\n",
    "        state = next_state.copy()\n",
    "        episodic_reward += reward\n",
    "        rewards_log.append(reward)\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "        \n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes_dict = dict(zip(codes, range(len(codes))))\n",
    "# train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.040, Average Reward 0.053, valReward 0.309, val Average Reward 0.258, Asset 1038651.19, Validation Asset 1348176.98, Average Validation Sharpe 1.88\n",
      "Episode 200, Reward -0.013, Average Reward 0.103, valReward 0.289, val Average Reward 0.264, Asset 987042.41, Validation Asset 1323401.92, Average Validation Sharpe 1.92\n",
      "Episode 300, Reward 0.370, Average Reward 0.149, valReward 0.217, val Average Reward 0.275, Asset 1440195.55, Validation Asset 1228941.25, Average Validation Sharpe 1.97\n",
      "Episode 400, Reward 0.013, Average Reward 0.207, valReward 0.244, val Average Reward 0.273, Asset 1012877.57, Validation Asset 1261734.37, Average Validation Sharpe 1.97\n",
      "Episode 500, Reward 0.444, Average Reward 0.211, valReward 0.259, val Average Reward 0.250, Asset 1550176.63, Validation Asset 1286109.59, Average Validation Sharpe 1.83\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.191, Average Reward 0.068, valReward -0.074, val Average Reward -0.022, Asset 1194699.66, Validation Asset 914661.65, Average Validation Sharpe -0.11\n",
      "Episode 200, Reward 0.124, Average Reward 0.193, valReward 0.003, val Average Reward -0.018, Asset 1127613.45, Validation Asset 987015.09, Average Validation Sharpe -0.099\n",
      "Episode 300, Reward 0.234, Average Reward 0.290, valReward -0.018, val Average Reward -0.035, Asset 1259498.65, Validation Asset 965535.43, Average Validation Sharpe -0.18\n",
      "Episode 400, Reward 0.050, Average Reward 0.331, valReward -0.012, val Average Reward -0.038, Asset 1050568.83, Validation Asset 967302.37, Average Validation Sharpe -0.19\n",
      "Episode 500, Reward 0.797, Average Reward 0.389, valReward 0.158, val Average Reward -0.061, Asset 2183945.63, Validation Asset 1146608.64, Average Validation Sharpe -0.31\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.002, Average Reward 0.021, valReward 0.061, val Average Reward 0.063, Asset 1001150.85, Validation Asset 1047134.87, Average Validation Sharpe 0.50\n",
      "Episode 200, Reward 0.084, Average Reward 0.048, valReward 0.010, val Average Reward 0.039, Asset 1086898.23, Validation Asset 1003765.38, Average Validation Sharpe 0.33\n",
      "Episode 300, Reward -0.006, Average Reward 0.085, valReward 0.127, val Average Reward 0.057, Asset 994142.69, Validation Asset 1124520.82, Average Validation Sharpe 0.48\n",
      "Episode 400, Reward 0.086, Average Reward 0.105, valReward 0.013, val Average Reward 0.044, Asset 1088058.05, Validation Asset 1000508.67, Average Validation Sharpe 0.35\n",
      "Episode 500, Reward 0.160, Average Reward 0.135, valReward 0.038, val Average Reward 0.042, Asset 1170776.03, Validation Asset 1031071.32, Average Validation Sharpe 0.32\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.068, Average Reward 0.088, valReward -0.083, val Average Reward -0.005, Asset 1067459.55, Validation Asset 913125.17, Average Validation Sharpe -0.04\n",
      "Episode 200, Reward 0.171, Average Reward 0.145, valReward 0.008, val Average Reward -0.008, Asset 1175494.96, Validation Asset 1000101.30, Average Validation Sharpe -0.06\n",
      "Episode 300, Reward -0.011, Average Reward 0.184, valReward -0.073, val Average Reward -0.005, Asset 988391.92, Validation Asset 922369.64, Average Validation Sharpe -0.03\n",
      "Episode 400, Reward 0.040, Average Reward 0.233, valReward -0.056, val Average Reward -0.006, Asset 1040931.65, Validation Asset 940497.07, Average Validation Sharpe -0.05\n",
      "Episode 500, Reward 0.206, Average Reward 0.268, valReward 0.050, val Average Reward -0.003, Asset 1224374.95, Validation Asset 1042195.22, Average Validation Sharpe -0.02\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.067, Average Reward 0.093, valReward 0.260, val Average Reward 0.152, Asset 1068228.87, Validation Asset 1276948.64, Average Validation Sharpe 1.03\n",
      "Episode 200, Reward 0.147, Average Reward 0.127, valReward 0.282, val Average Reward 0.146, Asset 1155384.71, Validation Asset 1303423.95, Average Validation Sharpe 0.98\n",
      "Episode 300, Reward 0.184, Average Reward 0.143, valReward 0.056, val Average Reward 0.147, Asset 1199654.22, Validation Asset 1044895.29, Average Validation Sharpe 0.99\n",
      "Episode 400, Reward 0.006, Average Reward 0.153, valReward 0.035, val Average Reward 0.125, Asset 1006059.59, Validation Asset 1027489.79, Average Validation Sharpe 0.85\n",
      "Episode 500, Reward 0.196, Average Reward 0.149, valReward 0.147, val Average Reward 0.146, Asset 1213155.33, Validation Asset 1139790.18, Average Validation Sharpe 0.99\n"
     ]
    }
   ],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))\n",
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, codes_dict=codes_dict, train=True, code=code, time_period = time_period, codes=codes)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, codes_dict=codes_dict, train=False, code=code, time_period = time_period,  codes=codes)\n",
    "    agent = Agent(2*5, env.action_space, 64, 0.001, 0.001, 0.99, 51, -0.1, 0.1, 'cuda', True,personality = 1)\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
