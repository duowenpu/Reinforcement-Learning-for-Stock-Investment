{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "time_period = 2\n",
    "sys.path.append('./')\n",
    "from utlis import get_data, Stock_Env\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import deque\n",
    "\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden=[64, 64], duel=False):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size)\n",
    "        self.duel = duel\n",
    "        if self.duel:\n",
    "            self.fc4 = nn.Linear(hidden[1], 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if self.duel:\n",
    "            x1 = self.fc3(x)\n",
    "            x1 = x1 - torch.max(x1, dim=1, keepdim=True)[0] # set the max to be 0\n",
    "            x2 = self.fc4(x)\n",
    "            # print(x1.shape, x2.shape)\n",
    "            return x1 + x2\n",
    "        else:\n",
    "            x = self.fc3(x)\n",
    "            # print(x.shape)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:01<00:00, 432.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 251/251 [00:00<00:00, 470.16it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train, stock_df_test, stock_df_train_, stock_df_test_, codes = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        # the first capacity-1 positions are not leaves\n",
    "        self.vals = [0 for _ in range(2*capacity - 1)] # think about why if you are not familiar with this\n",
    "        \n",
    "    def retrive(self, num):\n",
    "        '''\n",
    "        This function find the first index whose cumsum is no smaller than num\n",
    "        '''\n",
    "        ind = 0 # search from root\n",
    "        while ind < self.capacity-1: # not a leaf\n",
    "            left = 2*ind + 1\n",
    "            right = left + 1\n",
    "            if num > self.vals[left]: # the sum of the whole left tree is not large enouth\n",
    "                num -= self.vals[left] # think about why?\n",
    "                ind = right\n",
    "            else: # search in the left tree\n",
    "                ind = left\n",
    "        return ind - self.capacity + 1\n",
    "    \n",
    "    def update(self, delta, ind):\n",
    "        '''\n",
    "        Change the value at ind by delta, and update the tree\n",
    "        Notice that this ind should be the index in real memory part, instead of the ind in self.vals\n",
    "        '''\n",
    "        ind += self.capacity - 1\n",
    "        while True:\n",
    "            self.vals[ind] += delta\n",
    "            if ind == 0:\n",
    "                break\n",
    "            ind -= 1\n",
    "            ind //= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import deque\n",
    "\n",
    "# test = deque(maxlen=5)\n",
    "# for i in range(10):\n",
    "#     test.append(i)\n",
    "#     print(test)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import bisect\n",
    "import torch\n",
    "\n",
    "ALPHA = 0.5\n",
    "EPSILON = 0.05\n",
    "TD_INIT = 1\n",
    "\n",
    "class Replay_Buffer:\n",
    "    '''\n",
    "    Vanilla replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=None):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.memory = [None for _ in range(capacity)] # save tuples (state, action, reward, next_state, done)\n",
    "        self.ind_max = 0 # how many transitions have been stored\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        ind = self.ind_max % self.capacity\n",
    "        self.memory[ind] = (state, action, reward, next_state, done)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, k):\n",
    "        '''\n",
    "        return sampled transitions. Make sure that there are at least k transitions stored before calling this method \n",
    "        '''\n",
    "        index_set = random.sample(list(range(len(self))), k)\n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.ind_max, self.capacity)\n",
    "        \n",
    "class Rank_Replay_Buffer:\n",
    "    '''\n",
    "    Rank-based replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=64):\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = ALPHA\n",
    "        self.memory = [None for _ in range(capacity)]\n",
    "        self.segments = [-1] + [None for _ in range(batch_size)] # the ith index will be in [segments[i-1]+1, segments[i]]\n",
    "        \n",
    "        self.errors = [] # saves (-TD_error, index of transition), sorted\n",
    "        self.memory_to_rank = [None for _ in range(capacity)]\n",
    "        \n",
    "        self.ind_max = 0 # how many transitions have been stored\n",
    "        self.total_weights = 0 # sum of p_i\n",
    "        self.cumulated_weights = []\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        index = self.ind_max % self.capacity\n",
    "        if self.ind_max >= self.capacity: # memory is full, need to pop\n",
    "            self.pop(index)\n",
    "        else: # memory is not full, need to adjust weights and find segment points\n",
    "            self.total_weights += (1/(1+self.ind_max))**self.alpha # memory is not full, calculate new weights\n",
    "            self.cumulated_weights.append(self.total_weights)\n",
    "            self.update_segments()\n",
    "        \n",
    "        max_error = -self.errors[0][0] if self.errors else 0\n",
    "        self.insert(max_error, index)\n",
    "        self.memory[index] = (state, action, reward, next_state, done)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, batch_size=None): # notive that batch_size is not used. It's just to unify the calling form\n",
    "        index_set = [random.randint(self.segments[i]+1, self.segments[i+1]) for i in range(self.batch_size)]\n",
    "        probs = torch.from_numpy(np.vstack([(1/(1+ind))**self.alpha/self.total_weights for ind in index_set])).float()\n",
    "        \n",
    "        index_set = [self.errors[ind][1] for ind in index_set]\n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "        for ind in index_set:\n",
    "            self.pop(ind)\n",
    "        \n",
    "        return index_set, states, actions, rewards, next_states, dones, probs\n",
    "    \n",
    "    def insert(self, error, index):\n",
    "        '''\n",
    "        Input : \n",
    "            error : the TD-error of this transition\n",
    "            index : the location of this transition\n",
    "        insert error into self.errors, update self.memory_to_rank and self.rank_to_memory accordingly\n",
    "        '''\n",
    "        ind = bisect.bisect(self.errors, (-error, index))\n",
    "        self.memory_to_rank[index] = ind\n",
    "        self.errors.insert(ind, (-error, index))\n",
    "        for i in range(ind+1, len(self.errors)):\n",
    "            self.memory_to_rank[self.errors[i][1]] += 1\n",
    "        \n",
    "    def pop(self, index):\n",
    "        '''\n",
    "        Input :\n",
    "            index : the location of a transition\n",
    "        remove this transition, update self.memory_to_rank and self.rank_to_memory accordingly\n",
    "        '''\n",
    "        ind = self.memory_to_rank[index]\n",
    "        self.memory_to_rank[index] = None\n",
    "        self.errors.pop(ind)\n",
    "        for i in range(ind, len(self.errors)):\n",
    "            self.memory_to_rank[self.errors[i][1]] -= 1\n",
    "        \n",
    "    def update_segments(self):\n",
    "        '''\n",
    "        Update the segment points.\n",
    "        '''\n",
    "        if self.ind_max+1 < self.batch_size: # if there is no enough transitions\n",
    "            return None\n",
    "        for i in range(self.batch_size):\n",
    "            ind = bisect.bisect_left(self.cumulated_weights, self.total_weights*((i+1)/self.batch_size))\n",
    "            self.segments[i+1] = max(ind, self.segments[i]+1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return min(self.capacity, self.ind_max)\n",
    "    \n",
    "\n",
    "class Proportion_Replay_Buffer:\n",
    "    '''\n",
    "    Proportion-based replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=None):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = ALPHA\n",
    "        self.memory = [None for _ in range(capacity)]\n",
    "        self.weights = SumTree(self.capacity)\n",
    "        self.default = TD_INIT\n",
    "        self.ind_max = 0\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        index = self.ind_max % self.capacity\n",
    "        self.memory[index] = (state, action, reward, next_state, done)\n",
    "        delta = self.default+EPSILON - self.weights.vals[index+self.capacity-1]\n",
    "        self.weights.update(delta, index)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        index_set = [self.weights.retrive(self.weights.vals[0]*random.random()) for _ in range(batch_size)]\n",
    "        #print(index_set)\n",
    "        probs = torch.from_numpy(np.vstack([self.weights.vals[ind+self.capacity-1]/self.weights.vals[0] for ind in index_set])).float()                     \n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "\n",
    "        return index_set, states, actions, rewards, next_states, dones, probs\n",
    "                                 \n",
    "    def insert(self, error, index):\n",
    "        delta = error+EPSILON - self.weights.vals[index+self.capacity-1]\n",
    "        self.weights.update(delta, index)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return min(self.capacity, self.ind_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = None\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# from networks import *\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, device, visual=False):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size,duel=False).to(device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size,duel=False).to(device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = Proportion_Replay_Buffer(int(1e5), bs)\n",
    "        self.tst = None\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        index_set, states, actions, rewards, next_states, dones, probs = self.memory.sample(self.bs)\n",
    "        w = 1/len(self.memory)/probs\n",
    "        w = w/torch.max(w)\n",
    "        w = w.to(self.device)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        # print(states.shape)\n",
    "        Q_values = self.Q_local(states)\n",
    "        # print(actions.shape)\n",
    "        Q_values = torch.gather(input=Q_values, dim=-1, index=actions)\n",
    "        with torch.no_grad():\n",
    "            Q_targets = self.Q_target(next_states)\n",
    "            Q_targets, _ = torch.max(input=Q_targets, dim=-1, keepdim=True)\n",
    "            Q_targets = rewards + self.gamma * (1 - dones) * Q_targets\n",
    "    \n",
    "        deltas = Q_values - Q_targets\n",
    "        loss = (w*deltas.pow(2)).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        deltas = np.abs(deltas.detach().cpu().numpy().reshape(-1))\n",
    "        for i in range(self.bs):\n",
    "            self.memory.insert(deltas[i], index_set[i])\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.995\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    # agent.mu=0\n",
    "    env.mu=[0]\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    state = frame.reshape(-1)\n",
    "    t = 0\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        rewards_log.append(reward)\n",
    "        episodic_reward += reward\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset().reshape(-1)\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        # state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            frame = frame.reshape(-1)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            # next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "\n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward -0.000, Average Reward 0.036, valReward 0.141, val Average Reward 0.270, Asset 999201.17, Validation Asset 1141927.06, Average Validation Sharpe 1.95\n",
      "Episode 200, Reward -0.002, Average Reward 0.075, valReward 0.324, val Average Reward 0.269, Asset 997728.99, Validation Asset 1368835.87, Average Validation Sharpe 1.95\n",
      "Episode 300, Reward 0.197, Average Reward 0.115, valReward 0.206, val Average Reward 0.268, Asset 1212834.83, Validation Asset 1218419.70, Average Validation Sharpe 1.94\n",
      "Episode 400, Reward 0.239, Average Reward 0.181, valReward 0.197, val Average Reward 0.260, Asset 1265412.61, Validation Asset 1207763.34, Average Validation Sharpe 1.87\n",
      "Episode 500, Reward 0.098, Average Reward 0.183, valReward 0.303, val Average Reward 0.278, Asset 1101351.76, Validation Asset 1342021.37, Average Validation Sharpe 2.01\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.115, Average Reward 0.088, valReward -0.036, val Average Reward -0.024, Asset 1116766.97, Validation Asset 946394.25, Average Validation Sharpe -0.12\n",
      "Episode 200, Reward 0.014, Average Reward 0.154, valReward -0.007, val Average Reward -0.028, Asset 1013395.84, Validation Asset 976385.86, Average Validation Sharpe -0.14\n",
      "Episode 300, Reward 0.058, Average Reward 0.296, valReward -0.044, val Average Reward -0.030, Asset 1059065.40, Validation Asset 938451.89, Average Validation Sharpe -0.16\n",
      "Episode 400, Reward 0.233, Average Reward 0.363, valReward 0.051, val Average Reward -0.040, Asset 1255291.37, Validation Asset 1035186.61, Average Validation Sharpe -0.20\n",
      "Episode 500, Reward 0.272, Average Reward 0.344, valReward -0.264, val Average Reward -0.029, Asset 1305425.90, Validation Asset 752145.43, Average Validation Sharpe -0.15\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.031, Average Reward 0.021, valReward 0.037, val Average Reward 0.051, Asset 1030141.98, Validation Asset 1031035.56, Average Validation Sharpe 0.44\n",
      "Episode 200, Reward 0.068, Average Reward 0.045, valReward 0.084, val Average Reward 0.055, Asset 1067839.69, Validation Asset 1079379.15, Average Validation Sharpe 0.44\n",
      "Episode 300, Reward 0.122, Average Reward 0.066, valReward 0.102, val Average Reward 0.037, Asset 1127670.15, Validation Asset 1097441.91, Average Validation Sharpe 0.29\n",
      "Episode 400, Reward 0.064, Average Reward 0.080, valReward 0.017, val Average Reward 0.062, Asset 1065554.84, Validation Asset 1009234.27, Average Validation Sharpe 0.50\n",
      "Episode 500, Reward 0.114, Average Reward 0.104, valReward 0.068, val Average Reward 0.046, Asset 1119402.18, Validation Asset 1059338.07, Average Validation Sharpe 0.38\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.111, Average Reward 0.055, valReward -0.044, val Average Reward 0.004, Asset 1112129.76, Validation Asset 948281.67, Average Validation Sharpe 0.03\n",
      "Episode 200, Reward 0.143, Average Reward 0.103, valReward 0.014, val Average Reward -0.010, Asset 1146604.70, Validation Asset 1004815.57, Average Validation Sharpe -0.08\n",
      "Episode 300, Reward 0.343, Average Reward 0.168, valReward -0.010, val Average Reward -0.019, Asset 1400306.26, Validation Asset 983146.16, Average Validation Sharpe -0.15\n",
      "Episode 400, Reward 0.365, Average Reward 0.197, valReward -0.021, val Average Reward -0.006, Asset 1432211.21, Validation Asset 971904.73, Average Validation Sharpe -0.04\n",
      "Episode 500, Reward 0.196, Average Reward 0.226, valReward -0.055, val Average Reward 0.003, Asset 1212606.55, Validation Asset 938398.74, Average Validation Sharpe 0.0200\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.000, Average Reward 0.084, valReward -0.026, val Average Reward 0.129, Asset 1000000.00, Validation Asset 963941.24, Average Validation Sharpe 0.88\n",
      "Episode 200, Reward 0.116, Average Reward 0.096, valReward 0.291, val Average Reward 0.147, Asset 1120408.25, Validation Asset 1319167.39, Average Validation Sharpe 0.97\n",
      "Episode 300, Reward 0.201, Average Reward 0.126, valReward 0.154, val Average Reward 0.155, Asset 1219580.20, Validation Asset 1151823.18, Average Validation Sharpe 1.06\n",
      "Episode 400, Reward 0.141, Average Reward 0.135, valReward 0.201, val Average Reward 0.154, Asset 1150294.45, Validation Asset 1208664.52, Average Validation Sharpe 1.04\n",
      "Episode 500, Reward 0.126, Average Reward 0.132, valReward 0.255, val Average Reward 0.153, Asset 1132984.44, Validation Asset 1267028.84, Average Validation Sharpe 1.05\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, codes_dict=codes_dict, train=True, code=code, time_period = time_period, codes=codes)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, codes_dict=codes_dict, train=False, code=code, time_period = time_period,  codes=codes)\n",
    "    agent = Agent(2*5, 11, 64, 0.001, 0.001, 0.99, 'cuda', True)\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
