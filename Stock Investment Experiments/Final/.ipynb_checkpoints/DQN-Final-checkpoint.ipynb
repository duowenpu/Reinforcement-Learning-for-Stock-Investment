{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "time_period = 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import deque\n",
    "\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden=[64, 64]):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# class Q_Network(nn.Module):\n",
    "#     '''\n",
    "#     The input of this network should have shape (num_frame, 80, 80)\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self, num_frame, num_action):\n",
    "#         super(Q_Network, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=num_frame, out_channels=32, kernel_size=(2,1), stride=1, padding=2)  # 16, 20, 20\n",
    "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "#         self.conv4 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "#         self.conv5 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,2), stride=1)  # 32, 9, 9\n",
    "#         self.pool = nn.AvgPool2d(kernel_size=(2,1))\n",
    "#         self.fc1 = nn.Linear(672, 256)\n",
    "#         self.fc2 = nn.Linear(256, num_action)\n",
    "#         self.sf = nn.Softmax()\n",
    "\n",
    "#     def forward(self, image):\n",
    "#         x = F.relu(self.pool(self.conv1(image)))\n",
    "#         x = F.relu(self.pool(self.conv2(x)))\n",
    "#         x = F.relu(self.pool(self.conv3(x)))\n",
    "#         x = x.view(-1, 672)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.sf(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('../../test_data3.csv')\n",
    "codes = data['symbol'].unique()\n",
    "stock_df = data\n",
    "stock_df = stock_df[['Date','symbol','Open','High','Low','Close','Volume','Dividends','Stock Splits','Pctchange', 'Neg','Neu','Pos']]\n",
    "stock_df.columns = ['date', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'dividends','stock splits', 'pctchange','Neg', 'Neu', 'Pos']\n",
    "stock_df['pctchange'] = (stock_df['close'] - stock_df['open'])/stock_df['open']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finta import TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df['SMA42'] = TA.SMA(stock_df, 42)\n",
    "stock_df['SMA5'] = TA.SMA(stock_df, 5)\n",
    "stock_df['SMA15'] = TA.SMA(stock_df, 15)\n",
    "stock_df['AO'] = TA.AO(stock_df)\n",
    "stock_df['OVB'] = TA.OBV(stock_df)\n",
    "stock_df[['VW_MACD','MACD_SIGNAL']] = TA.VW_MACD(stock_df)\n",
    "stock_df['RSI'] = TA.RSI(stock_df)\n",
    "stock_df['CMO'] = TA.CMO(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df_train = stock_df[stock_df['date']<='2019-01-01'].groupby(['date','symbol']).agg('mean')\n",
    "# stock_df_train = stock_df_train[stock_df_train['date']>='2023-01-01']\n",
    "stock_df_test = stock_df[stock_df['date']>'2019-01-01']\n",
    "stock_df_test = stock_df_test[stock_df_test['date']<='2019-12-31'].groupby(['date','symbol']).agg('mean')\n",
    "\n",
    "train_date = sorted([x[0] for x in stock_df_train.index])\n",
    "test_date = sorted([x[0] for x in stock_df_test.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:00<00:00, 719.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 726.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# indicators = ['open', 'high', 'low', 'close', 'volume', 'positive', 'neutral', 'negative','SMA42', 'SMA5', 'SMA15', 'AO', 'OVB','VW_MACD',\n",
    "#        'MACD_SIGNAL', 'RSI', 'CMO']\n",
    "\n",
    "indicators = ['Neg','Neu','Pos']\n",
    "\n",
    "# indicators = ['pctchange', 'volume', 'positive', 'neutral', 'negative']\n",
    "# indicators = ['positive', 'neutral', 'negative']\n",
    "# indicators = ['sentiment']\n",
    "\n",
    "from tqdm import tqdm\n",
    "def get_full_data(x, date):\n",
    "        full_df = pd.DataFrame(0, index = codes, columns = x.columns)\n",
    "        full_df.loc[set(full_df.index).intersection(set(x.index))] = x.loc[set(full_df.index).intersection(set(x.index))]\n",
    "        v = full_df.values.reshape(1,-1)\n",
    "        # full_df['date']=date\n",
    "        return [date]+list(v[0])\n",
    "    \n",
    "dates = np.unique([x[0] for x in stock_df_train.index])\n",
    "res = []\n",
    "for date in tqdm(dates):\n",
    "    x = stock_df_train[indicators].loc[date]\n",
    "    res.append(get_full_data(x, date))\n",
    "\n",
    "# res = pd.concat(res).reset_index()\n",
    "# res.columns = ['tic', 'open', 'high', 'low', 'close', 'volume', 'positive',\n",
    "#        'neutral', 'negative', 'pctchange', 'date']\n",
    "stock_df_train_ = pd.DataFrame(res).set_index(0)\n",
    "\n",
    "dates = np.unique([x[0] for x in stock_df_test.index])\n",
    "res = []\n",
    "for date in tqdm(dates):\n",
    "    x = stock_df_test[indicators].loc[date]\n",
    "    res.append(get_full_data(x, date))\n",
    "    \n",
    "# res = pd.concat(res).reset_index()\n",
    "# res.columns = ['tic', 'open', 'high', 'low', 'close', 'volume', 'positive',\n",
    "#        'neutral', 'negative', 'pctchange', 'date']\n",
    "stock_df_test_ = pd.DataFrame(res).set_index(0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "stock_df_train_1 = scaler.fit_transform(stock_df_train_)\n",
    "stock_df_test_1 = scaler.transform(stock_df_test_)\n",
    "\n",
    "stock_df_train_ = pd.DataFrame(stock_df_train_1, index = stock_df_train_.index, columns = stock_df_train_.columns)\n",
    "stock_df_test_ = pd.DataFrame(stock_df_test_1, index = stock_df_test_.index, columns = stock_df_test_.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = None\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# from networks import *\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, device):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size).to(device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size).to(device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.tst = None\n",
    "        self.mu = [0]\n",
    "        self.last_action = 0\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state).reshape(-1)\n",
    "            if (action_values).max() > np.max(self.mu):\n",
    "                # self.mu = 0.95*self.mu + 0.05*action_values.max()\n",
    "\n",
    "                self.mu.append(action_values.max().cpu().data.numpy())                \n",
    "                if len(self.mu) > 10:\n",
    "                    self.mu = self.mu[-10:]\n",
    "                self.last_action = np.argmax(action_values.cpu().data.numpy())\n",
    "                return self.last_action\n",
    "            else:\n",
    "                return self.last_action\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "            self.last_action = action\n",
    "            return action\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "        self.tst = states\n",
    "        Q_values = self.Q_local(states).reshape(-1,11)\n",
    "        Q_values = torch.gather(input=Q_values, dim=-1, index=actions)\n",
    "        with torch.no_grad():\n",
    "            Q_targets = self.Q_target(next_states)\n",
    "            Q_targets, _ = torch.max(input=Q_targets, dim=-1, keepdim=True)\n",
    "            Q_targets = rewards + self.gamma * (1 - dones) * Q_targets\n",
    "\n",
    "        loss = (Q_values - Q_targets).pow(2).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Stock_Env:\n",
    "#     def __init__(self, initial_asset, data, cost):\n",
    "#         self.asset = initial_asset\n",
    "#         self.cash = initial_asset\n",
    "#         self.stock = 0\n",
    "#         self.data = data\n",
    "#         self.time = data.iloc[time_period]['date']\n",
    "#         self.cost = cost\n",
    "#         self.history=[]\n",
    "#         self.total_cost = 0\n",
    "#         self.initial_asset = initial_asset\n",
    "#         self.rowid = time_period\n",
    "#         self.action_space = np.array(list(range(11)))\n",
    "    \n",
    "#     def reset(self):\n",
    "#         self.asset = self.initial_asset\n",
    "#         self.cash = self.initial_asset\n",
    "#         self.stock = 0\n",
    "#         self.time = self.data.iloc[100]['date']\n",
    "#         self.history=[]\n",
    "#         self.total_cost = 0    \n",
    "#         self.rowid = time_period\n",
    "#         return self.data[:time_period][indicators].values\n",
    "    \n",
    "#     def step(self, action):\n",
    "#         done = False\n",
    "#         states = self.data.iloc[self.rowid]        \n",
    "#         self.rowid +=1\n",
    "#         if self.rowid == len(self.data)-1:\n",
    "#             done = True\n",
    "#         next_state = self.data.iloc[self.rowid]\n",
    "#         last_asset = self.asset\n",
    "#         price = next_state['open']\n",
    "#         old_asset = self.cash + self.stock*price\n",
    "#         self.asset = old_asset\n",
    "#         target_value = action*0.1*self.asset\n",
    "#         distance = target_value - self.stock*price\n",
    "#         stock_distance = int(distance/(price*(1+self.cost)))\n",
    "#         self.stock += stock_distance\n",
    "#         self.cash = self.cash - distance - np.abs(stock_distance*self.cost*price)\n",
    "#         self.asset = self.cash+self.stock*price\n",
    "#         market_value = self.stock * next_state['close']\n",
    "#         self.asset = market_value + self.cash\n",
    "#         reward = self.asset - last_asset\n",
    "#         self.time = next_state['date']\n",
    "#         # self.stock = stock\n",
    "#         return (self.data[self.rowid-time_period:self.rowid][indicators].values, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Stock_Env:\n",
    "    def __init__(self, initial_asset, data, cost, time, record,train=True,market=False, code='AAPL'):\n",
    "        self.asset = initial_asset\n",
    "        self.cash = initial_asset\n",
    "        self.stock = 0\n",
    "        self.stockvalue = 0\n",
    "        self.data = data\n",
    "        self.time = np.unique(time)\n",
    "        self.cost = cost\n",
    "        self.totalday = 0\n",
    "        self.history=[]\n",
    "        self.total_cost = 0\n",
    "        self.initial_asset = initial_asset\n",
    "        self.timeid = time_period\n",
    "        self.rowid = self.time[time_period]\n",
    "        self.action_space = 11\n",
    "        self.codeid = pd.DataFrame(range(len(codes)), index=codes)\n",
    "        self.record = record\n",
    "        self.train=train\n",
    "        self.market=market\n",
    "        self.code = code\n",
    "    \n",
    "    def reset(self):\n",
    "        self.asset = self.initial_asset\n",
    "        self.cash = self.initial_asset\n",
    "        self.stock = 0\n",
    "        self.stockvalue = 0\n",
    "        self.history=[]\n",
    "        self.total_cost = 0\n",
    "        if self.train:\n",
    "            temp_time = np.random.randint(time_period, len(self.time)-252)\n",
    "            self.rowid = self.time[temp_time]\n",
    "            while (self.rowid, self.code) not in self.data.index:\n",
    "                temp_time = np.random.randint(time_period, len(self.time)-252)\n",
    "                self.rowid = self.time[temp_time]\n",
    "            self.timeid = temp_time\n",
    "            self.totalday = temp_time\n",
    "        else:\n",
    "            temp_time = time_period\n",
    "            self.rowid = self.time[temp_time]\n",
    "            self.timeid = temp_time\n",
    "            self.totalday = temp_time\n",
    "        self.totalday = temp_time\n",
    "        temp = self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1],codes_dict[self.code]*3+1:codes_dict[self.code]*3+3].values.reshape(1,-1)\n",
    "        # print(temp.shape, self.stockvalue.shape)\n",
    "        return temp\n",
    "        # for i in range(time_period):\n",
    "        #     temp.append(list(self.get_full_data(self.data.loc[self.time[temp_time-time_period+i+1]]).values.reshape(-1)))       \n",
    "        # return np.array(temp)\n",
    "    \n",
    "    def get_full_data(self,x):\n",
    "        full_df = pd.DataFrame(0, index = self.codes, columns = x.columns)\n",
    "        full_df.loc[set(full_df.index).intersection(set(x.index))] = x.loc[set(full_df.index).intersection(set(x.index))]\n",
    "        return full_df\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        # print(self.timeid, self.totalday)\n",
    "        states = self.data.loc[self.rowid, self.code]   \n",
    "        self.timeid +=1\n",
    "        self.rowid = self.time[self.timeid]\n",
    "        self.totalday += 1\n",
    "        while (self.rowid, self.code) not in self.data.index:\n",
    "            self.timeid +=1\n",
    "            if (self.timeid != len(self.time)-1):\n",
    "                self.rowid = self.time[self.timeid]\n",
    "                self.totalday += 1\n",
    "            else:\n",
    "                return np.zeros(time_period*3), 0, True\n",
    "        if (self.timeid == len(self.time)-1):\n",
    "            done = True\n",
    "        if (self.train==True) and (self.totalday>=251) :\n",
    "            done = True\n",
    "        next_state = self.data.loc[self.rowid, self.code]\n",
    "        last_asset = self.asset\n",
    "        price = next_state['open']\n",
    "        old_asset = self.cash + self.stock*price\n",
    "        self.asset = old_asset\n",
    "        target_value = action*0.1*self.asset\n",
    "        distance = target_value - self.stock*price\n",
    "        stock_distance = int(distance/(price*(1+self.cost)))\n",
    "        self.stock += stock_distance\n",
    "        self.cash = self.cash - distance - np.abs(stock_distance*self.cost*price)\n",
    "        self.asset = self.cash+self.stock*price\n",
    "        market_value = self.stock * next_state['close']\n",
    "        self.asset = market_value + self.cash\n",
    "        reward = self.asset - last_asset\n",
    "        reward = reward/last_asset\n",
    "        # self.stock = stock\n",
    "        # print(self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1]])\n",
    "        return (self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1], codes_dict[self.code]*3+1:codes_dict[self.code]*3+3].values.reshape(1,-1), reward, done)\n",
    "\n",
    "#     def step(self, action):\n",
    "#         done = False\n",
    "#         states = self.data.loc[self.rowid]        \n",
    "#         self.timeid +=1\n",
    "#         self.rowid = self.time[self.timeid]\n",
    "#         if (self.timeid == len(self.time)-1):\n",
    "#             done = True\n",
    "#         if (self.train==True) and (self.totalday>=252) :\n",
    "#             dont = True\n",
    "#         self.totalday+=1\n",
    "#         next_state = self.data.loc[self.rowid]\n",
    "#         last_asset = self.asset\n",
    "#         idx = self.codeid.loc[next_state.index].values.reshape(1,-1)\n",
    "#         # Calculate the total assets at the beginning of the next day\n",
    "#         self.stockvalue[idx] = self.stock[idx].reshape(1,-1)*next_state['open'].values.reshape(1,-1)\n",
    "#         old_asset = self.cash + self.stockvalue.sum()\n",
    "        \n",
    "#         self.asset = old_asset\n",
    "#         # Calculate the position for each stock and cash\n",
    "#         action = np.exp(action)/np.exp(action).sum()\n",
    "#         # Get the stock asset value, where the last value of action is the position of cash.\n",
    "#         stockvalue_ = old_asset * (1-action[-1])\n",
    "        \n",
    "#         # Adjust the postion\n",
    "#         target_value = action*old_asset\n",
    "#         distance = target_value[:-1] - self.stockvalue\n",
    "#         stock_distance = (distance[idx].reshape(-1))/((next_state['open'].values*(1+self.cost)).astype(int).reshape(-1))\n",
    "#         # stock_distance /= 5\n",
    "#         self.stock[idx] += stock_distance\n",
    "#         self.cash = self.cash - distance[idx].sum() - np.abs(stock_distance*self.cost*next_state['open'].values).sum()\n",
    "#         self.stockvalue[idx] = self.stock[idx] * next_state['close'].values\n",
    "            \n",
    "#         # Calculate new asset\n",
    "#         self.asset = self.stockvalue.sum() + self.cash\n",
    "        \n",
    "#         reward = (self.asset - last_asset)/self.initial_asset\n",
    "#         if self.market:\n",
    "#             reward -= next_state['market'].values.mean()\n",
    "        \n",
    "#         # Generate new states\n",
    "#         temp = self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1]].values\n",
    "#         temp = np.concatenate((temp, self.stockvalue/(self.asset)),axis=None)\n",
    "#         # print(temp.shape)\n",
    "#         # for i in range(time_period):\n",
    "#         #     temp.append(list(self.get_full_data(self.data.loc[self.time[self.timeid-time_period+i+1]]).values.reshape(-1)))\n",
    "#         return (temp, reward, done)\n",
    "#         # return (self.data[self.rowid-time_period:self.rowid][indicators].values, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31880\\2726654923.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'env_test' is not defined"
     ]
    }
   ],
   "source": [
    "len(env_test.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #env = gym.make()\n",
    "# env = Stock_Env(1000000, stock_df_train, 0.002, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, code='AAPL')\n",
    "# num_episode = 5\n",
    "# max_t = 1000\n",
    "# reward_log = []\n",
    "\n",
    "# for _ in range(num_episode):\n",
    "    \n",
    "#     # initialize\n",
    "#     env.reset()\n",
    "#     t = 0\n",
    "#     episodic_reward = 0\n",
    "    \n",
    "#     for t in range(max_t):\n",
    "        \n",
    "#         #env.render()\n",
    "#         action = np.random.randint(11) # random action\n",
    "#         _, reward, done = env.step(action)\n",
    "#         episodic_reward += reward\n",
    "#         if done:\n",
    "#             break\n",
    "    \n",
    "#     reward_log.append(episodic_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, train=True, code='META')\n",
    "env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, train=False, code='META')\n",
    "agent = Agent(2*3, 11, 64, 0.001, 0.001, 0.99, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 1000\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    # agent.mu=0\n",
    "    env.mu=[0]\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    state = frame\n",
    "    t = 0\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        rewards_log.append(reward)\n",
    "        episodic_reward += reward\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    # global rewards_log, average_log, state_history, action_history, done_history, reward_history\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "    for i in range(1, 1 + num_episode):\n",
    "        env.mu=[0]\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "\n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        # print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}'.format(i, episodic_reward, average_log[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Reward 0.149, Average Reward 0.067, valReward 0.020, val Average Reward 0.136, Asset 1159046.67, Validation Asset 1007133.83, Average Validation Sharpe 0.95\n",
      "Episode 200, Reward 0.146, Average Reward 0.079, valReward 0.228, val Average Reward 0.152, Asset 1155355.00, Validation Asset 1239291.81, Average Validation Sharpe 1.06\n",
      "Episode 300, Reward 0.162, Average Reward 0.074, valReward 0.069, val Average Reward 0.143, Asset 1172819.31, Validation Asset 1061096.15, Average Validation Sharpe 1.00\n",
      "Episode 400, Reward 0.017, Average Reward 0.082, valReward -0.012, val Average Reward 0.160, Asset 1016732.79, Validation Asset 978388.28, Average Validation Sharpe 1.14\n",
      "Episode 500, Reward 0.112, Average Reward 0.083, valReward 0.113, val Average Reward 0.140, Asset 1116405.10, Validation Asset 1106083.33, Average Validation Sharpe 1.00\n",
      "Episode 600, Reward 0.053, Average Reward 0.082, valReward 0.075, val Average Reward 0.129, Asset 1054144.96, Validation Asset 1062301.37, Average Validation Sharpe 0.91\n",
      "Episode 700, Reward 0.059, Average Reward 0.085, valReward -0.085, val Average Reward 0.141, Asset 1060371.03, Validation Asset 908057.05, Average Validation Sharpe 1.00\n",
      "Episode 800, Reward 0.125, Average Reward 0.080, valReward 0.180, val Average Reward 0.156, Asset 1130542.00, Validation Asset 1182051.60, Average Validation Sharpe 1.10\n",
      "Episode 900, Reward 0.083, Average Reward 0.095, valReward 0.232, val Average Reward 0.136, Asset 1085829.77, Validation Asset 1241970.13, Average Validation Sharpe 0.96\n",
      "Episode 1000, Reward 0.011, Average Reward 0.082, valReward 0.083, val Average Reward 0.136, Asset 1010713.58, Validation Asset 1077200.10, Average Validation Sharpe 0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06257656551460147,\n",
       " 0.0582885995087275,\n",
       " 0.0712353202545123,\n",
       " -0.001973518635573842,\n",
       " 0.08786027652134903,\n",
       " 0.05297677193611557,\n",
       " 0.08620213518201852,\n",
       " 0.06801009416020443,\n",
       " 0.06877563576635294,\n",
       " 0.1059738025315718,\n",
       " 0.10031878924713047,\n",
       " -0.008551638852485183,\n",
       " 0.04836795896808232,\n",
       " 0.01323562596069566,\n",
       " -0.015578154378565603,\n",
       " 0.13756432222805262,\n",
       " 0.10572857452491076,\n",
       " 0.040799524443656275,\n",
       " 0.09971896683980616,\n",
       " 0.0842040385710471,\n",
       " 0.09169840334034533,\n",
       " 0.0871222459631878,\n",
       " 0.08413791929558533,\n",
       " 0.07771636384057774,\n",
       " 0.049213723703907704,\n",
       " 0.008507699428157794,\n",
       " 0.09129160716054831,\n",
       " 0.13521602098317329,\n",
       " 0.10973551318263998,\n",
       " 0.07591058588049646,\n",
       " 0.10722738562745496,\n",
       " 0.04585893905649825,\n",
       " 0.04200912122631058,\n",
       " 0.07047641451991835,\n",
       " 0.028199530088499437,\n",
       " 0.09637984606391933,\n",
       " 0.02449857505227989,\n",
       " 0.04591047390438034,\n",
       " 0.10093567663862477,\n",
       " 0.07854220089823004,\n",
       " -0.006900440964379466,\n",
       " 0.10269572321211011,\n",
       " 0.09665563213722544,\n",
       " 0.039087245510347625,\n",
       " 0.11752935307814247,\n",
       " 0.14014920095130173,\n",
       " 0.037235744410568976,\n",
       " 0.15876984059764476,\n",
       " 0.11257470026923704,\n",
       " 0.02382610479468994,\n",
       " 0.028374804843310175,\n",
       " 0.05288091382231742,\n",
       " 0.017821367353012027,\n",
       " 0.08120328509144407,\n",
       " 0.08702404637526895,\n",
       " 0.02616483523278378,\n",
       " 0.15794561230180365,\n",
       " 0.08718396213607549,\n",
       " 0.1299538086422422,\n",
       " 0.12157511155275018,\n",
       " 0.04753643649452787,\n",
       " 0.04160503925842579,\n",
       " -0.00860529785290996,\n",
       " 0.03768682872371851,\n",
       " 0.05965559289211119,\n",
       " 0.07177534872604206,\n",
       " 0.0855557049058775,\n",
       " 0.008511015305124017,\n",
       " 0.0836109643225393,\n",
       " 0.03563134998594061,\n",
       " 0.07771163748738617,\n",
       " -0.004136061525421543,\n",
       " 0.07874480490019378,\n",
       " 0.10269832968037168,\n",
       " 0.005870088068700005,\n",
       " 0.10056314869848411,\n",
       " 0.030930442366262894,\n",
       " 0.03824624817738046,\n",
       " 0.12183288491000319,\n",
       " 0.13516534014422135,\n",
       " 0.018945459387088987,\n",
       " 0.04564197943649584,\n",
       " 0.05848931675908467,\n",
       " 0.11271939952294804,\n",
       " 0.08288650059678623,\n",
       " 0.10242326114718947,\n",
       " -0.023286874425668597,\n",
       " 0.06810866560920244,\n",
       " 0.07257827602567929,\n",
       " 0.11941142720940898,\n",
       " 0.04606413388319518,\n",
       " 0.04349360963844622,\n",
       " 0.07344212377137203,\n",
       " 0.0225951505562663,\n",
       " 0.07884927376771123,\n",
       " 0.049729275985509955,\n",
       " 0.020419137063675347,\n",
       " 0.09532903598992541,\n",
       " 0.028056333972017497,\n",
       " 0.14918677672889036,\n",
       " 0.05949904271520435,\n",
       " 0.10672248615146042,\n",
       " 0.06603070553638296,\n",
       " 0.11124018424792312,\n",
       " 0.12356113857870732,\n",
       " 0.004851819805436518,\n",
       " 0.10672485673435117,\n",
       " 0.08932486155878366,\n",
       " 0.10249812165251526,\n",
       " 0.07959933529376723,\n",
       " 0.04819486143820767,\n",
       " 0.09925770744516381,\n",
       " 0.016607713483080006,\n",
       " 0.08547436637041221,\n",
       " 0.11240622065513636,\n",
       " 0.1085129478525034,\n",
       " 0.1521259883716362,\n",
       " 0.07210913969067946,\n",
       " 0.06158451060119359,\n",
       " 0.11606618664238824,\n",
       " 0.08811007689701211,\n",
       " 0.10665030886371966,\n",
       " 0.03688098620462753,\n",
       " 0.06957335707365592,\n",
       " 0.07589301567526481,\n",
       " 0.03756652037359422,\n",
       " 0.03969256818697771,\n",
       " 0.08068028673392524,\n",
       " 0.0304325949352827,\n",
       " 0.0657154123654591,\n",
       " 0.043017645600947445,\n",
       " 0.05775822826570947,\n",
       " -0.007319746871322626,\n",
       " 0.09139519567329303,\n",
       " 0.09646531781506798,\n",
       " 0.08218428125360283,\n",
       " 0.15643541396781108,\n",
       " 0.07729859309224195,\n",
       " 0.04966004433555833,\n",
       " 0.10184133401234222,\n",
       " 0.12361408269963081,\n",
       " 0.04987341355198279,\n",
       " 0.024536938278631383,\n",
       " 0.09732355795224466,\n",
       " 0.04220017431086963,\n",
       " 0.12372215927211014,\n",
       " 0.09743827064030253,\n",
       " 0.114546909863709,\n",
       " 0.004185285292859554,\n",
       " 0.0529578540896162,\n",
       " 0.1136043394731817,\n",
       " 0.1184420666453047,\n",
       " 0.07116031026188108,\n",
       " 0.1473704485605858,\n",
       " 0.0872878960951151,\n",
       " 0.06020721499768289,\n",
       " 0.08196266514402296,\n",
       " 0.04503410132015363,\n",
       " 0.02935242095364437,\n",
       " 0.046922618861407785,\n",
       " 0.11282827138277987,\n",
       " 0.12973425148304418,\n",
       " 0.024754123998458652,\n",
       " 0.021188656024364712,\n",
       " 0.01100557315178396,\n",
       " 0.05730296381469756,\n",
       " 0.04277460843422845,\n",
       " 0.12588223449653382,\n",
       " 0.10764513836401546,\n",
       " 0.055632083150772144,\n",
       " 0.053925076774228586,\n",
       " 0.03683826380564479,\n",
       " 0.11105537810947796,\n",
       " 0.18373459293566496,\n",
       " 0.02822199426788237,\n",
       " 0.0922078149203667,\n",
       " 0.09448159274746169,\n",
       " 0.09144807031843863,\n",
       " 0.09233895981231169,\n",
       " 0.126206567778894,\n",
       " 0.04167614455842993,\n",
       " 0.07663316192626372,\n",
       " 0.05238109434516126,\n",
       " 0.14712589925743963,\n",
       " 0.13535644862257534,\n",
       " 0.09580560013178335,\n",
       " 0.0707488255439136,\n",
       " 0.025195573246007616,\n",
       " 0.09580090834512803,\n",
       " 0.0888509851226512,\n",
       " 0.13467731784648881,\n",
       " 0.1295928269021957,\n",
       " 0.007546691289305613,\n",
       " 0.07156347785292017,\n",
       " 0.13441640825006992,\n",
       " 0.12946743360363472,\n",
       " -0.01684189242332475,\n",
       " 0.08448130901504852,\n",
       " 0.048026001415243116,\n",
       " 0.14597780251699086,\n",
       " 0.20448483346151328,\n",
       " 0.07594703658149315,\n",
       " 0.015651046345980815,\n",
       " 0.03210719273417255,\n",
       " 0.06176831474199322,\n",
       " 0.07874660270827546,\n",
       " 0.1241966939660133,\n",
       " 0.06195155070536966,\n",
       " 0.1307552644711778,\n",
       " 0.11931262767641299,\n",
       " 0.0626680053035906,\n",
       " 0.0448119013157273,\n",
       " 0.16689315623018355,\n",
       " 0.05096158979398517,\n",
       " 0.07580511800821876,\n",
       " 0.06141593604111395,\n",
       " 0.11013499520084015,\n",
       " 0.08094328783662126,\n",
       " 0.04935220347962237,\n",
       " 0.1098823750834219,\n",
       " 0.048260260769673144,\n",
       " 0.06837673526584007,\n",
       " 0.06831940478105024,\n",
       " 0.1256966357750854,\n",
       " 0.12015654552277033,\n",
       " 0.08337856635803731,\n",
       " 0.07996083863524969,\n",
       " 0.059875054438106125,\n",
       " 0.10801347058453242,\n",
       " 0.0486349090511547,\n",
       " 0.05502494899907004,\n",
       " 0.11770944664097052,\n",
       " 0.0806818734266615,\n",
       " 0.12394163287058949,\n",
       " 0.13624882987839163,\n",
       " 0.005309714433574057,\n",
       " 0.13823197202395038,\n",
       " 0.09564240070872329,\n",
       " 0.010857713588579761,\n",
       " 0.1475757944791934,\n",
       " 0.0873477562441389,\n",
       " 0.054865014440833366,\n",
       " 0.12284512288862404,\n",
       " 0.0979589000862483,\n",
       " -0.01684189242332475,\n",
       " 0.06070083697806325,\n",
       " 0.030657420611290648,\n",
       " 0.023424590066327397,\n",
       " 0.05887566834676399,\n",
       " 0.049894641490752525,\n",
       " 0.048845884396342895,\n",
       " 0.11705806254161222,\n",
       " 0.008886897577408213,\n",
       " 0.13926571617195824,\n",
       " 0.059731422134667554,\n",
       " 0.09220466449307171,\n",
       " 0.07225532592502787,\n",
       " 0.02005499397101078,\n",
       " 0.10791070053104748,\n",
       " 0.037212254439824816,\n",
       " 0.11011327369903955,\n",
       " 0.028441671628979724,\n",
       " 0.010259341200813466,\n",
       " 0.13650982079866375,\n",
       " 0.09473000224317012,\n",
       " 0.07247190066575207,\n",
       " 0.03138489069557877,\n",
       " 0.1616069119998069,\n",
       " 0.08434884980972013,\n",
       " 0.0830767695283103,\n",
       " 0.049761742521571645,\n",
       " 0.11645134490147035,\n",
       " 0.046384436780517985,\n",
       " 0.07013332246854315,\n",
       " 0.09200960495995918,\n",
       " -0.017662454349136048,\n",
       " 0.032151383684256206,\n",
       " -0.023983501587326057,\n",
       " 0.08877964048957339,\n",
       " 0.0240782649178733,\n",
       " 0.046837164873286825,\n",
       " -0.0020838968856610606,\n",
       " 0.034938384142054926,\n",
       " 0.12542747970350304,\n",
       " 0.07678761206779366,\n",
       " 0.06943198146926326,\n",
       " 0.04957185438156532,\n",
       " 0.0948072018810957,\n",
       " 0.060603091192883665,\n",
       " 0.03577667889844159,\n",
       " 0.07598212574347037,\n",
       " 0.09682396365013127,\n",
       " 0.05601536860528786,\n",
       " 0.0650789060166255,\n",
       " 0.08259601880466808,\n",
       " 0.04686950769205881,\n",
       " 0.16163468406688114,\n",
       " 0.03587500038443594,\n",
       " 0.06217892120431813,\n",
       " 0.16182046617831272,\n",
       " 0.06921718451106919,\n",
       " 0.13064580274253626,\n",
       " 0.0225814896547499,\n",
       " 0.015237856531425361,\n",
       " 0.07057677517578649,\n",
       " 0.10831942118274737,\n",
       " 0.11549358301434054,\n",
       " 0.08658443911206631,\n",
       " 0.15704099430985086,\n",
       " 0.07539540670812057,\n",
       " 0.0736591148762912,\n",
       " 0.07296082545880614,\n",
       " 0.08553134886310318,\n",
       " 0.12675670111495507,\n",
       " 0.039023435057598935,\n",
       " 0.06298028864683482,\n",
       " 0.07607154827922397,\n",
       " 0.11996646940196107,\n",
       " 0.07115203746154786,\n",
       " 0.04556131510002259,\n",
       " 0.033134600293816575,\n",
       " 0.05561992772228057,\n",
       " 0.11754112685678633,\n",
       " 0.15789525863486187,\n",
       " 0.05425274659914246,\n",
       " -0.01684189242332475,\n",
       " 0.05388264586744129,\n",
       " 0.0611016931385118,\n",
       " 0.031794270946466516,\n",
       " 0.04556310986060307,\n",
       " 0.09516224122563333,\n",
       " 0.042647084439600524,\n",
       " 0.14824556413776985,\n",
       " 0.025758169506171887,\n",
       " 0.008048672425931843,\n",
       " 0.13702120230133052,\n",
       " 0.13013176300785256,\n",
       " 0.02030229402840594,\n",
       " 0.11075667507679715,\n",
       " 0.07998542607900655,\n",
       " 0.0665946947778189,\n",
       " 0.04887261331789215,\n",
       " 0.11943443722922706,\n",
       " 0.10138620987855022,\n",
       " 0.13039017126664687,\n",
       " 0.07022559577345137,\n",
       " 0.11390018783323073,\n",
       " 0.15169492552105676,\n",
       " 0.07260707855749517,\n",
       " 0.07517319108966498,\n",
       " 0.10036581560307199,\n",
       " 0.11513344284503474,\n",
       " 0.2165576438801233,\n",
       " 0.04166355333756759,\n",
       " 0.013344422101798092,\n",
       " 0.02015893985716923,\n",
       " 0.2108222077327862,\n",
       " 0.14779385063898925,\n",
       " 0.07447642923568812,\n",
       " 0.07197173650869566,\n",
       " 0.03269286219219637,\n",
       " 0.1257741740016665,\n",
       " 0.0494739392575883,\n",
       " 0.0811329427069296,\n",
       " 0.13439779007865443,\n",
       " 0.04843995067535892,\n",
       " 0.07912567826871146,\n",
       " 0.10007406796122753,\n",
       " 0.04011339290172254,\n",
       " 0.18394614624360678,\n",
       " 0.10009067680659078,\n",
       " 0.06755263308752654,\n",
       " 0.1302980375543433,\n",
       " 0.09581137575454633,\n",
       " 0.04312963546269051,\n",
       " 0.09806796583764049,\n",
       " 0.12420782170217529,\n",
       " 0.040124097277305336,\n",
       " 0.09650582849871626,\n",
       " 0.1020623320516787,\n",
       " 0.17198964797038285,\n",
       " 0.10645340513129668,\n",
       " 0.04691827748603852,\n",
       " 0.04452182883063159,\n",
       " 0.09321924320096593,\n",
       " 0.13367932722315176,\n",
       " 0.046073963972020286,\n",
       " 0.0032872878577998106,\n",
       " 0.12898775166184231,\n",
       " 0.06590368548491957,\n",
       " 0.05765157976807925,\n",
       " 0.07913197189558555,\n",
       " -0.00562340081046819,\n",
       " 0.024039381316073125,\n",
       " 0.029316217054545273,\n",
       " 0.12214453819011752,\n",
       " 0.02000722276268912,\n",
       " 0.15630472791848213,\n",
       " 0.10435815188864592,\n",
       " 0.01664782822518503,\n",
       " 0.09440099581255584,\n",
       " 0.14109608420446665,\n",
       " 0.016639589397600435,\n",
       " 0.04131131619235364,\n",
       " 0.021620826576958175,\n",
       " 0.09390190984651425,\n",
       " 0.03934770334711348,\n",
       " -0.0013848684482720836,\n",
       " 0.08578424884385526,\n",
       " 0.10621129453601674,\n",
       " 0.09120459284909412,\n",
       " 0.038001540106389994,\n",
       " 0.053491128337624906,\n",
       " 0.11094133981343879,\n",
       " 0.046254345860326444,\n",
       " 0.039638453156877904,\n",
       " 0.10481877352506838,\n",
       " 0.07580361847832175,\n",
       " 0.04990877410459559,\n",
       " 0.1133871672112095,\n",
       " 0.06399019127560333,\n",
       " 0.05920918487827399,\n",
       " 0.07227304838148946,\n",
       " 0.11008679310989096,\n",
       " 0.0666575483556443,\n",
       " 0.11479801169636701,\n",
       " 0.13026053967997894,\n",
       " 0.07042714682448735,\n",
       " 0.12302305320735897,\n",
       " 0.07920396238930896,\n",
       " 0.13364094107223923,\n",
       " 0.03829453992590093,\n",
       " 0.13266645987066839,\n",
       " 0.11096764208085155,\n",
       " 0.06708920880341605,\n",
       " 0.12524833641108063,\n",
       " 0.11971410673902816,\n",
       " 0.09973595958561542,\n",
       " 0.10520527270585864,\n",
       " 0.08410035352312692,\n",
       " 0.0061036643048209644,\n",
       " 0.10625769386421632,\n",
       " 0.05607738922461547,\n",
       " 0.08431721976056297,\n",
       " 0.05312932454091383,\n",
       " 0.0656278883111819,\n",
       " 0.08070315112444852,\n",
       " 0.027098945600157034,\n",
       " 0.11814281597775231,\n",
       " 0.12218896844209379,\n",
       " 0.10879418470453231,\n",
       " 0.049062357055755755,\n",
       " 0.16354500059635485,\n",
       " 0.0670074403695454,\n",
       " 0.18997027107387116,\n",
       " 0.08371283021066048,\n",
       " 0.10208108608587446,\n",
       " 0.09365171129804457,\n",
       " 0.07370332022269714,\n",
       " 0.12636846671392638,\n",
       " 0.1253934677221093,\n",
       " 0.0920706952234352,\n",
       " 0.16021640792367073,\n",
       " 0.03843714627408871,\n",
       " 0.03292542530080239,\n",
       " 0.08738768144269626,\n",
       " 0.08418980026013032,\n",
       " 0.11860470096538792,\n",
       " -0.004939210483322153,\n",
       " 0.14523665837582658,\n",
       " 0.06430529486669675,\n",
       " 0.04687188297056728,\n",
       " 0.034698948558464004,\n",
       " 0.0874321162633927,\n",
       " 0.009190019366294322,\n",
       " 0.07713279991728067,\n",
       " 0.07179015429105809,\n",
       " 0.10192373959218774,\n",
       " 0.11738755556451438,\n",
       " 0.09024668285301804,\n",
       " 0.0850347118583532,\n",
       " 0.05684306124515632,\n",
       " 0.03723363029329338,\n",
       " 0.156464354530315,\n",
       " 0.03369264053816194,\n",
       " 0.06227279774849177,\n",
       " 0.1284528106256747,\n",
       " 0.04759026477400048,\n",
       " 0.11289160535807659,\n",
       " 0.06226427341345499,\n",
       " 0.09976831178147501,\n",
       " 0.03289920474378948,\n",
       " 0.07153310827317948,\n",
       " 0.11393466437706537,\n",
       " 0.08429408927000599,\n",
       " 0.1342677617922783,\n",
       " 0.0779298206214139,\n",
       " 0.14061213224669644,\n",
       " 0.05990507503403296,\n",
       " 0.11175983700019455,\n",
       " 0.056820139335790214,\n",
       " 0.0893804647016882,\n",
       " 0.022157409527943817,\n",
       " 0.09370549730843034,\n",
       " 0.10014022160224732,\n",
       " 0.045046683528318275,\n",
       " 0.008173699603091158,\n",
       " 0.050936495667649045,\n",
       " 0.12091180479873267,\n",
       " 0.11483102965767268,\n",
       " 0.12268699552342799,\n",
       " 0.04880023864159144,\n",
       " 0.046973557664654726,\n",
       " 0.08161628259874767,\n",
       " 0.08398709982211809,\n",
       " 0.130790833256437,\n",
       " 0.052587672532401576,\n",
       " 0.05414118192272296,\n",
       " 0.050109887691113485,\n",
       " 0.062061100404826805,\n",
       " 0.09395112319693069,\n",
       " 0.039139220988998885,\n",
       " 0.07064786972887037,\n",
       " 0.05283836076058321,\n",
       " 0.01756092412279281,\n",
       " 0.08999995482178105,\n",
       " 0.1349917313170826,\n",
       " 0.11745578140100381,\n",
       " 0.0810932313470794,\n",
       " 0.15244064626517098,\n",
       " 0.07105453356945754,\n",
       " 0.10514272512519669,\n",
       " 0.0561272410250659,\n",
       " 0.09970932593106602,\n",
       " 0.049202806753560674,\n",
       " 0.1082683573620753,\n",
       " 0.10208318360803356,\n",
       " 0.09024641728486138,\n",
       " 0.08110272574644652,\n",
       " 0.0014356687037237828,\n",
       " 0.17636067540929215,\n",
       " 0.1118302304132992,\n",
       " 0.06525026535903872,\n",
       " 0.12790921237726932,\n",
       " 0.11100911511193357,\n",
       " 0.0556008914164665,\n",
       " 0.09083851038783719,\n",
       " 0.10896898903591348,\n",
       " 0.13544126887452573,\n",
       " 0.0695206805810467,\n",
       " 0.02668500031185341,\n",
       " 0.034105288062966505,\n",
       " 0.08047159122221946,\n",
       " 0.07827812663758266,\n",
       " 0.10819413638161501,\n",
       " 0.0591898575614628,\n",
       " 0.029189871076935762,\n",
       " 0.0393047834097473,\n",
       " 0.043674249085058764,\n",
       " 0.1639159161406286,\n",
       " 0.022129759294742077,\n",
       " 0.03788603862735133,\n",
       " 0.05454261874050878,\n",
       " 0.10395043297780612,\n",
       " 0.1261452497738424,\n",
       " 0.13270505834350219,\n",
       " 0.03836287877376786,\n",
       " 0.08525896238212724,\n",
       " 0.04180190587537739,\n",
       " 0.0641423770250781,\n",
       " 0.05114583800805024,\n",
       " 0.03950197248413249,\n",
       " 0.1511383416226892,\n",
       " 0.14109581734433563,\n",
       " 0.1450332925461709,\n",
       " 0.03923128714264003,\n",
       " 0.058087365042862424,\n",
       " 0.09213831334568395,\n",
       " 0.09822398836954124,\n",
       " 0.10476162423044623,\n",
       " 0.05092810272845778,\n",
       " 0.09870983722089993,\n",
       " 0.1594683526505506,\n",
       " 0.09167316095164312,\n",
       " 0.10805874711209254,\n",
       " 0.053987867509261836,\n",
       " 0.1381444250810601,\n",
       " 0.06535988939845802,\n",
       " 0.004185285292859554,\n",
       " 0.20500224017377958,\n",
       " 0.17846524348532283,\n",
       " 0.05634544380107576,\n",
       " 0.14295177567370262,\n",
       " 0.15460029013647147,\n",
       " 0.002007565307153486,\n",
       " 0.10782034954354355,\n",
       " 0.04821477981317961,\n",
       " 0.009224896733188933,\n",
       " 0.042044430591875744,\n",
       " 0.053155969507745364,\n",
       " 0.1089885239453874,\n",
       " 0.13111763226371936,\n",
       " 0.15133664692916673,\n",
       " 0.04087711304169356,\n",
       " 0.06337150008783679,\n",
       " 0.05976457652894282,\n",
       " 0.1022296514633035,\n",
       " 0.055691404126266816,\n",
       " 0.13611845280111187,\n",
       " 0.04118353555005634,\n",
       " 0.048207732437198905,\n",
       " 0.08464727754247037,\n",
       " 0.10167414059556841,\n",
       " 0.05762580798054376,\n",
       " 0.000963442615791868,\n",
       " 0.028602491839903696,\n",
       " 0.11561109968652163,\n",
       " 0.04132326510004308,\n",
       " 0.07246584032600982,\n",
       " 0.06466540153144887,\n",
       " 0.07468147598835788,\n",
       " 0.15415086335018408,\n",
       " 0.06131322889450286,\n",
       " 0.1030821473858928,\n",
       " 0.1699281691151373,\n",
       " 0.02668500031185341,\n",
       " 0.15675582147496675,\n",
       " 0.169549226062375,\n",
       " 0.0913807422245801,\n",
       " 0.05512589604687788,\n",
       " 0.0826701239348786,\n",
       " 0.07101130127525737,\n",
       " 0.15028667297204687,\n",
       " 0.052427727586055635,\n",
       " 0.15536901058859548,\n",
       " -0.021602965199325582,\n",
       " -0.05441782941787674,\n",
       " 0.02725587466018666,\n",
       " 0.11025641152307139,\n",
       " 0.11270320064343853,\n",
       " 0.05488837879115577,\n",
       " 0.00834784142019057,\n",
       " 0.04292417731923361,\n",
       " 0.08402123499255965,\n",
       " 0.13652575128886324,\n",
       " 0.12165608054621359,\n",
       " 0.08575384076339969,\n",
       " 0.09031594868167553,\n",
       " 0.07538854520254279,\n",
       " 0.12325653641200271,\n",
       " 0.036671062187886946,\n",
       " 0.15706983946234654,\n",
       " 0.05724097541666959,\n",
       " 0.0,\n",
       " 0.08934715280238532,\n",
       " 0.008360073263042828,\n",
       " 0.15321089466997032,\n",
       " 0.11544719228398662,\n",
       " 0.06528206277080649,\n",
       " 0.06510823130289697,\n",
       " 0.1332184275401477,\n",
       " 0.10351884871159706,\n",
       " 0.14170337852839976,\n",
       " 0.047606670767055645,\n",
       " 0.15286043482925493,\n",
       " 0.07275737677543827,\n",
       " 0.10588135334347096,\n",
       " 0.08571997058665454,\n",
       " 0.12854225936255614,\n",
       " 0.08981931777572734,\n",
       " 0.06103788828008773,\n",
       " 0.0856588114405867,\n",
       " 0.048992370832698605,\n",
       " 0.17010341635680573,\n",
       " 0.142779219611586,\n",
       " 0.13075597201509218,\n",
       " 0.06836524551000664,\n",
       " 0.06779408484127011,\n",
       " 0.11956457186771163,\n",
       " -0.02762140731516898,\n",
       " 0.05624544466003889,\n",
       " 0.18836025345859747,\n",
       " 0.10782034954354355,\n",
       " 0.07143331844090993,\n",
       " 0.07859075019933182,\n",
       " 0.12268284257479559,\n",
       " 0.13035541801091344,\n",
       " 0.04212532192477658,\n",
       " 0.07085747156588545,\n",
       " 0.13653723430920078,\n",
       " 0.13008666227649712,\n",
       " 0.12683661145731526,\n",
       " 0.11558693038252014,\n",
       " -0.001389565650784862,\n",
       " 0.03450294516268988,\n",
       " 0.034630774633053425,\n",
       " 0.07425551272383506,\n",
       " 0.11705315284071213,\n",
       " 0.0503355801868349,\n",
       " 0.0588458249383623,\n",
       " 0.14489227127391655,\n",
       " 0.1913578740636056,\n",
       " 0.124641859536453,\n",
       " -0.01922242881132511,\n",
       " 0.07613995146833914,\n",
       " 0.18851160674706977,\n",
       " 0.09641595051037175,\n",
       " 0.01770265679776036,\n",
       " 0.02275469276881747,\n",
       " 0.14487718121099605,\n",
       " 0.12520502678826334,\n",
       " 0.19032404718785334,\n",
       " -0.0025586740953216793,\n",
       " 0.006935454036219725,\n",
       " -0.0097002832593231,\n",
       " 0.08244683588033072,\n",
       " 0.008360073263042828,\n",
       " 0.01769343741649328,\n",
       " 0.09103163663587252,\n",
       " 0.03418394588575264,\n",
       " 0.0,\n",
       " 0.1273605796423745,\n",
       " 0.08222185578675532,\n",
       " 0.16397108751284056,\n",
       " 0.07539685858719944,\n",
       " 0.09039093456121468,\n",
       " 0.07818228066585835,\n",
       " 0.002434262769250222,\n",
       " 0.2103462211808363,\n",
       " -0.021602965199325582,\n",
       " 0.13943386646403227,\n",
       " -0.0021818955733374315,\n",
       " 0.0,\n",
       " 0.08293315448903882,\n",
       " 0.03241682472760129,\n",
       " 0.04010331277037689,\n",
       " 0.031606544408085524,\n",
       " 0.07362643559635654,\n",
       " 0.05583074141650454,\n",
       " 0.13157735174385343,\n",
       " 0.054955987035659314,\n",
       " 0.1645199132413525,\n",
       " 0.07607301299971368,\n",
       " 0.062285179381773435,\n",
       " 0.1345537975710976,\n",
       " 0.11292659415863393,\n",
       " 0.07679226376675975,\n",
       " 0.05094849714826481,\n",
       " -0.0011808977008273217,\n",
       " 0.09456556398455175,\n",
       " 0.15335722747488467,\n",
       " 0.10622695665790494,\n",
       " 0.05528215700350228,\n",
       " 0.08196176561579134,\n",
       " 0.1313731697417569,\n",
       " 0.0501873457188379,\n",
       " 0.11195302898918673,\n",
       " 0.11736743096946234,\n",
       " 0.04692113844909551,\n",
       " 0.08364714983223721,\n",
       " 0.06411406494574641,\n",
       " 0.08265514088007657,\n",
       " 0.06999554428721591,\n",
       " 0.09299539962402067,\n",
       " 0.03442430786177671,\n",
       " 0.08522644712802857,\n",
       " 0.18927421428304678,\n",
       " 0.07579724138142467,\n",
       " 0.036547484587195436,\n",
       " 0.06918365742593414,\n",
       " 0.03286877013050938,\n",
       " 0.13196000667004432,\n",
       " 0.13459149842711857,\n",
       " 0.06188658821486373,\n",
       " 0.06760130583319349,\n",
       " 0.022050056710753062,\n",
       " 0.09595559141395794,\n",
       " 0.09801065421054077,\n",
       " 0.04739503243580412,\n",
       " 0.05672303487959128,\n",
       " 0.06362638905449308,\n",
       " 0.022787768906081078,\n",
       " 0.06059519893456705,\n",
       " 0.07702774102878895,\n",
       " 0.14689567631014547,\n",
       " 0.06752007712926934,\n",
       " 0.08239161687044329,\n",
       " 0.1681228709262517,\n",
       " 0.09660078397755766,\n",
       " 0.10307110206046213,\n",
       " 0.01958980956265144,\n",
       " 0.09567282205800805,\n",
       " 0.13306500517824316,\n",
       " 0.12818155516773055,\n",
       " 0.10389610223044504,\n",
       " 0.11927850113161322,\n",
       " 0.0741619695949724,\n",
       " 0.043061613591264994,\n",
       " 0.005309714433574057,\n",
       " 0.12508540859098866,\n",
       " 0.1231187456829608,\n",
       " 0.06098902883149737,\n",
       " 0.14547220614318176,\n",
       " 0.10989835296215106,\n",
       " 0.08632519261726773,\n",
       " -0.0036508028673269,\n",
       " 0.0715595842677048,\n",
       " 0.17292255508311077,\n",
       " 0.020323514787560046,\n",
       " 0.1311930496757816,\n",
       " 0.14473524324550374,\n",
       " 0.08166739316833947,\n",
       " 0.12130231484650256,\n",
       " 0.08371237856368835,\n",
       " 0.1768821800999287,\n",
       " 0.1211566184253387,\n",
       " 0.059806677339919526,\n",
       " 0.09756457996156843,\n",
       " 0.16559512003442683,\n",
       " 0.12644176602056892,\n",
       " 0.08614975697925659,\n",
       " 0.1074655224258447,\n",
       " 0.06111816865951536,\n",
       " 0.15472138065273353,\n",
       " 0.1358286718578781,\n",
       " 0.10796632950589098,\n",
       " 0.002187314862201783,\n",
       " 0.08320423915687947,\n",
       " 0.061716662483738194,\n",
       " 0.0,\n",
       " 0.08246602136758634,\n",
       " 0.1493480161316437,\n",
       " 0.06083341597934651,\n",
       " 0.08571997058665454,\n",
       " 0.05624544466003889,\n",
       " 0.08584671350051323,\n",
       " 0.07724267644884374,\n",
       " 0.06455474522046842,\n",
       " 0.022077595579500862,\n",
       " 0.1491031796296972,\n",
       " 0.16186753708696525,\n",
       " 0.12499044078758191,\n",
       " 0.11478431428446947,\n",
       " 0.04622902022442605,\n",
       " 0.09144224528172455,\n",
       " 0.166789967195417,\n",
       " 0.12042150125272394,\n",
       " 0.03907954673327442,\n",
       " 0.1024610557297245,\n",
       " 0.10718599553176753,\n",
       " 0.010259341200813466,\n",
       " 0.1272124136500217,\n",
       " 0.12666086099898763,\n",
       " 0.015327350397683872,\n",
       " 0.016403903294712995,\n",
       " 0.1663035678148331,\n",
       " 0.10093828691535467,\n",
       " 0.08428937875451606,\n",
       " 0.12473257542359174,\n",
       " 0.16611557602942578,\n",
       " 0.13026175979434226,\n",
       " 0.005663576281427867,\n",
       " 0.13048034819382642,\n",
       " 0.01020304726162806,\n",
       " 0.12104380818797664,\n",
       " 0.14622384535099808,\n",
       " 0.12739995708347635,\n",
       " 0.06161851915308228,\n",
       " 0.0501873457188379,\n",
       " 0.13225501253154812,\n",
       " 0.02684779960260373,\n",
       " 0.1589660269146231,\n",
       " 0.1736410749037433,\n",
       " 0.1433406347968978,\n",
       " 0.15648149266683126,\n",
       " 0.14636485885421918,\n",
       " 0.03166513816787142,\n",
       " 0.08620245677211116,\n",
       " 0.1551440022785468,\n",
       " 0.10752461696049237,\n",
       " 0.12237591727831706,\n",
       " 0.09198021424327568,\n",
       " 0.0896224700580147,\n",
       " 0.03649353879479322,\n",
       " 0.1536759038498995,\n",
       " 0.14327606218898506,\n",
       " 0.04653379413719341,\n",
       " 0.11839657203896828,\n",
       " 0.09874068557272705,\n",
       " 0.12193148758938087,\n",
       " 0.06913113566170813,\n",
       " 0.019561399251152595,\n",
       " 0.026949184178748486,\n",
       " 0.12266965428119134,\n",
       " 0.08522644712802857,\n",
       " 0.025705870853822088,\n",
       " 0.08534425231094114,\n",
       " 0.07715082003268181,\n",
       " 0.008173699603091158,\n",
       " 0.08336320719475608,\n",
       " 0.11478361246808302,\n",
       " 0.10827864724076736,\n",
       " 0.11483938332104608,\n",
       " -0.008630212393298808,\n",
       " 0.0739002550917263,\n",
       " 0.1204505427409248,\n",
       " 0.10401507444965803,\n",
       " 0.12964914480460132,\n",
       " 0.011055883893694794,\n",
       " 0.04774813556206355,\n",
       " 0.011452834215204294,\n",
       " 0.06069217049754006,\n",
       " 0.07919751288913739,\n",
       " 0.17314500808904687,\n",
       " 0.16860442034341902,\n",
       " 0.1038316222000017,\n",
       " 0.18202801024701237,\n",
       " 0.01902169573462197,\n",
       " 0.06959509205092054,\n",
       " 0.10246312540249775,\n",
       " -0.01230042796870551,\n",
       " 0.12381536574782649,\n",
       " 0.06827367360039861,\n",
       " 0.04692113844909551,\n",
       " 0.03570790826714465,\n",
       " 0.0,\n",
       " 0.04526250515841877,\n",
       " 0.03407490191478882,\n",
       " 0.02800431929244955,\n",
       " 0.11209209816436501,\n",
       " 0.044084924282586174,\n",
       " 0.08359700241092137,\n",
       " 0.048124337790435595,\n",
       " 0.1560466693293047,\n",
       " 0.11345120688513459,\n",
       " 0.09506276080427206,\n",
       " 0.061484744006393595,\n",
       " 0.03520744637772316,\n",
       " 0.08226125632158382,\n",
       " 0.07165036392812485,\n",
       " 0.047577041206528205,\n",
       " 0.04519379180419596,\n",
       " 0.14183053210677588,\n",
       " 0.12572101623517296,\n",
       " 0.06252535859163863,\n",
       " 0.1611052474084856,\n",
       " 0.06504071554257375,\n",
       " 0.014081752817774212,\n",
       " 0.10556342081190453,\n",
       " 0.07233030157096397,\n",
       " 0.1172852804192201,\n",
       " 0.12938182131971335,\n",
       " 0.07276733380789621,\n",
       " 0.1248624640628736,\n",
       " 0.10700953451549984,\n",
       " 0.013655434490648287,\n",
       " 0.1766648770959914,\n",
       " 0.09348591933651638,\n",
       " 0.10868765537400855,\n",
       " 0.07274542532636559,\n",
       " 0.1225851357991968,\n",
       " 0.10622695665790494,\n",
       " 0.09897100983851417,\n",
       " 0.1699858720273184,\n",
       " 0.10709723399455612,\n",
       " 0.14665878692791065,\n",
       " 0.07626839822791205,\n",
       " 0.12343642187918086,\n",
       " 0.06716882288672385,\n",
       " 0.16172404593377712,\n",
       " 0.16290303383218635,\n",
       " 0.16142566324975138,\n",
       " 0.03205379245401205,\n",
       " 0.00834784142019057,\n",
       " 0.03174862777808281,\n",
       " 0.09619634111013173,\n",
       " 0.14820588128248835,\n",
       " 0.0543007360603677,\n",
       " 0.131016178902576,\n",
       " 0.08965878515049726,\n",
       " 0.07660690296654754,\n",
       " 0.04692113844909551,\n",
       " 0.024008724822760955,\n",
       " 0.059300864039034604,\n",
       " -0.01922242881132511,\n",
       " 0.07433997351543278,\n",
       " 0.022871928097922694,\n",
       " 0.021287449143559806,\n",
       " 0.08490476330963394,\n",
       " 0.09952610641042428,\n",
       " 0.05716866141799674,\n",
       " 0.055523700166422454,\n",
       " 0.10761546176798585,\n",
       " 0.10896719170647103,\n",
       " 0.005309714433574057,\n",
       " 0.05468259477600275,\n",
       " 0.07151241049179367,\n",
       " 0.08490476330963394,\n",
       " 0.16854973148830882,\n",
       " 0.010688554589496841]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_test.timeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.108, Average Reward 0.027, valReward 0.339, val Average Reward 0.273, Asset 1106570.61, Validation Asset 1392007.80, Average Validation Sharpe 1.97\n",
      "Episode 200, Reward 0.117, Average Reward 0.041, valReward 0.354, val Average Reward 0.266, Asset 1118960.24, Validation Asset 1412565.14, Average Validation Sharpe 1.94\n",
      "Episode 300, Reward 0.014, Average Reward 0.049, valReward 0.328, val Average Reward 0.279, Asset 1010468.78, Validation Asset 1374042.76, Average Validation Sharpe 2.04\n",
      "Episode 400, Reward 0.149, Average Reward 0.055, valReward 0.302, val Average Reward 0.285, Asset 1154856.22, Validation Asset 1339423.05, Average Validation Sharpe 2.07\n",
      "Episode 500, Reward -0.006, Average Reward 0.073, valReward 0.311, val Average Reward 0.290, Asset 994032.45, Validation Asset 1355198.59, Average Validation Sharpe 2.12\n",
      "Episode 600, Reward 0.077, Average Reward 0.078, valReward 0.340, val Average Reward 0.274, Asset 1077964.08, Validation Asset 1394670.17, Average Validation Sharpe 1.99\n",
      "Episode 700, Reward 0.078, Average Reward 0.071, valReward 0.259, val Average Reward 0.273, Asset 1079599.48, Validation Asset 1285768.56, Average Validation Sharpe 1.98\n",
      "Episode 800, Reward -0.001, Average Reward 0.071, valReward 0.259, val Average Reward 0.286, Asset 999469.22, Validation Asset 1280750.25, Average Validation Sharpe 2.07\n",
      "Episode 900, Reward 0.060, Average Reward 0.072, valReward 0.283, val Average Reward 0.274, Asset 1056085.08, Validation Asset 1314134.15, Average Validation Sharpe 2.01\n",
      "Episode 1000, Reward 0.001, Average Reward 0.071, valReward 0.253, val Average Reward 0.283, Asset 1000619.05, Validation Asset 1275834.37, Average Validation Sharpe 2.07\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.089, Average Reward 0.084, valReward -0.050, val Average Reward -0.052, Asset 1085465.14, Validation Asset 935710.28, Average Validation Sharpe -0.26\n",
      "Episode 200, Reward 0.205, Average Reward 0.093, valReward 0.002, val Average Reward -0.058, Asset 1218755.73, Validation Asset 984582.02, Average Validation Sharpe -0.301\n",
      "Episode 300, Reward 0.060, Average Reward 0.099, valReward -0.189, val Average Reward -0.040, Asset 1060515.70, Validation Asset 812277.86, Average Validation Sharpe -0.21\n",
      "Episode 400, Reward -0.002, Average Reward 0.106, valReward 0.154, val Average Reward -0.041, Asset 994839.76, Validation Asset 1145117.73, Average Validation Sharpe -0.21\n",
      "Episode 500, Reward 0.020, Average Reward 0.110, valReward 0.182, val Average Reward -0.044, Asset 1019723.89, Validation Asset 1180998.38, Average Validation Sharpe -0.23\n",
      "Episode 600, Reward 0.149, Average Reward 0.110, valReward -0.136, val Average Reward -0.050, Asset 1147394.07, Validation Asset 858865.52, Average Validation Sharpe -0.26\n",
      "Episode 700, Reward 0.251, Average Reward 0.113, valReward 0.033, val Average Reward -0.063, Asset 1270725.63, Validation Asset 1012666.56, Average Validation Sharpe -0.33\n",
      "Episode 800, Reward 0.124, Average Reward 0.112, valReward -0.083, val Average Reward -0.059, Asset 1121612.22, Validation Asset 906711.07, Average Validation Sharpe -0.31\n",
      "Episode 900, Reward 0.255, Average Reward 0.117, valReward -0.120, val Average Reward -0.054, Asset 1261414.24, Validation Asset 871039.73, Average Validation Sharpe -0.28\n",
      "Episode 1000, Reward 0.025, Average Reward 0.123, valReward -0.056, val Average Reward -0.046, Asset 1023638.21, Validation Asset 928179.73, Average Validation Sharpe -0.24\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward -0.017, Average Reward 0.076, valReward 0.027, val Average Reward -0.005, Asset 982708.38, Validation Asset 1017019.68, Average Validation Sharpe -0.03\n",
      "Episode 200, Reward 0.119, Average Reward 0.090, valReward -0.137, val Average Reward -0.003, Asset 1121028.19, Validation Asset 864781.67, Average Validation Sharpe -0.02\n",
      "Episode 300, Reward 0.159, Average Reward 0.099, valReward -0.022, val Average Reward -0.023, Asset 1161019.77, Validation Asset 970864.34, Average Validation Sharpe -0.18\n",
      "Episode 400, Reward 0.154, Average Reward 0.087, valReward 0.056, val Average Reward -0.018, Asset 1156050.36, Validation Asset 1049445.81, Average Validation Sharpe -0.14\n",
      "Episode 500, Reward 0.186, Average Reward 0.107, valReward -0.009, val Average Reward -0.002, Asset 1193159.41, Validation Asset 984214.55, Average Validation Sharpe -0.02\n",
      "Episode 600, Reward 0.124, Average Reward 0.111, valReward 0.121, val Average Reward -0.012, Asset 1120021.78, Validation Asset 1118273.38, Average Validation Sharpe -0.09\n",
      "Episode 700, Reward -0.000, Average Reward 0.127, valReward 0.032, val Average Reward -0.009, Asset 999321.28, Validation Asset 1025643.07, Average Validation Sharpe -0.06\n",
      "Episode 800, Reward 0.083, Average Reward 0.102, valReward -0.013, val Average Reward -0.003, Asset 1077379.96, Validation Asset 978094.21, Average Validation Sharpe -0.02\n",
      "Episode 900, Reward 0.129, Average Reward 0.116, valReward 0.052, val Average Reward -0.016, Asset 1128937.48, Validation Asset 1046905.76, Average Validation Sharpe -0.12\n",
      "Episode 1000, Reward 0.158, Average Reward 0.122, valReward 0.000, val Average Reward -0.010, Asset 1157219.66, Validation Asset 993465.02, Average Validation Sharpe -0.08\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.041, Average Reward 0.073, valReward 0.084, val Average Reward 0.142, Asset 1040561.46, Validation Asset 1073607.13, Average Validation Sharpe 1.00\n",
      "Episode 200, Reward 0.124, Average Reward 0.077, valReward 0.097, val Average Reward 0.148, Asset 1130036.70, Validation Asset 1089239.17, Average Validation Sharpe 1.05\n",
      "Episode 300, Reward 0.067, Average Reward 0.079, valReward 0.085, val Average Reward 0.148, Asset 1069163.45, Validation Asset 1076664.52, Average Validation Sharpe 1.05\n",
      "Episode 400, Reward 0.125, Average Reward 0.083, valReward 0.237, val Average Reward 0.137, Asset 1130192.21, Validation Asset 1250590.41, Average Validation Sharpe 0.99\n",
      "Episode 500, Reward 0.045, Average Reward 0.083, valReward 0.263, val Average Reward 0.146, Asset 1045547.03, Validation Asset 1281851.24, Average Validation Sharpe 1.04\n",
      "Episode 600, Reward 0.124, Average Reward 0.085, valReward 0.129, val Average Reward 0.146, Asset 1129751.87, Validation Asset 1125506.74, Average Validation Sharpe 1.01\n",
      "Episode 700, Reward 0.088, Average Reward 0.089, valReward 0.065, val Average Reward 0.139, Asset 1090602.20, Validation Asset 1054390.32, Average Validation Sharpe 0.98\n",
      "Episode 800, Reward 0.030, Average Reward 0.086, valReward 0.233, val Average Reward 0.143, Asset 1030246.21, Validation Asset 1244077.79, Average Validation Sharpe 1.00\n",
      "Episode 900, Reward 0.045, Average Reward 0.095, valReward 0.121, val Average Reward 0.158, Asset 1045568.55, Validation Asset 1114232.21, Average Validation Sharpe 1.09\n",
      "Episode 1000, Reward 0.022, Average Reward 0.077, valReward 0.100, val Average Reward 0.160, Asset 1022041.66, Validation Asset 1094216.99, Average Validation Sharpe 1.12\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.074, Average Reward 0.044, valReward 0.028, val Average Reward 0.071, Asset 1075339.01, Validation Asset 1019935.76, Average Validation Sharpe 0.61\n",
      "Episode 200, Reward -0.025, Average Reward 0.047, valReward -0.073, val Average Reward 0.061, Asset 973215.68, Validation Asset 923920.70, Average Validation Sharpe 0.52\n",
      "Episode 300, Reward 0.006, Average Reward 0.054, valReward 0.015, val Average Reward 0.070, Asset 1005674.44, Validation Asset 1003872.74, Average Validation Sharpe 0.62\n",
      "Episode 400, Reward 0.210, Average Reward 0.063, valReward 0.019, val Average Reward 0.053, Asset 1226065.40, Validation Asset 1009610.73, Average Validation Sharpe 0.46\n",
      "Episode 500, Reward 0.068, Average Reward 0.060, valReward 0.149, val Average Reward 0.080, Asset 1069158.28, Validation Asset 1151181.17, Average Validation Sharpe 0.69\n",
      "Episode 600, Reward 0.026, Average Reward 0.059, valReward -0.030, val Average Reward 0.060, Asset 1025481.18, Validation Asset 962331.23, Average Validation Sharpe 0.54\n",
      "Episode 700, Reward 0.171, Average Reward 0.066, valReward 0.060, val Average Reward 0.064, Asset 1182975.17, Validation Asset 1053956.21, Average Validation Sharpe 0.54\n",
      "Episode 800, Reward 0.050, Average Reward 0.064, valReward 0.075, val Average Reward 0.058, Asset 1050697.98, Validation Asset 1069906.46, Average Validation Sharpe 0.49\n",
      "Episode 900, Reward 0.030, Average Reward 0.063, valReward -0.068, val Average Reward 0.047, Asset 1028175.72, Validation Asset 928710.83, Average Validation Sharpe 0.40\n",
      "Episode 1000, Reward 0.038, Average Reward 0.057, valReward 0.130, val Average Reward 0.059, Asset 1037604.13, Validation Asset 1127095.02, Average Validation Sharpe 0.51\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, train=True, code=code)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, train=False, code=code)\n",
    "    agent = Agent(2*3, 11, 64, 0.001, 0.001, 0.99, 'cuda')\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_init = eps\n",
    "# constant = C\n",
    "# num_frame =1\n",
    "\n",
    "# rewards_log = []\n",
    "# average_log = []\n",
    "# eps = eps_init\n",
    "\n",
    "# for i in range(1, 1 + num_episode):\n",
    "#     episodic_reward = 0\n",
    "#     done = False\n",
    "#     frame = env.reset()\n",
    "#     state_deque = deque(maxlen=num_frame)\n",
    "#     for _ in range(num_frame):\n",
    "#         state_deque.append(frame)\n",
    "#     state = np.stack(state_deque, axis=0)\n",
    "#     state = np.expand_dims(state, axis=0)\n",
    "#     t = 0\n",
    "\n",
    "#     while not done and t < max_t:\n",
    "\n",
    "#         t += 1\n",
    "#         action = agent.act(state, eps)\n",
    "#         frame, reward, done = env.step(action)\n",
    "#         state_deque.append(frame)\n",
    "#         next_state = np.stack(state_deque, axis=0)\n",
    "#         next_state = np.expand_dims(next_state, axis=0)\n",
    "#         agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "#         if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "#             agent.learn()\n",
    "#             agent.soft_update(agent.tau)\n",
    "\n",
    "#         state = next_state.copy()\n",
    "#         episodic_reward += reward\n",
    "\n",
    "#     rewards_log.append(episodic_reward)\n",
    "#     average_log.append(np.mean(rewards_log[-100:]))\n",
    "#     print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}'.format(i, episodic_reward, average_log[-1]), end='')\n",
    "#     if i % 100 == 0:\n",
    "#         print()\n",
    "\n",
    "#     eps = max(eps * eps_decay, eps_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
