{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "time_period = 2\n",
    "# class Q_Network(nn.Module):\n",
    "#     '''\n",
    "#     The input of this network should have shape (num_frame, 80, 80)\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self, num_frame, num_action):\n",
    "#         super(Q_Network, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels=num_frame, out_channels=32, kernel_size=(2,1), stride=1, padding=2)  # 16, 20, 20\n",
    "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "#         self.conv4 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "#         self.conv5 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,2), stride=1)  # 32, 9, 9\n",
    "#         self.pool = nn.AvgPool2d(kernel_size=(2,1))\n",
    "#         self.fc1 = nn.Linear(544, 256)\n",
    "#         self.fc2 = nn.Linear(256, num_action)\n",
    "#         self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "#     def forward(self, image):\n",
    "#         x = F.relu(self.pool(self.conv1(image)))\n",
    "#         x = F.relu(self.pool(self.conv2(x)))\n",
    "#         x = F.relu(self.pool(self.conv3(x)))\n",
    "#         x = x.view(-1, 544)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x1 = self.fc2(x)\n",
    "#         x1 = x1 - torch.max(x1, dim=1, keepdim=True)[0]\n",
    "#         x2 = self.fc3(x)\n",
    "#         return x1 + x2\n",
    "\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden=[64, 64], duel=False):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size)\n",
    "        self.duel = duel\n",
    "        if self.duel:\n",
    "            self.fc4 = nn.Linear(hidden[1], 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if self.duel:\n",
    "            x1 = self.fc3(x)\n",
    "            x1 = x1 - torch.max(x1, dim=1, keepdim=True)[0] # set the max to be 0\n",
    "            x2 = self.fc4(x)\n",
    "            # print(x1.shape, x2.shape)\n",
    "            return x1 + x2\n",
    "        else:\n",
    "            x = self.fc3(x)\n",
    "            # print(x.shape)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('../../test_data3.csv')\n",
    "codes = data['symbol'].unique()\n",
    "stock_df = data\n",
    "stock_df = stock_df[['Date','symbol','Open','High','Low','Close','Volume','Dividends','Stock Splits','Pctchange', 'Neg','Neu','Pos']]\n",
    "stock_df.columns = ['date', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'dividends','stock splits', 'pctchange','Neg', 'Neu', 'Pos']\n",
    "stock_df['pctchange'] = (stock_df['close'] - stock_df['open'])/stock_df['open']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finta import TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df['SMA42'] = TA.SMA(stock_df, 42)\n",
    "stock_df['SMA5'] = TA.SMA(stock_df, 5)\n",
    "stock_df['SMA15'] = TA.SMA(stock_df, 15)\n",
    "stock_df['AO'] = TA.AO(stock_df)\n",
    "stock_df['OVB'] = TA.OBV(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:00<00:00, 762.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 780.13it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train = stock_df[stock_df['date']<='2019-01-01'].groupby(['date','symbol']).agg('mean')\n",
    "# stock_df_train = stock_df_train[stock_df_train['date']>='2023-01-01']\n",
    "stock_df_test = stock_df[stock_df['date']>'2019-01-01']\n",
    "stock_df_test = stock_df_test[stock_df_test['date']<='2019-12-31'].groupby(['date','symbol']).agg('mean')\n",
    "\n",
    "train_date = sorted([x[0] for x in stock_df_train.index])\n",
    "test_date = sorted([x[0] for x in stock_df_test.index])\n",
    "\n",
    "# indicators = ['open', 'high', 'low', 'close', 'volume', 'positive', 'neutral', 'negative','SMA42', 'SMA5', 'SMA15', 'AO', 'OVB','VW_MACD',\n",
    "#        'MACD_SIGNAL', 'RSI', 'CMO']\n",
    "\n",
    "indicators = ['Neg','Neu','Pos']\n",
    "\n",
    "# indicators = ['pctchange', 'volume', 'positive', 'neutral', 'negative']\n",
    "# indicators = ['positive', 'neutral', 'negative']\n",
    "# indicators = ['sentiment']\n",
    "\n",
    "from tqdm import tqdm\n",
    "def get_full_data(x, date):\n",
    "        full_df = pd.DataFrame(0, index = codes, columns = x.columns)\n",
    "        full_df.loc[set(full_df.index).intersection(set(x.index))] = x.loc[set(full_df.index).intersection(set(x.index))]\n",
    "        v = full_df.values.reshape(1,-1)\n",
    "        # full_df['date']=date\n",
    "        return [date]+list(v[0])\n",
    "    \n",
    "dates = np.unique([x[0] for x in stock_df_train.index])\n",
    "res = []\n",
    "for date in tqdm(dates):\n",
    "    x = stock_df_train[indicators].loc[date]\n",
    "    res.append(get_full_data(x, date))\n",
    "\n",
    "# res = pd.concat(res).reset_index()\n",
    "# res.columns = ['tic', 'open', 'high', 'low', 'close', 'volume', 'positive',\n",
    "#        'neutral', 'negative', 'pctchange', 'date']\n",
    "stock_df_train_ = pd.DataFrame(res).set_index(0)\n",
    "\n",
    "dates = np.unique([x[0] for x in stock_df_test.index])\n",
    "res = []\n",
    "for date in tqdm(dates):\n",
    "    x = stock_df_test[indicators].loc[date]\n",
    "    res.append(get_full_data(x, date))\n",
    "    \n",
    "# res = pd.concat(res).reset_index()\n",
    "# res.columns = ['tic', 'open', 'high', 'low', 'close', 'volume', 'positive',\n",
    "#        'neutral', 'negative', 'pctchange', 'date']\n",
    "stock_df_test_ = pd.DataFrame(res).set_index(0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "stock_df_train_1 = scaler.fit_transform(stock_df_train_)\n",
    "stock_df_test_1 = scaler.transform(stock_df_test_)\n",
    "\n",
    "stock_df_train_ = pd.DataFrame(stock_df_train_1, index = stock_df_train_.index, columns = stock_df_train_.columns)\n",
    "stock_df_test_ = pd.DataFrame(stock_df_test_1, index = stock_df_test_.index, columns = stock_df_test_.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        # the first capacity-1 positions are not leaves\n",
    "        self.vals = [0 for _ in range(2*capacity - 1)] # think about why if you are not familiar with this\n",
    "        \n",
    "    def retrive(self, num):\n",
    "        '''\n",
    "        This function find the first index whose cumsum is no smaller than num\n",
    "        '''\n",
    "        ind = 0 # search from root\n",
    "        while ind < self.capacity-1: # not a leaf\n",
    "            left = 2*ind + 1\n",
    "            right = left + 1\n",
    "            if num > self.vals[left]: # the sum of the whole left tree is not large enouth\n",
    "                num -= self.vals[left] # think about why?\n",
    "                ind = right\n",
    "            else: # search in the left tree\n",
    "                ind = left\n",
    "        return ind - self.capacity + 1\n",
    "    \n",
    "    def update(self, delta, ind):\n",
    "        '''\n",
    "        Change the value at ind by delta, and update the tree\n",
    "        Notice that this ind should be the index in real memory part, instead of the ind in self.vals\n",
    "        '''\n",
    "        ind += self.capacity - 1\n",
    "        while True:\n",
    "            self.vals[ind] += delta\n",
    "            if ind == 0:\n",
    "                break\n",
    "            ind -= 1\n",
    "            ind //= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import deque\n",
    "\n",
    "# test = deque(maxlen=5)\n",
    "# for i in range(10):\n",
    "#     test.append(i)\n",
    "#     print(test)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import bisect\n",
    "import torch\n",
    "\n",
    "ALPHA = 0.5\n",
    "EPSILON = 0.05\n",
    "TD_INIT = 1\n",
    "\n",
    "class Replay_Buffer:\n",
    "    '''\n",
    "    Vanilla replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=None):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.memory = [None for _ in range(capacity)] # save tuples (state, action, reward, next_state, done)\n",
    "        self.ind_max = 0 # how many transitions have been stored\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        ind = self.ind_max % self.capacity\n",
    "        self.memory[ind] = (state, action, reward, next_state, done)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, k):\n",
    "        '''\n",
    "        return sampled transitions. Make sure that there are at least k transitions stored before calling this method \n",
    "        '''\n",
    "        index_set = random.sample(list(range(len(self))), k)\n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.ind_max, self.capacity)\n",
    "        \n",
    "class Rank_Replay_Buffer:\n",
    "    '''\n",
    "    Rank-based replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=64):\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = ALPHA\n",
    "        self.memory = [None for _ in range(capacity)]\n",
    "        self.segments = [-1] + [None for _ in range(batch_size)] # the ith index will be in [segments[i-1]+1, segments[i]]\n",
    "        \n",
    "        self.errors = [] # saves (-TD_error, index of transition), sorted\n",
    "        self.memory_to_rank = [None for _ in range(capacity)]\n",
    "        \n",
    "        self.ind_max = 0 # how many transitions have been stored\n",
    "        self.total_weights = 0 # sum of p_i\n",
    "        self.cumulated_weights = []\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        index = self.ind_max % self.capacity\n",
    "        if self.ind_max >= self.capacity: # memory is full, need to pop\n",
    "            self.pop(index)\n",
    "        else: # memory is not full, need to adjust weights and find segment points\n",
    "            self.total_weights += (1/(1+self.ind_max))**self.alpha # memory is not full, calculate new weights\n",
    "            self.cumulated_weights.append(self.total_weights)\n",
    "            self.update_segments()\n",
    "        \n",
    "        max_error = -self.errors[0][0] if self.errors else 0\n",
    "        self.insert(max_error, index)\n",
    "        self.memory[index] = (state, action, reward, next_state, done)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, batch_size=None): # notive that batch_size is not used. It's just to unify the calling form\n",
    "        index_set = [random.randint(self.segments[i]+1, self.segments[i+1]) for i in range(self.batch_size)]\n",
    "        probs = torch.from_numpy(np.vstack([(1/(1+ind))**self.alpha/self.total_weights for ind in index_set])).float()\n",
    "        \n",
    "        index_set = [self.errors[ind][1] for ind in index_set]\n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "        for ind in index_set:\n",
    "            self.pop(ind)\n",
    "        \n",
    "        return index_set, states, actions, rewards, next_states, dones, probs\n",
    "    \n",
    "    def insert(self, error, index):\n",
    "        '''\n",
    "        Input : \n",
    "            error : the TD-error of this transition\n",
    "            index : the location of this transition\n",
    "        insert error into self.errors, update self.memory_to_rank and self.rank_to_memory accordingly\n",
    "        '''\n",
    "        ind = bisect.bisect(self.errors, (-error, index))\n",
    "        self.memory_to_rank[index] = ind\n",
    "        self.errors.insert(ind, (-error, index))\n",
    "        for i in range(ind+1, len(self.errors)):\n",
    "            self.memory_to_rank[self.errors[i][1]] += 1\n",
    "        \n",
    "    def pop(self, index):\n",
    "        '''\n",
    "        Input :\n",
    "            index : the location of a transition\n",
    "        remove this transition, update self.memory_to_rank and self.rank_to_memory accordingly\n",
    "        '''\n",
    "        ind = self.memory_to_rank[index]\n",
    "        self.memory_to_rank[index] = None\n",
    "        self.errors.pop(ind)\n",
    "        for i in range(ind, len(self.errors)):\n",
    "            self.memory_to_rank[self.errors[i][1]] -= 1\n",
    "        \n",
    "    def update_segments(self):\n",
    "        '''\n",
    "        Update the segment points.\n",
    "        '''\n",
    "        if self.ind_max+1 < self.batch_size: # if there is no enough transitions\n",
    "            return None\n",
    "        for i in range(self.batch_size):\n",
    "            ind = bisect.bisect_left(self.cumulated_weights, self.total_weights*((i+1)/self.batch_size))\n",
    "            self.segments[i+1] = max(ind, self.segments[i]+1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return min(self.capacity, self.ind_max)\n",
    "    \n",
    "\n",
    "class Proportion_Replay_Buffer:\n",
    "    '''\n",
    "    Proportion-based replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=None):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = ALPHA\n",
    "        self.memory = [None for _ in range(capacity)]\n",
    "        self.weights = SumTree(self.capacity)\n",
    "        self.default = TD_INIT\n",
    "        self.ind_max = 0\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        index = self.ind_max % self.capacity\n",
    "        self.memory[index] = (state, action, reward, next_state, done)\n",
    "        delta = self.default+EPSILON - self.weights.vals[index+self.capacity-1]\n",
    "        self.weights.update(delta, index)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        index_set = [self.weights.retrive(self.weights.vals[0]*random.random()) for _ in range(batch_size)]\n",
    "        #print(index_set)\n",
    "        probs = torch.from_numpy(np.vstack([self.weights.vals[ind+self.capacity-1]/self.weights.vals[0] for ind in index_set])).float()                     \n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "\n",
    "        return index_set, states, actions, rewards, next_states, dones, probs\n",
    "                                 \n",
    "    def insert(self, error, index):\n",
    "        delta = error+EPSILON - self.weights.vals[index+self.capacity-1]\n",
    "        self.weights.update(delta, index)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return min(self.capacity, self.ind_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = None\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# from networks import *\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, device, visual=False):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size,duel=False).to(device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size,duel=False).to(device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = Proportion_Replay_Buffer(int(1e5), bs)\n",
    "        self.tst = None\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        index_set, states, actions, rewards, next_states, dones, probs = self.memory.sample(self.bs)\n",
    "        w = 1/len(self.memory)/probs\n",
    "        w = w/torch.max(w)\n",
    "        w = w.to(self.device)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        # print(states.shape)\n",
    "        Q_values = self.Q_local(states)\n",
    "        # print(actions.shape)\n",
    "        Q_values = torch.gather(input=Q_values, dim=-1, index=actions)\n",
    "        with torch.no_grad():\n",
    "            Q_targets = self.Q_target(next_states)\n",
    "            Q_targets, _ = torch.max(input=Q_targets, dim=-1, keepdim=True)\n",
    "            Q_targets = rewards + self.gamma * (1 - dones) * Q_targets\n",
    "    \n",
    "        deltas = Q_values - Q_targets\n",
    "        loss = (w*deltas.pow(2)).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        deltas = np.abs(deltas.detach().cpu().numpy().reshape(-1))\n",
    "        for i in range(self.bs):\n",
    "            self.memory.insert(deltas[i], index_set[i])\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stock_Env:\n",
    "    def __init__(self, initial_asset, data, cost, time, record,train=True,market=False, code='AAPL'):\n",
    "        self.asset = initial_asset\n",
    "        self.cash = initial_asset\n",
    "        self.stock = 0\n",
    "        self.stockvalue = 0\n",
    "        self.data = data\n",
    "        self.time = np.unique(time)\n",
    "        self.cost = cost\n",
    "        self.totalday = 0\n",
    "        self.history=[]\n",
    "        self.total_cost = 0\n",
    "        self.initial_asset = initial_asset\n",
    "        self.timeid = time_period\n",
    "        self.rowid = self.time[time_period]\n",
    "        self.action_space = 11\n",
    "        self.codeid = pd.DataFrame(range(len(codes)), index=codes)\n",
    "        self.record = record\n",
    "        self.train=train\n",
    "        self.market=market\n",
    "        self.code = code\n",
    "    \n",
    "    def reset(self):\n",
    "        self.asset = self.initial_asset\n",
    "        self.cash = self.initial_asset\n",
    "        self.stock = 0\n",
    "        self.stockvalue = 0\n",
    "        self.history=[]\n",
    "        self.total_cost = 0\n",
    "        if self.train:\n",
    "            temp_time = np.random.randint(time_period, len(self.time)-252)\n",
    "            self.rowid = self.time[temp_time]\n",
    "            while (self.rowid, self.code) not in self.data.index:\n",
    "                temp_time = np.random.randint(time_period, len(self.time)-252)\n",
    "                self.rowid = self.time[temp_time]\n",
    "            self.timeid = temp_time\n",
    "            self.totalday = temp_time\n",
    "        else:\n",
    "            temp_time = time_period\n",
    "            self.rowid = self.time[temp_time]\n",
    "            self.timeid = temp_time\n",
    "            self.totalday = temp_time\n",
    "        self.totalday = temp_time\n",
    "        temp = self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1],codes_dict[self.code]*3+1:codes_dict[self.code]*3+3].values.reshape(1,-1)\n",
    "        # print(temp.shape, self.stockvalue.shape)\n",
    "        return temp\n",
    "        # for i in range(time_period):\n",
    "        #     temp.append(list(self.get_full_data(self.data.loc[self.time[temp_time-time_period+i+1]]).values.reshape(-1)))       \n",
    "        # return np.array(temp)\n",
    "    \n",
    "    def get_full_data(self,x):\n",
    "        full_df = pd.DataFrame(0, index = self.codes, columns = x.columns)\n",
    "        full_df.loc[set(full_df.index).intersection(set(x.index))] = x.loc[set(full_df.index).intersection(set(x.index))]\n",
    "        return full_df\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        # print(self.timeid, self.totalday)\n",
    "        states = self.data.loc[self.rowid, self.code]   \n",
    "        self.timeid +=1\n",
    "        self.rowid = self.time[self.timeid]\n",
    "        self.totalday += 1\n",
    "        while (self.rowid, self.code) not in self.data.index:\n",
    "            self.timeid +=1\n",
    "            if (self.timeid != len(self.time)-1):\n",
    "                self.rowid = self.time[self.timeid]\n",
    "                self.totalday += 1\n",
    "            else:\n",
    "                return np.zeros(time_period*3), 0, True\n",
    "        if (self.timeid == len(self.time)-1):\n",
    "            done = True\n",
    "        if (self.train==True) and (self.totalday>=251) :\n",
    "            done = True\n",
    "        next_state = self.data.loc[self.rowid, self.code]\n",
    "        last_asset = self.asset\n",
    "        price = next_state['open']\n",
    "        old_asset = self.cash + self.stock*price\n",
    "        self.asset = old_asset\n",
    "        target_value = action*0.1*self.asset\n",
    "        distance = target_value - self.stock*price\n",
    "        stock_distance = int(distance/(price*(1+self.cost)))\n",
    "        self.stock += stock_distance\n",
    "        self.cash = self.cash - distance - np.abs(stock_distance*self.cost*price)\n",
    "        self.asset = self.cash+self.stock*price\n",
    "        market_value = self.stock * next_state['close']\n",
    "        self.asset = market_value + self.cash\n",
    "        reward = self.asset - last_asset\n",
    "        reward = reward/last_asset\n",
    "        # self.stock = stock\n",
    "        # print(self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1]])\n",
    "        return (self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1], codes_dict[self.code]*3+1:codes_dict[self.code]*3+3].values.reshape(1,-1), reward, done)\n",
    "\n",
    "#     def step(self, action):\n",
    "#         done = False\n",
    "#         states = self.data.loc[self.rowid]        \n",
    "#         self.timeid +=1\n",
    "#         self.rowid = self.time[self.timeid]\n",
    "#         if (self.timeid == len(self.time)-1):\n",
    "#             done = True\n",
    "#         if (self.train==True) and (self.totalday>=252) :\n",
    "#             dont = True\n",
    "#         self.totalday+=1\n",
    "#         next_state = self.data.loc[self.rowid]\n",
    "#         last_asset = self.asset\n",
    "#         idx = self.codeid.loc[next_state.index].values.reshape(1,-1)\n",
    "#         # Calculate the total assets at the beginning of the next day\n",
    "#         self.stockvalue[idx] = self.stock[idx].reshape(1,-1)*next_state['open'].values.reshape(1,-1)\n",
    "#         old_asset = self.cash + self.stockvalue.sum()\n",
    "        \n",
    "#         self.asset = old_asset\n",
    "#         # Calculate the position for each stock and cash\n",
    "#         action = np.exp(action)/np.exp(action).sum()\n",
    "#         # Get the stock asset value, where the last value of action is the position of cash.\n",
    "#         stockvalue_ = old_asset * (1-action[-1])\n",
    "        \n",
    "#         # Adjust the postion\n",
    "#         target_value = action*old_asset\n",
    "#         distance = target_value[:-1] - self.stockvalue\n",
    "#         stock_distance = (distance[idx].reshape(-1))/((next_state['open'].values*(1+self.cost)).astype(int).reshape(-1))\n",
    "#         # stock_distance /= 5\n",
    "#         self.stock[idx] += stock_distance\n",
    "#         self.cash = self.cash - distance[idx].sum() - np.abs(stock_distance*self.cost*next_state['open'].values).sum()\n",
    "#         self.stockvalue[idx] = self.stock[idx] * next_state['close'].values\n",
    "            \n",
    "#         # Calculate new asset\n",
    "#         self.asset = self.stockvalue.sum() + self.cash\n",
    "        \n",
    "#         reward = (self.asset - last_asset)/self.initial_asset\n",
    "#         if self.market:\n",
    "#             reward -= next_state['market'].values.mean()\n",
    "        \n",
    "#         # Generate new states\n",
    "#         temp = self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1]].values\n",
    "#         temp = np.concatenate((temp, self.stockvalue/(self.asset)),axis=None)\n",
    "#         # print(temp.shape)\n",
    "#         # for i in range(time_period):\n",
    "#         #     temp.append(list(self.get_full_data(self.data.loc[self.time[self.timeid-time_period+i+1]]).values.reshape(-1)))\n",
    "#         return (temp, reward, done)\n",
    "#         # return (self.data[self.rowid-time_period:self.rowid][indicators].values, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, train=True, code='META')\n",
    "env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, train=False, code='META')\n",
    "agent = Agent(2*3, 11, 64, 0.001, 0.001, 0.99, 'cuda', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.995\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    # agent.mu=0\n",
    "    env.mu=[0]\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    state = frame.reshape(-1)\n",
    "    t = 0\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        rewards_log.append(reward)\n",
    "        episodic_reward += reward\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset().reshape(-1)\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        # state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            frame = frame.reshape(-1)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            # next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "\n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Reward 0.082, Average Reward 0.101, valReward 0.030, val Average Reward 0.137, Asset 1084160.56, Validation Asset 1019392.06, Average Validation Sharpe 0.98\n",
      "Episode 200, Reward 0.122, Average Reward 0.145, valReward 0.196, val Average Reward 0.126, Asset 1127154.06, Validation Asset 1207427.74, Average Validation Sharpe 0.90\n",
      "Episode 300, Reward 0.129, Average Reward 0.179, valReward 0.283, val Average Reward 0.140, Asset 1136195.73, Validation Asset 1310643.77, Average Validation Sharpe 1.00\n",
      "Episode 400, Reward 0.242, Average Reward 0.199, valReward 0.181, val Average Reward 0.150, Asset 1268522.25, Validation Asset 1182523.00, Average Validation Sharpe 1.06\n",
      "Episode 435, Reward 0.246, Average Reward 0.199, valReward 0.191, val Average Reward 0.138, Asset 1275521.92, Validation Asset 1197626.99, Average Validation Sharpe 0.97"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44792\\223791933.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcodes_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_frame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44792\\4126901690.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame, constant)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoft_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44792\\1295659562.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mQ_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mQ_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mQ_targets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mQ_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mQ_targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mdeltas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mQ_values\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mQ_targets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))\n",
    "train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.129, Average Reward 0.032, valReward 0.297, val Average Reward 0.287, Asset 1132380.56, Validation Asset 1327628.71, Average Validation Sharpe 2.08\n",
      "Episode 200, Reward 0.177, Average Reward 0.108, valReward 0.355, val Average Reward 0.280, Asset 1188433.05, Validation Asset 1411277.41, Average Validation Sharpe 2.00\n",
      "Episode 300, Reward 0.159, Average Reward 0.154, valReward 0.376, val Average Reward 0.280, Asset 1167362.36, Validation Asset 1438848.95, Average Validation Sharpe 2.02\n",
      "Episode 400, Reward 0.313, Average Reward 0.215, valReward 0.329, val Average Reward 0.276, Asset 1361017.75, Validation Asset 1375814.26, Average Validation Sharpe 2.01\n",
      "Episode 500, Reward 0.342, Average Reward 0.220, valReward 0.324, val Average Reward 0.283, Asset 1399216.84, Validation Asset 1370050.60, Average Validation Sharpe 2.04\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.247, Average Reward 0.098, valReward 0.085, val Average Reward -0.059, Asset 1267910.57, Validation Asset 1065475.73, Average Validation Sharpe -0.31\n",
      "Episode 200, Reward 0.334, Average Reward 0.187, valReward -0.279, val Average Reward -0.047, Asset 1381170.67, Validation Asset 739767.53, Average Validation Sharpe -0.24\n",
      "Episode 300, Reward 0.404, Average Reward 0.275, valReward 0.098, val Average Reward -0.045, Asset 1482370.29, Validation Asset 1076747.24, Average Validation Sharpe -0.23\n",
      "Episode 400, Reward 0.046, Average Reward 0.361, valReward 0.066, val Average Reward -0.049, Asset 1046431.23, Validation Asset 1045468.60, Average Validation Sharpe -0.26\n",
      "Episode 500, Reward 0.174, Average Reward 0.376, valReward -0.033, val Average Reward -0.046, Asset 1187093.04, Validation Asset 950327.53, Average Validation Sharpe -0.24\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.046, Average Reward 0.079, valReward -0.002, val Average Reward -0.005, Asset 1041644.16, Validation Asset 990958.28, Average Validation Sharpe -0.04\n",
      "Episode 200, Reward 0.330, Average Reward 0.116, valReward 0.004, val Average Reward -0.006, Asset 1383478.43, Validation Asset 994665.53, Average Validation Sharpe -0.055\n",
      "Episode 300, Reward 0.029, Average Reward 0.170, valReward -0.120, val Average Reward -0.018, Asset 1029495.01, Validation Asset 878017.75, Average Validation Sharpe -0.14\n",
      "Episode 400, Reward 0.034, Average Reward 0.198, valReward 0.096, val Average Reward -0.021, Asset 1034296.03, Validation Asset 1090998.67, Average Validation Sharpe -0.17\n",
      "Episode 500, Reward 0.346, Average Reward 0.238, valReward -0.003, val Average Reward -0.007, Asset 1398458.89, Validation Asset 988467.73, Average Validation Sharpe -0.04\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.087, Average Reward 0.078, valReward 0.095, val Average Reward 0.144, Asset 1089906.83, Validation Asset 1086716.96, Average Validation Sharpe 1.00\n",
      "Episode 200, Reward 0.112, Average Reward 0.100, valReward 0.160, val Average Reward 0.145, Asset 1116514.54, Validation Asset 1160435.00, Average Validation Sharpe 1.01\n",
      "Episode 300, Reward 0.206, Average Reward 0.124, valReward 0.179, val Average Reward 0.133, Asset 1225411.68, Validation Asset 1182797.39, Average Validation Sharpe 0.94\n",
      "Episode 400, Reward 0.190, Average Reward 0.149, valReward 0.212, val Average Reward 0.160, Asset 1205676.40, Validation Asset 1221527.35, Average Validation Sharpe 1.12\n",
      "Episode 500, Reward 0.251, Average Reward 0.177, valReward 0.144, val Average Reward 0.128, Asset 1281006.62, Validation Asset 1135168.94, Average Validation Sharpe 0.91\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.017, Average Reward 0.048, valReward 0.047, val Average Reward 0.058, Asset 1015073.16, Validation Asset 1038426.18, Average Validation Sharpe 0.50\n",
      "Episode 200, Reward 0.059, Average Reward 0.083, valReward 0.048, val Average Reward 0.068, Asset 1058584.33, Validation Asset 1043778.76, Average Validation Sharpe 0.57\n",
      "Episode 300, Reward 0.128, Average Reward 0.119, valReward 0.129, val Average Reward 0.052, Asset 1133727.01, Validation Asset 1129434.70, Average Validation Sharpe 0.45\n",
      "Episode 400, Reward 0.126, Average Reward 0.137, valReward -0.010, val Average Reward 0.054, Asset 1132394.06, Validation Asset 985253.09, Average Validation Sharpe 0.46\n",
      "Episode 500, Reward 0.370, Average Reward 0.156, valReward 0.154, val Average Reward 0.070, Asset 1440915.29, Validation Asset 1155442.97, Average Validation Sharpe 0.63\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, train=True, code=code)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, train=False, code=code)\n",
    "    agent = Agent(2*3, 11, 64, 0.001, 0.001, 0.99, 'cuda', True)\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
