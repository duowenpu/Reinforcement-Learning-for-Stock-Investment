{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import quantstats as qs\n",
    "import torch.nn.functional as F\n",
    "time_period = 2\n",
    "    \n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, N, hidden=[64, 64]):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size*N)\n",
    "\n",
    "        self.action_size = action_size\n",
    "        self.N = N\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state #(batch_size, state_size)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #(batch_size, action_size*N)\n",
    "        x = x.view(-1, self.action_size, self.N) #(batch_size, action_size, N)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_41404\\1183940963.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['pctchange'] = (stock_df['close'] - stock_df['open'])/stock_df['open']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('../../test_data3.csv')\n",
    "codes = data['symbol'].unique()\n",
    "stock_df = data\n",
    "stock_df = stock_df[['Date','symbol','Open','High','Low','Close','Volume','Dividends','Stock Splits','Pctchange', 'Neg','Neu','Pos']]\n",
    "stock_df.columns = ['date', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'dividends','stock splits', 'pctchange','Neg', 'Neu', 'Pos']\n",
    "stock_df['pctchange'] = (stock_df['close'] - stock_df['open'])/stock_df['open']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finta import TA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df['SMA42'] = TA.SMA(stock_df, 42)\n",
    "stock_df['SMA5'] = TA.SMA(stock_df, 5)\n",
    "stock_df['SMA15'] = TA.SMA(stock_df, 15)\n",
    "stock_df['AO'] = TA.AO(stock_df)\n",
    "stock_df['OVB'] = TA.OBV(stock_df)\n",
    "stock_df[['VW_MACD','MACD_SIGNAL']] = TA.VW_MACD(stock_df)\n",
    "stock_df['RSI'] = TA.RSI(stock_df)\n",
    "stock_df['CMO'] = TA.CMO(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:00<00:00, 669.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 711.54it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train = stock_df[stock_df['date']<='2019-01-01'].groupby(['date','symbol']).agg('mean')\n",
    "# stock_df_train = stock_df_train[stock_df_train['date']>='2023-01-01']\n",
    "stock_df_test = stock_df[stock_df['date']>'2019-01-01']\n",
    "stock_df_test = stock_df_test[stock_df_test['date']<='2019-12-31'].groupby(['date','symbol']).agg('mean')\n",
    "\n",
    "train_date = sorted([x[0] for x in stock_df_train.index])\n",
    "test_date = sorted([x[0] for x in stock_df_test.index])\n",
    "\n",
    "# indicators = ['open', 'high', 'low', 'close', 'volume', 'positive', 'neutral', 'negative','SMA42', 'SMA5', 'SMA15', 'AO', 'OVB','VW_MACD',\n",
    "#        'MACD_SIGNAL', 'RSI', 'CMO']\n",
    "\n",
    "indicators = ['Neg','Neu','Pos']\n",
    "\n",
    "# indicators = ['pctchange', 'volume', 'positive', 'neutral', 'negative']\n",
    "# indicators = ['positive', 'neutral', 'negative']\n",
    "# indicators = ['sentiment']\n",
    "\n",
    "from tqdm import tqdm\n",
    "def get_full_data(x, date):\n",
    "        full_df = pd.DataFrame(0, index = codes, columns = x.columns)\n",
    "        full_df.loc[set(full_df.index).intersection(set(x.index))] = x.loc[set(full_df.index).intersection(set(x.index))]\n",
    "        v = full_df.values.reshape(1,-1)\n",
    "        # full_df['date']=date\n",
    "        return [date]+list(v[0])\n",
    "    \n",
    "dates = np.unique([x[0] for x in stock_df_train.index])\n",
    "res = []\n",
    "for date in tqdm(dates):\n",
    "    x = stock_df_train[indicators].loc[date]\n",
    "    res.append(get_full_data(x, date))\n",
    "\n",
    "# res = pd.concat(res).reset_index()\n",
    "# res.columns = ['tic', 'open', 'high', 'low', 'close', 'volume', 'positive',\n",
    "#        'neutral', 'negative', 'pctchange', 'date']\n",
    "stock_df_train_ = pd.DataFrame(res).set_index(0)\n",
    "\n",
    "dates = np.unique([x[0] for x in stock_df_test.index])\n",
    "res = []\n",
    "for date in tqdm(dates):\n",
    "    x = stock_df_test[indicators].loc[date]\n",
    "    res.append(get_full_data(x, date))\n",
    "    \n",
    "# res = pd.concat(res).reset_index()\n",
    "# res.columns = ['tic', 'open', 'high', 'low', 'close', 'volume', 'positive',\n",
    "#        'neutral', 'negative', 'pctchange', 'date']\n",
    "stock_df_test_ = pd.DataFrame(res).set_index(0)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "stock_df_train_1 = scaler.fit_transform(stock_df_train_)\n",
    "stock_df_test_1 = scaler.transform(stock_df_test_)\n",
    "\n",
    "stock_df_train_ = pd.DataFrame(stock_df_train_1, index = stock_df_train_.index, columns = stock_df_train_.columns)\n",
    "stock_df_test_ = pd.DataFrame(stock_df_test_1, index = stock_df_test_.index, columns = stock_df_test_.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([0], maxlen=5)\n",
      "deque([0, 1], maxlen=5)\n",
      "deque([0, 1, 2], maxlen=5)\n",
      "deque([0, 1, 2, 3], maxlen=5)\n",
      "deque([0, 1, 2, 3, 4], maxlen=5)\n",
      "deque([1, 2, 3, 4, 5], maxlen=5)\n",
      "deque([2, 3, 4, 5, 6], maxlen=5)\n",
      "deque([3, 4, 5, 6, 7], maxlen=5)\n",
      "deque([4, 5, 6, 7, 8], maxlen=5)\n",
      "deque([5, 6, 7, 8, 9], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "test = deque(maxlen=5)\n",
    "for i in range(10):\n",
    "    test.append(i)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, N, kappa, device, visual=False):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.tau = torch.linspace(0, 1, N+1)\n",
    "        self.tau = (self.tau[:-1] + self.tau[1:]) / 2\n",
    "        self.tau_hat = self.tau.to(device).unsqueeze(0) #(1, N)\n",
    "        self.tau = tau\n",
    "        self.kappa = kappa\n",
    "\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size, N).to(self.device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size, N).to(self.device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state)\n",
    "                action_values = action_values.mean(dim=-1, keepdim=False).view(-1)\n",
    "            return np.argmax(action_values.cpu().numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        quantiles_local = self.Q_local(states) #(batch_size, action_size, N)\n",
    "        quantiles_local = torch.gather(input=quantiles_local, dim=1, index=actions.unsqueeze(1).repeat(1, 1, self.N)) #(batch_size, 1, N)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            quantiles_target = self.Q_target(next_states) #(batch_size, action_size, N)\n",
    "            next_actions = torch.max(quantiles_target.sum(dim=2, keepdim=True), dim=1, keepdim=True)[1] #(batch_size, 1, 1)\n",
    "            quantiles_target = torch.gather(input=quantiles_target, index=next_actions.repeat(1,1,self.N), dim=1) #(batch_size, 1, N)\n",
    "            quantiles_target = rewards.unsqueeze(1).repeat(1, 1, self.N) + self.gamma * (1 - dones.unsqueeze(1).repeat(1, 1, self.N)) * quantiles_target #(batch_size, 1, N)\n",
    "            quantiles_target = quantiles_target.permute(0, 2, 1) #(batch_size, N, 1)\n",
    "\n",
    "        diff = quantiles_target - quantiles_local #(batch_size, N, N)\n",
    "        tau = self.tau_hat.unsqueeze(0).repeat(diff.size(0), 1, 1) #(batch_size, 1, N)\n",
    "        loss = (tau - (diff<0).float()).abs() * self.huber(diff) #(batch_size, N, N)\n",
    "        loss = loss.mean(dim=2, keepdim=False).sum(dim=1, keepdim=False) #(batch_size,)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.tau)\n",
    "\n",
    "    def huber(self, u):\n",
    "        if self.kappa > 0:\n",
    "            flag = (u.abs()<self.kappa).float()\n",
    "            loss = 0.5*u.pow(2) * flag + self.kappa*(u.abs()-0.5*self.kappa) * (1-flag)\n",
    "        else:\n",
    "            loss = u.abs()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stock_Env:\n",
    "    def __init__(self, initial_asset, data, cost, time, record,train=True,market=False, code='AAPL'):\n",
    "        self.asset = initial_asset\n",
    "        self.cash = initial_asset\n",
    "        self.stock = 0\n",
    "        self.stockvalue = 0\n",
    "        self.data = data\n",
    "        self.time = np.unique(time)\n",
    "        self.cost = cost\n",
    "        self.totalday = 0\n",
    "        self.history=[]\n",
    "        self.total_cost = 0\n",
    "        self.initial_asset = initial_asset\n",
    "        self.timeid = time_period\n",
    "        self.rowid = self.time[time_period]\n",
    "        self.action_space = 11\n",
    "        self.codeid = pd.DataFrame(range(len(codes)), index=codes)\n",
    "        self.record = record\n",
    "        self.train=train\n",
    "        self.market=market\n",
    "        self.code = code\n",
    "    \n",
    "    def reset(self):\n",
    "        self.asset = self.initial_asset\n",
    "        self.cash = self.initial_asset\n",
    "        self.stock = 0\n",
    "        self.stockvalue = 0\n",
    "        self.history=[]\n",
    "        self.total_cost = 0\n",
    "        if self.train:\n",
    "            temp_time = np.random.randint(time_period, len(self.time)-252)\n",
    "            self.rowid = self.time[temp_time]\n",
    "            while (self.rowid, self.code) not in self.data.index:\n",
    "                temp_time = np.random.randint(time_period, len(self.time)-252)\n",
    "                self.rowid = self.time[temp_time]\n",
    "            self.timeid = temp_time\n",
    "            self.totalday = temp_time\n",
    "        else:\n",
    "            temp_time = time_period\n",
    "            self.rowid = self.time[temp_time]\n",
    "            self.timeid = temp_time\n",
    "            self.totalday = temp_time\n",
    "        self.totalday = temp_time\n",
    "        temp = self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1],codes_dict[self.code]*3+1:codes_dict[self.code]*3+3].values.reshape(1,-1)\n",
    "        # print(temp.shape, self.stockvalue.shape)\n",
    "        return temp\n",
    "        # for i in range(time_period):\n",
    "        #     temp.append(list(self.get_full_data(self.data.loc[self.time[temp_time-time_period+i+1]]).values.reshape(-1)))       \n",
    "        # return np.array(temp)\n",
    "    \n",
    "    def get_full_data(self,x):\n",
    "        full_df = pd.DataFrame(0, index = self.codes, columns = x.columns)\n",
    "        full_df.loc[set(full_df.index).intersection(set(x.index))] = x.loc[set(full_df.index).intersection(set(x.index))]\n",
    "        return full_df\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        # print(self.timeid, self.totalday)\n",
    "        states = self.data.loc[self.rowid, self.code]   \n",
    "        self.timeid +=1\n",
    "        self.rowid = self.time[self.timeid]\n",
    "        self.totalday += 1\n",
    "        while (self.rowid, self.code) not in self.data.index:\n",
    "            self.timeid +=1\n",
    "            if (self.timeid != len(self.time)-1):\n",
    "                self.rowid = self.time[self.timeid]\n",
    "                self.totalday += 1\n",
    "            else:\n",
    "                return np.zeros(time_period*3), 0, True\n",
    "        if (self.timeid == len(self.time)-1):\n",
    "            done = True\n",
    "        if (self.train==True) and (self.totalday>=251) :\n",
    "            done = True\n",
    "        next_state = self.data.loc[self.rowid, self.code]\n",
    "        last_asset = self.asset\n",
    "        price = next_state['open']\n",
    "        old_asset = self.cash + self.stock*price\n",
    "        self.asset = old_asset\n",
    "        target_value = action*0.1*self.asset\n",
    "        distance = target_value - self.stock*price\n",
    "        stock_distance = int(distance/(price*(1+self.cost)))\n",
    "        self.stock += stock_distance\n",
    "        self.cash = self.cash - distance - np.abs(stock_distance*self.cost*price)\n",
    "        self.asset = self.cash+self.stock*price\n",
    "        market_value = self.stock * next_state['close']\n",
    "        self.asset = market_value + self.cash\n",
    "        reward = self.asset - last_asset\n",
    "        reward = reward/last_asset\n",
    "        # self.stock = stock\n",
    "        # print(self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1]])\n",
    "        return (self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1], codes_dict[self.code]*3+1:codes_dict[self.code]*3+3].values.reshape(1,-1), reward, done)\n",
    "\n",
    "#     def step(self, action):\n",
    "#         done = False\n",
    "#         states = self.data.loc[self.rowid]        \n",
    "#         self.timeid +=1\n",
    "#         self.rowid = self.time[self.timeid]\n",
    "#         if (self.timeid == len(self.time)-1):\n",
    "#             done = True\n",
    "#         if (self.train==True) and (self.totalday>=252) :\n",
    "#             dont = True\n",
    "#         self.totalday+=1\n",
    "#         next_state = self.data.loc[self.rowid]\n",
    "#         last_asset = self.asset\n",
    "#         idx = self.codeid.loc[next_state.index].values.reshape(1,-1)\n",
    "#         # Calculate the total assets at the beginning of the next day\n",
    "#         self.stockvalue[idx] = self.stock[idx].reshape(1,-1)*next_state['open'].values.reshape(1,-1)\n",
    "#         old_asset = self.cash + self.stockvalue.sum()\n",
    "        \n",
    "#         self.asset = old_asset\n",
    "#         # Calculate the position for each stock and cash\n",
    "#         action = np.exp(action)/np.exp(action).sum()\n",
    "#         # Get the stock asset value, where the last value of action is the position of cash.\n",
    "#         stockvalue_ = old_asset * (1-action[-1])\n",
    "        \n",
    "#         # Adjust the postion\n",
    "#         target_value = action*old_asset\n",
    "#         distance = target_value[:-1] - self.stockvalue\n",
    "#         stock_distance = (distance[idx].reshape(-1))/((next_state['open'].values*(1+self.cost)).astype(int).reshape(-1))\n",
    "#         # stock_distance /= 5\n",
    "#         self.stock[idx] += stock_distance\n",
    "#         self.cash = self.cash - distance[idx].sum() - np.abs(stock_distance*self.cost*next_state['open'].values).sum()\n",
    "#         self.stockvalue[idx] = self.stock[idx] * next_state['close'].values\n",
    "            \n",
    "#         # Calculate new asset\n",
    "#         self.asset = self.stockvalue.sum() + self.cash\n",
    "        \n",
    "#         reward = (self.asset - last_asset)/self.initial_asset\n",
    "#         if self.market:\n",
    "#             reward -= next_state['market'].values.mean()\n",
    "        \n",
    "#         # Generate new states\n",
    "#         temp = self.record.loc[self.time[self.timeid+1-time_period:self.timeid+1]].values\n",
    "#         temp = np.concatenate((temp, self.stockvalue/(self.asset)),axis=None)\n",
    "#         # print(temp.shape)\n",
    "#         # for i in range(time_period):\n",
    "#         #     temp.append(list(self.get_full_data(self.data.loc[self.time[self.timeid-time_period+i+1]]).values.reshape(-1)))\n",
    "#         return (temp, reward, done)\n",
    "#         # return (self.data[self.rowid-time_period:self.rowid][indicators].values, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, train=True, code='META')\n",
    "env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, train=False, code='META')\n",
    "agent = Agent(2*3, env.action_space, 64, 0.001, 0.001, 0.99, 51, 1, 'cuda', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    state = env.reset()\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        next_state = frame\n",
    "        state = next_state.copy()\n",
    "        episodic_reward += reward\n",
    "        rewards_log.append(reward)\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "        \n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes_dict = dict(zip(codes, range(len(codes))))\n",
    "# train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.204, Average Reward 0.045, valReward 0.140, val Average Reward 0.270, Asset 1218694.66, Validation Asset 1139655.53, Average Validation Sharpe 1.98\n",
      "Episode 200, Reward 0.166, Average Reward 0.108, valReward 0.293, val Average Reward 0.276, Asset 1175208.86, Validation Asset 1327910.01, Average Validation Sharpe 1.99\n",
      "Episode 300, Reward 0.241, Average Reward 0.156, valReward 0.340, val Average Reward 0.285, Asset 1263943.29, Validation Asset 1390256.49, Average Validation Sharpe 2.07\n",
      "Episode 400, Reward 0.055, Average Reward 0.170, valReward 0.307, val Average Reward 0.281, Asset 1054461.37, Validation Asset 1346884.47, Average Validation Sharpe 2.07\n",
      "Episode 500, Reward 0.367, Average Reward 0.231, valReward 0.291, val Average Reward 0.279, Asset 1435238.46, Validation Asset 1323391.66, Average Validation Sharpe 2.05\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.003, Average Reward 0.091, valReward -0.145, val Average Reward -0.046, Asset 1001768.38, Validation Asset 848950.74, Average Validation Sharpe -0.24\n",
      "Episode 200, Reward 0.093, Average Reward 0.178, valReward -0.032, val Average Reward -0.057, Asset 1096264.40, Validation Asset 949573.98, Average Validation Sharpe -0.29\n",
      "Episode 300, Reward 0.146, Average Reward 0.255, valReward 0.074, val Average Reward -0.049, Asset 1154381.10, Validation Asset 1057952.26, Average Validation Sharpe -0.26\n",
      "Episode 400, Reward 0.493, Average Reward 0.310, valReward -0.059, val Average Reward -0.052, Asset 1618229.11, Validation Asset 924244.81, Average Validation Sharpe -0.27\n",
      "Episode 500, Reward 0.440, Average Reward 0.353, valReward 0.015, val Average Reward -0.036, Asset 1536438.66, Validation Asset 998054.54, Average Validation Sharpe -0.188\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.116, Average Reward 0.075, valReward -0.072, val Average Reward -0.018, Asset 1115706.74, Validation Asset 923196.06, Average Validation Sharpe -0.14\n",
      "Episode 200, Reward 0.095, Average Reward 0.105, valReward -0.077, val Average Reward -0.005, Asset 1096030.09, Validation Asset 918648.30, Average Validation Sharpe -0.03\n",
      "Episode 300, Reward 0.238, Average Reward 0.149, valReward -0.097, val Average Reward -0.026, Asset 1261507.91, Validation Asset 899515.40, Average Validation Sharpe -0.20\n",
      "Episode 400, Reward 0.236, Average Reward 0.168, valReward -0.076, val Average Reward -0.012, Asset 1252614.98, Validation Asset 918660.01, Average Validation Sharpe -0.10\n",
      "Episode 500, Reward 0.296, Average Reward 0.218, valReward -0.017, val Average Reward -0.016, Asset 1337130.90, Validation Asset 973927.15, Average Validation Sharpe -0.12\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.109, Average Reward 0.078, valReward 0.155, val Average Reward 0.140, Asset 1113618.09, Validation Asset 1154304.28, Average Validation Sharpe 0.98\n",
      "Episode 200, Reward 0.272, Average Reward 0.092, valReward 0.198, val Average Reward 0.126, Asset 1307575.81, Validation Asset 1198352.34, Average Validation Sharpe 0.89\n",
      "Episode 300, Reward 0.129, Average Reward 0.144, valReward 0.068, val Average Reward 0.134, Asset 1135367.94, Validation Asset 1058572.39, Average Validation Sharpe 0.95\n",
      "Episode 400, Reward 0.165, Average Reward 0.143, valReward 0.157, val Average Reward 0.142, Asset 1177542.79, Validation Asset 1157196.69, Average Validation Sharpe 0.99\n",
      "Episode 500, Reward 0.128, Average Reward 0.172, valReward 0.050, val Average Reward 0.137, Asset 1135783.66, Validation Asset 1039811.11, Average Validation Sharpe 0.99\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.065, Average Reward 0.039, valReward 0.036, val Average Reward 0.068, Asset 1066005.45, Validation Asset 1026131.54, Average Validation Sharpe 0.60\n",
      "Episode 200, Reward 0.114, Average Reward 0.059, valReward 0.135, val Average Reward 0.065, Asset 1119400.58, Validation Asset 1137783.40, Average Validation Sharpe 0.56\n",
      "Episode 300, Reward 0.043, Average Reward 0.097, valReward -0.043, val Average Reward 0.048, Asset 1043011.72, Validation Asset 950987.74, Average Validation Sharpe 0.42\n",
      "Episode 400, Reward 0.175, Average Reward 0.128, valReward 0.034, val Average Reward 0.054, Asset 1186427.02, Validation Asset 1027692.18, Average Validation Sharpe 0.47\n",
      "Episode 500, Reward 0.130, Average Reward 0.155, valReward 0.190, val Average Reward 0.052, Asset 1137488.80, Validation Asset 1198072.93, Average Validation Sharpe 0.46\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, train=True, code=code)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, train=False, code=code)\n",
    "    agent = Agent(2*3, env.action_space, 64, 0.001, 0.001, 0.99, 51, 1, 'cuda', True)\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
