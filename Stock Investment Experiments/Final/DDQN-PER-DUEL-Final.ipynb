{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "time_period = 2\n",
    "sys.path.append('./')\n",
    "from utlis import get_data, Stock_Env\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import deque\n",
    "\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden=[64, 64], duel=False):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size)\n",
    "        self.duel = duel\n",
    "        if self.duel:\n",
    "            self.fc4 = nn.Linear(hidden[1], 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        if self.duel:\n",
    "            x1 = self.fc3(x)\n",
    "            x1 = x1 - torch.max(x1, dim=1, keepdim=True)[0] # set the max to be 0\n",
    "            x2 = self.fc4(x)\n",
    "            # print(x1.shape, x2.shape)\n",
    "            return x1 + x2\n",
    "        else:\n",
    "            x = self.fc3(x)\n",
    "            # print(x.shape)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:00<00:00, 617.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252/252 [00:00<00:00, 393.02it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train, stock_df_test, stock_df_train_, stock_df_test_, codes = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        # the first capacity-1 positions are not leaves\n",
    "        self.vals = [0 for _ in range(2*capacity - 1)] # think about why if you are not familiar with this\n",
    "        \n",
    "    def retrive(self, num):\n",
    "        '''\n",
    "        This function find the first index whose cumsum is no smaller than num\n",
    "        '''\n",
    "        ind = 0 # search from root\n",
    "        while ind < self.capacity-1: # not a leaf\n",
    "            left = 2*ind + 1\n",
    "            right = left + 1\n",
    "            if num > self.vals[left]: # the sum of the whole left tree is not large enouth\n",
    "                num -= self.vals[left] # think about why?\n",
    "                ind = right\n",
    "            else: # search in the left tree\n",
    "                ind = left\n",
    "        return ind - self.capacity + 1\n",
    "    \n",
    "    def update(self, delta, ind):\n",
    "        '''\n",
    "        Change the value at ind by delta, and update the tree\n",
    "        Notice that this ind should be the index in real memory part, instead of the ind in self.vals\n",
    "        '''\n",
    "        ind += self.capacity - 1\n",
    "        while True:\n",
    "            self.vals[ind] += delta\n",
    "            if ind == 0:\n",
    "                break\n",
    "            ind -= 1\n",
    "            ind //= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import deque\n",
    "\n",
    "# test = deque(maxlen=5)\n",
    "# for i in range(10):\n",
    "#     test.append(i)\n",
    "#     print(test)\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import bisect\n",
    "import torch\n",
    "\n",
    "ALPHA = 0.5\n",
    "EPSILON = 0.05\n",
    "TD_INIT = 1\n",
    "\n",
    "class Replay_Buffer:\n",
    "    '''\n",
    "    Vanilla replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=None):\n",
    "        \n",
    "        self.capacity = capacity\n",
    "        self.memory = [None for _ in range(capacity)] # save tuples (state, action, reward, next_state, done)\n",
    "        self.ind_max = 0 # how many transitions have been stored\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \n",
    "        ind = self.ind_max % self.capacity\n",
    "        self.memory[ind] = (state, action, reward, next_state, done)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, k):\n",
    "        '''\n",
    "        return sampled transitions. Make sure that there are at least k transitions stored before calling this method \n",
    "        '''\n",
    "        index_set = random.sample(list(range(len(self))), k)\n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return min(self.ind_max, self.capacity)\n",
    "        \n",
    "class Rank_Replay_Buffer:\n",
    "    '''\n",
    "    Rank-based replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=64):\n",
    "        self.capacity = capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = ALPHA\n",
    "        self.memory = [None for _ in range(capacity)]\n",
    "        self.segments = [-1] + [None for _ in range(batch_size)] # the ith index will be in [segments[i-1]+1, segments[i]]\n",
    "        \n",
    "        self.errors = [] # saves (-TD_error, index of transition), sorted\n",
    "        self.memory_to_rank = [None for _ in range(capacity)]\n",
    "        \n",
    "        self.ind_max = 0 # how many transitions have been stored\n",
    "        self.total_weights = 0 # sum of p_i\n",
    "        self.cumulated_weights = []\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        index = self.ind_max % self.capacity\n",
    "        if self.ind_max >= self.capacity: # memory is full, need to pop\n",
    "            self.pop(index)\n",
    "        else: # memory is not full, need to adjust weights and find segment points\n",
    "            self.total_weights += (1/(1+self.ind_max))**self.alpha # memory is not full, calculate new weights\n",
    "            self.cumulated_weights.append(self.total_weights)\n",
    "            self.update_segments()\n",
    "        \n",
    "        max_error = -self.errors[0][0] if self.errors else 0\n",
    "        self.insert(max_error, index)\n",
    "        self.memory[index] = (state, action, reward, next_state, done)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, batch_size=None): # notive that batch_size is not used. It's just to unify the calling form\n",
    "        index_set = [random.randint(self.segments[i]+1, self.segments[i+1]) for i in range(self.batch_size)]\n",
    "        probs = torch.from_numpy(np.vstack([(1/(1+ind))**self.alpha/self.total_weights for ind in index_set])).float()\n",
    "        \n",
    "        index_set = [self.errors[ind][1] for ind in index_set]\n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "        for ind in index_set:\n",
    "            self.pop(ind)\n",
    "        \n",
    "        return index_set, states, actions, rewards, next_states, dones, probs\n",
    "    \n",
    "    def insert(self, error, index):\n",
    "        '''\n",
    "        Input : \n",
    "            error : the TD-error of this transition\n",
    "            index : the location of this transition\n",
    "        insert error into self.errors, update self.memory_to_rank and self.rank_to_memory accordingly\n",
    "        '''\n",
    "        ind = bisect.bisect(self.errors, (-error, index))\n",
    "        self.memory_to_rank[index] = ind\n",
    "        self.errors.insert(ind, (-error, index))\n",
    "        for i in range(ind+1, len(self.errors)):\n",
    "            self.memory_to_rank[self.errors[i][1]] += 1\n",
    "        \n",
    "    def pop(self, index):\n",
    "        '''\n",
    "        Input :\n",
    "            index : the location of a transition\n",
    "        remove this transition, update self.memory_to_rank and self.rank_to_memory accordingly\n",
    "        '''\n",
    "        ind = self.memory_to_rank[index]\n",
    "        self.memory_to_rank[index] = None\n",
    "        self.errors.pop(ind)\n",
    "        for i in range(ind, len(self.errors)):\n",
    "            self.memory_to_rank[self.errors[i][1]] -= 1\n",
    "        \n",
    "    def update_segments(self):\n",
    "        '''\n",
    "        Update the segment points.\n",
    "        '''\n",
    "        if self.ind_max+1 < self.batch_size: # if there is no enough transitions\n",
    "            return None\n",
    "        for i in range(self.batch_size):\n",
    "            ind = bisect.bisect_left(self.cumulated_weights, self.total_weights*((i+1)/self.batch_size))\n",
    "            self.segments[i+1] = max(ind, self.segments[i]+1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return min(self.capacity, self.ind_max)\n",
    "    \n",
    "\n",
    "class Proportion_Replay_Buffer:\n",
    "    '''\n",
    "    Proportion-based replay buffer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, capacity=int(1e6), batch_size=None):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = ALPHA\n",
    "        self.memory = [None for _ in range(capacity)]\n",
    "        self.weights = SumTree(self.capacity)\n",
    "        self.default = TD_INIT\n",
    "        self.ind_max = 0\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        index = self.ind_max % self.capacity\n",
    "        self.memory[index] = (state, action, reward, next_state, done)\n",
    "        delta = self.default+EPSILON - self.weights.vals[index+self.capacity-1]\n",
    "        self.weights.update(delta, index)\n",
    "        self.ind_max += 1\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        index_set = [self.weights.retrive(self.weights.vals[0]*random.random()) for _ in range(batch_size)]\n",
    "        #print(index_set)\n",
    "        probs = torch.from_numpy(np.vstack([self.weights.vals[ind+self.capacity-1]/self.weights.vals[0] for ind in index_set])).float()                     \n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([self.memory[ind][0] for ind in index_set])).float()\n",
    "        actions = torch.from_numpy(np.vstack([self.memory[ind][1] for ind in index_set])).long()\n",
    "        rewards = torch.from_numpy(np.vstack([self.memory[ind][2] for ind in index_set])).float()\n",
    "        next_states = torch.from_numpy(np.vstack([self.memory[ind][3] for ind in index_set])).float()\n",
    "        dones = torch.from_numpy(np.vstack([self.memory[ind][4] for ind in index_set]).astype(np.uint8)).float()\n",
    "\n",
    "        return index_set, states, actions, rewards, next_states, dones, probs\n",
    "                                 \n",
    "    def insert(self, error, index):\n",
    "        delta = error+EPSILON - self.weights.vals[index+self.capacity-1]\n",
    "        self.weights.update(delta, index)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return min(self.capacity, self.ind_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = None\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# from networks import *\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, device, visual=False):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size,duel=False).to(device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size,duel=False).to(device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = Proportion_Replay_Buffer(int(1e5), bs)\n",
    "        self.tst = None\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        index_set, states, actions, rewards, next_states, dones, probs = self.memory.sample(self.bs)\n",
    "        w = 1/len(self.memory)/probs\n",
    "        w = w/torch.max(w)\n",
    "        w = w.to(self.device)\n",
    "        states = states.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_states = next_states.to(self.device)\n",
    "        dones = dones.to(self.device)\n",
    "        # print(states.shape)\n",
    "        Q_values = self.Q_local(states)\n",
    "        # print(actions.shape)\n",
    "        Q_values = torch.gather(input=Q_values, dim=-1, index=actions)\n",
    "        with torch.no_grad():\n",
    "            Q_targets = self.Q_target(next_states)\n",
    "            Q_targets, _ = torch.max(input=Q_targets, dim=-1, keepdim=True)\n",
    "            Q_targets = rewards + self.gamma * (1 - dones) * Q_targets\n",
    "    \n",
    "        deltas = Q_values - Q_targets\n",
    "        loss = (w*deltas.pow(2)).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        deltas = np.abs(deltas.detach().cpu().numpy().reshape(-1))\n",
    "        for i in range(self.bs):\n",
    "            self.memory.insert(deltas[i], index_set[i])\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.995\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    # agent.mu=0\n",
    "    env.mu=[0]\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    state = frame.reshape(-1)\n",
    "    t = 0\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        rewards_log.append(reward)\n",
    "        episodic_reward += reward\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset().reshape(-1)\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        # state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            frame = frame.reshape(-1)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            # next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "\n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.100, Average Reward 0.046, valReward 0.259, val Average Reward 0.262, Asset 1103120.56, Validation Asset 1284929.95, Average Validation Sharpe 1.89\n",
      "Episode 200, Reward 0.349, Average Reward 0.109, valReward 0.262, val Average Reward 0.260, Asset 1410067.80, Validation Asset 1287450.05, Average Validation Sharpe 1.90\n",
      "Episode 300, Reward 0.300, Average Reward 0.173, valReward 0.257, val Average Reward 0.270, Asset 1343793.23, Validation Asset 1280955.57, Average Validation Sharpe 1.98\n",
      "Episode 400, Reward 0.420, Average Reward 0.222, valReward 0.367, val Average Reward 0.272, Asset 1512824.96, Validation Asset 1427073.90, Average Validation Sharpe 1.96\n",
      "Episode 500, Reward 0.183, Average Reward 0.229, valReward 0.206, val Average Reward 0.271, Asset 1198104.69, Validation Asset 1219503.35, Average Validation Sharpe 1.95\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.148, Average Reward 0.076, valReward 0.101, val Average Reward -0.004, Asset 1154058.11, Validation Asset 1094305.39, Average Validation Sharpe -0.04\n",
      "Episode 200, Reward 0.189, Average Reward 0.147, valReward -0.090, val Average Reward -0.014, Asset 1204658.40, Validation Asset 905052.06, Average Validation Sharpe -0.11\n",
      "Episode 300, Reward 0.294, Average Reward 0.215, valReward -0.034, val Average Reward -0.004, Asset 1335427.70, Validation Asset 959219.24, Average Validation Sharpe -0.03\n",
      "Episode 400, Reward 0.184, Average Reward 0.253, valReward 0.139, val Average Reward 0.002, Asset 1200215.41, Validation Asset 1142271.92, Average Validation Sharpe 0.0100\n",
      "Episode 500, Reward 0.057, Average Reward 0.274, valReward -0.031, val Average Reward -0.007, Asset 1058076.20, Validation Asset 961443.85, Average Validation Sharpe -0.06\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.048, Average Reward 0.098, valReward -0.134, val Average Reward -0.026, Asset 1047273.85, Validation Asset 861698.22, Average Validation Sharpe -0.13\n",
      "Episode 200, Reward 0.280, Average Reward 0.209, valReward -0.145, val Average Reward -0.009, Asset 1314957.81, Validation Asset 846185.72, Average Validation Sharpe -0.04\n",
      "Episode 300, Reward 0.281, Average Reward 0.314, valReward 0.105, val Average Reward -0.026, Asset 1319241.15, Validation Asset 1092036.17, Average Validation Sharpe -0.13\n",
      "Episode 400, Reward 0.540, Average Reward 0.386, valReward -0.003, val Average Reward -0.044, Asset 1705225.15, Validation Asset 977403.55, Average Validation Sharpe -0.23\n",
      "Episode 500, Reward 0.748, Average Reward 0.423, valReward -0.115, val Average Reward -0.022, Asset 2088092.83, Validation Asset 879287.06, Average Validation Sharpe -0.11\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.065, Average Reward 0.083, valReward 0.200, val Average Reward 0.155, Asset 1063129.09, Validation Asset 1204551.84, Average Validation Sharpe 1.05\n",
      "Episode 200, Reward 0.091, Average Reward 0.099, valReward 0.091, val Average Reward 0.155, Asset 1093920.66, Validation Asset 1080458.20, Average Validation Sharpe 1.05\n",
      "Episode 300, Reward -0.006, Average Reward 0.124, valReward 0.123, val Average Reward 0.151, Asset 994391.50, Validation Asset 1117480.50, Average Validation Sharpe 1.03\n",
      "Episode 400, Reward 0.169, Average Reward 0.127, valReward 0.207, val Average Reward 0.146, Asset 1182458.33, Validation Asset 1216791.46, Average Validation Sharpe 0.99\n",
      "Episode 500, Reward 0.210, Average Reward 0.130, valReward 0.153, val Average Reward 0.146, Asset 1230152.34, Validation Asset 1147465.95, Average Validation Sharpe 0.99\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.068, Average Reward 0.039, valReward -0.062, val Average Reward 0.057, Asset 1069374.83, Validation Asset 932215.17, Average Validation Sharpe 0.46\n",
      "Episode 200, Reward 0.122, Average Reward 0.058, valReward 0.093, val Average Reward 0.056, Asset 1125771.21, Validation Asset 1082247.43, Average Validation Sharpe 0.44\n",
      "Episode 300, Reward 0.140, Average Reward 0.104, valReward 0.083, val Average Reward 0.049, Asset 1147009.32, Validation Asset 1072639.86, Average Validation Sharpe 0.38\n",
      "Episode 400, Reward 0.102, Average Reward 0.129, valReward -0.055, val Average Reward 0.057, Asset 1104814.44, Validation Asset 936794.97, Average Validation Sharpe 0.47\n",
      "Episode 500, Reward 0.229, Average Reward 0.155, valReward 0.004, val Average Reward 0.049, Asset 1254717.45, Validation Asset 997930.09, Average Validation Sharpe 0.398\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, codes_dict=codes_dict, train=True, code=code, time_period = time_period, codes=codes)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, codes_dict=codes_dict, train=False, code=code, time_period = time_period,  codes=codes)\n",
    "    agent = Agent(2*3, 11, 64, 0.001, 0.001, 0.99, 'cuda', True)\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
