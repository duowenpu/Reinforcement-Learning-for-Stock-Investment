{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import quantstats as qs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "time_period = 2\n",
    "sys.path.append('./')\n",
    "from utlis import get_data, Stock_Env\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import deque\n",
    "\n",
    "class Q_Network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden=[64, 64]):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden[0])\n",
    "        self.fc2 = nn.Linear(hidden[0], hidden[1])\n",
    "        self.fc3 = nn.Linear(hidden[1], action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = state\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 502/502 [00:02<00:00, 195.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 252/252 [00:01<00:00, 189.98it/s]\n"
     ]
    }
   ],
   "source": [
    "stock_df_train, stock_df_test, stock_df_train_, stock_df_test_, codes = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_dict = dict(zip(codes, range(len(codes))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = None\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# from networks import *\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, device):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size).to(device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size).to(device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.tst = None\n",
    "        self.mu = [0]\n",
    "        self.last_action = 0\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state).reshape(-1)\n",
    "            if (action_values).max() > np.max(self.mu):\n",
    "                # self.mu = 0.95*self.mu + 0.05*action_values.max()\n",
    "\n",
    "                self.mu.append(action_values.max().cpu().data.numpy())                \n",
    "                if len(self.mu) > 10:\n",
    "                    self.mu = self.mu[-10:]\n",
    "                self.last_action = np.argmax(action_values.cpu().data.numpy())\n",
    "                return self.last_action\n",
    "            else:\n",
    "                return self.last_action\n",
    "        else:\n",
    "            action = random.choice(np.arange(self.action_size))\n",
    "            self.last_action = action\n",
    "            return action\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "        self.tst = states\n",
    "        Q_values = self.Q_local(states).reshape(-1,11)\n",
    "        Q_values = torch.gather(input=Q_values, dim=-1, index=actions)\n",
    "        with torch.no_grad():\n",
    "            Q_targets = self.Q_target(next_states)\n",
    "            Q_targets, _ = torch.max(input=Q_targets, dim=-1, keepdim=True)\n",
    "            Q_targets = rewards + self.gamma * (1 - dones) * Q_targets\n",
    "\n",
    "        loss = (Q_values - Q_targets).pow(2).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 500\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    # agent.mu=0\n",
    "    env.mu=[0]\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    frame = env.reset()\n",
    "    state = frame\n",
    "    t = 0\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        rewards_log.append(reward)\n",
    "        episodic_reward += reward\n",
    "    sharpe = qs.stats.sharpe(pd.DataFrame(rewards_log))\n",
    "    return env.asset, episodic_reward, sharpe\n",
    "\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    # global rewards_log, average_log, state_history, action_history, done_history, reward_history\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    state_history = []\n",
    "    action_history = []\n",
    "    done_history = []\n",
    "    reward_history = []\n",
    "    validation_log = []\n",
    "    validation_average_log = []\n",
    "    sharpe_log = []\n",
    "    average_sharpe = []\n",
    "    eps = eps_init\n",
    "    for i in range(1, 1 + num_episode):\n",
    "        env.mu=[0]\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "\n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        val_asset, val_reward, val_sharpe = validation(env_test, agent)\n",
    "\n",
    "        validation_log.append(val_reward)\n",
    "        validation_average_log.append(np.mean(validation_log[-100:]))\n",
    "        sharpe_log.append(val_sharpe.values[0])\n",
    "        average_sharpe.append(np.mean(sharpe_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, valReward {:.3f}, val Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}, Average Validation Sharpe {:.2f}'.format(i, episodic_reward, average_log[-1], val_reward, validation_average_log[-1], env.asset, val_asset, average_sharpe[-1]), end='')\n",
    "        # print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}'.format(i, episodic_reward, average_log[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.062, Average Reward 0.037, valReward 0.462, val Average Reward 0.267, Asset 1057783.72, Validation Asset 1567928.03, Average Validation Sharpe 1.93\n",
      "Episode 200, Reward 0.065, Average Reward 0.040, valReward 0.447, val Average Reward 0.275, Asset 1061808.40, Validation Asset 1546606.59, Average Validation Sharpe 1.98\n",
      "Episode 300, Reward 0.115, Average Reward 0.057, valReward 0.286, val Average Reward 0.264, Asset 1115427.41, Validation Asset 1320320.42, Average Validation Sharpe 1.94\n",
      "Episode 400, Reward 0.007, Average Reward 0.057, valReward 0.201, val Average Reward 0.273, Asset 1006620.31, Validation Asset 1208987.13, Average Validation Sharpe 1.98\n",
      "Episode 500, Reward -0.002, Average Reward 0.063, valReward 0.318, val Average Reward 0.279, Asset 997702.20, Validation Asset 1360210.87, Average Validation Sharpe 2.05\n",
      "AMZN  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.014, Average Reward 0.067, valReward -0.056, val Average Reward -0.028, Asset 1013367.22, Validation Asset 936392.02, Average Validation Sharpe -0.22\n",
      "Episode 200, Reward 0.081, Average Reward 0.086, valReward 0.027, val Average Reward -0.022, Asset 1081299.63, Validation Asset 1019959.39, Average Validation Sharpe -0.17\n",
      "Episode 300, Reward 0.007, Average Reward 0.083, valReward 0.047, val Average Reward -0.008, Asset 1006621.75, Validation Asset 1038413.77, Average Validation Sharpe -0.06\n",
      "Episode 400, Reward 0.127, Average Reward 0.092, valReward -0.019, val Average Reward -0.020, Asset 1125162.10, Validation Asset 972316.67, Average Validation Sharpe -0.15\n",
      "Episode 500, Reward 0.089, Average Reward 0.101, valReward -0.003, val Average Reward -0.010, Asset 1087741.70, Validation Asset 988579.68, Average Validation Sharpe -0.08\n",
      "NFLX  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward -0.115, Average Reward 0.073, valReward 0.020, val Average Reward -0.029, Asset 885253.16, Validation Asset 1004037.73, Average Validation Sharpe -0.15\n",
      "Episode 200, Reward 0.136, Average Reward 0.083, valReward -0.063, val Average Reward -0.026, Asset 1127599.82, Validation Asset 923081.17, Average Validation Sharpe -0.13\n",
      "Episode 300, Reward 0.019, Average Reward 0.099, valReward -0.260, val Average Reward -0.039, Asset 1012668.22, Validation Asset 755740.85, Average Validation Sharpe -0.20\n",
      "Episode 400, Reward 0.150, Average Reward 0.089, valReward -0.198, val Average Reward -0.033, Asset 1151780.24, Validation Asset 801872.12, Average Validation Sharpe -0.16\n",
      "Episode 500, Reward 0.030, Average Reward 0.100, valReward -0.010, val Average Reward -0.019, Asset 1022038.85, Validation Asset 972160.77, Average Validation Sharpe -0.09\n",
      "META  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.112, Average Reward 0.082, valReward 0.266, val Average Reward 0.165, Asset 1114948.75, Validation Asset 1285438.63, Average Validation Sharpe 1.10\n",
      "Episode 200, Reward 0.116, Average Reward 0.099, valReward 0.140, val Average Reward 0.153, Asset 1120760.12, Validation Asset 1131033.80, Average Validation Sharpe 1.05\n",
      "Episode 300, Reward 0.020, Average Reward 0.089, valReward 0.174, val Average Reward 0.152, Asset 1018614.05, Validation Asset 1173014.53, Average Validation Sharpe 1.01\n",
      "Episode 400, Reward 0.138, Average Reward 0.091, valReward 0.156, val Average Reward 0.154, Asset 1145026.17, Validation Asset 1152879.17, Average Validation Sharpe 1.03\n",
      "Episode 500, Reward 0.098, Average Reward 0.092, valReward 0.230, val Average Reward 0.156, Asset 1101123.37, Validation Asset 1240485.66, Average Validation Sharpe 1.05\n",
      "GOOGL  Begins\n",
      "---------------------------------------------\n",
      "Episode 100, Reward 0.059, Average Reward 0.030, valReward -0.104, val Average Reward 0.046, Asset 1058121.42, Validation Asset 893944.60, Average Validation Sharpe 0.37\n",
      "Episode 200, Reward 0.025, Average Reward 0.038, valReward -0.021, val Average Reward 0.048, Asset 1023543.07, Validation Asset 968904.47, Average Validation Sharpe 0.39\n",
      "Episode 300, Reward 0.068, Average Reward 0.045, valReward -0.057, val Average Reward 0.047, Asset 1067225.32, Validation Asset 938502.92, Average Validation Sharpe 0.36\n",
      "Episode 400, Reward 0.001, Average Reward 0.037, valReward 0.128, val Average Reward 0.056, Asset 1001269.82, Validation Asset 1127852.54, Average Validation Sharpe 0.43\n",
      "Episode 500, Reward 0.116, Average Reward 0.055, valReward 0.119, val Average Reward 0.051, Asset 1118641.90, Validation Asset 1115174.18, Average Validation Sharpe 0.41\n"
     ]
    }
   ],
   "source": [
    "for code in codes:\n",
    "    print(code, ' Begins')\n",
    "    print('---------------------------------------------')\n",
    "    env = Stock_Env(1000000, stock_df_train, 0.001, time = [x[0] for x in stock_df_train.index], record = stock_df_train_, codes_dict=codes_dict, train=True, code=code, time_period = time_period, codes=codes)\n",
    "    env_test = Stock_Env(1000000, stock_df_test, 0.001, time = [x[0] for x in stock_df_test.index], record = stock_df_test_, codes_dict=codes_dict, train=False, code=code, time_period = time_period,  codes=codes)\n",
    "    agent = Agent(2*3, 11, 64, 0.001, 0.001, 0.99, 'cuda')\n",
    "    train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
