{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "time_period = 15\n",
    "class Q_Network(nn.Module):\n",
    "    '''\n",
    "    The input of this network should have shape (num_frame, 80, 80)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_frame, num_action, N, Vmin, Vmax):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_frame, out_channels=32, kernel_size=(2,1), stride=1, padding=2)  # 16, 20, 20\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,2), stride=1)  # 32, 9, 9\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2,1))\n",
    "        self.fc1 = nn.Linear(576, 256)\n",
    "        self.fc2 = nn.Linear(256, num_action*N)\n",
    "        self.action_size = num_action\n",
    "        self.N = N\n",
    "        self.values = torch.linspace(Vmin, Vmax, N).view(1, 1, -1).to('cuda')\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = F.relu(self.pool(self.conv1(image)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = F.relu(self.pool(self.conv3(x)))\n",
    "        x = x.view(-1, 576)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, self.action_size, self.N)\n",
    "        log_probs = F.log_softmax(x, dim=2)  # (batch_size, action_size, N)\n",
    "        Q_values = log_probs.exp() * self.values\n",
    "        Q_values = Q_values.sum(dim=2, keepdims=False)\n",
    "        return log_probs, Q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "file = open('../../FinBert/stock_data_full.bin', 'rb')\n",
    "data = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['AAPL','AMZN','C','GOOG','JPM','NFLX','PLTR']\n",
    "for i in range(len(codes)):\n",
    "    data[i]['symbol'] = codes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../FinRL/concat_data.csv')\n",
    "df=df[['date', 'open', 'high', 'low', 'close', 'volume',\n",
    "       'positive', 'neutral', 'negative', 'tic']]\n",
    "df['date'] = [x[:10] for x in df['date']]\n",
    "df = df[(df['date']>='2022-01-01') & (df['date']<'2023-09-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../min_data_adjust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = data[data['symbol']=='AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>vwap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:00:00+00:00</td>\n",
       "      <td>176.23</td>\n",
       "      <td>176.23</td>\n",
       "      <td>176.1800</td>\n",
       "      <td>176.1800</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>176.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:02:00+00:00</td>\n",
       "      <td>176.30</td>\n",
       "      <td>176.31</td>\n",
       "      <td>176.2800</td>\n",
       "      <td>176.2800</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>176.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:03:00+00:00</td>\n",
       "      <td>176.25</td>\n",
       "      <td>176.27</td>\n",
       "      <td>176.2500</td>\n",
       "      <td>176.2700</td>\n",
       "      <td>814.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>176.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:04:00+00:00</td>\n",
       "      <td>176.20</td>\n",
       "      <td>176.20</td>\n",
       "      <td>176.1200</td>\n",
       "      <td>176.1200</td>\n",
       "      <td>3744.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>176.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:05:00+00:00</td>\n",
       "      <td>176.17</td>\n",
       "      <td>176.17</td>\n",
       "      <td>176.1700</td>\n",
       "      <td>176.1700</td>\n",
       "      <td>464.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>176.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343433</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:53:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>209.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>171.305766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343434</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:54:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>810.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>171.305889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343435</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:57:00+00:00</td>\n",
       "      <td>171.32</td>\n",
       "      <td>171.32</td>\n",
       "      <td>171.3200</td>\n",
       "      <td>171.3200</td>\n",
       "      <td>439.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>171.330957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343436</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:58:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.2699</td>\n",
       "      <td>171.2699</td>\n",
       "      <td>532.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>171.282998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343437</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:59:00+00:00</td>\n",
       "      <td>171.23</td>\n",
       "      <td>171.27</td>\n",
       "      <td>171.2300</td>\n",
       "      <td>171.2300</td>\n",
       "      <td>3114.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>171.235370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343438 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       symbol                  timestamp    open    high       low     close  \\\n",
       "0        AAPL  2022-01-03 09:00:00+00:00  176.23  176.23  176.1800  176.1800   \n",
       "1        AAPL  2022-01-03 09:02:00+00:00  176.30  176.31  176.2800  176.2800   \n",
       "2        AAPL  2022-01-03 09:03:00+00:00  176.25  176.27  176.2500  176.2700   \n",
       "3        AAPL  2022-01-03 09:04:00+00:00  176.20  176.20  176.1200  176.1200   \n",
       "4        AAPL  2022-01-03 09:05:00+00:00  176.17  176.17  176.1700  176.1700   \n",
       "...       ...                        ...     ...     ...       ...       ...   \n",
       "343433   AAPL  2023-09-29 23:53:00+00:00  171.30  171.30  171.3000  171.3000   \n",
       "343434   AAPL  2023-09-29 23:54:00+00:00  171.30  171.30  171.3000  171.3000   \n",
       "343435   AAPL  2023-09-29 23:57:00+00:00  171.32  171.32  171.3200  171.3200   \n",
       "343436   AAPL  2023-09-29 23:58:00+00:00  171.30  171.30  171.2699  171.2699   \n",
       "343437   AAPL  2023-09-29 23:59:00+00:00  171.23  171.27  171.2300  171.2300   \n",
       "\n",
       "        volume  trade_count        vwap  \n",
       "0       1118.0         65.0  176.210000  \n",
       "1       1218.0         26.0  176.300000  \n",
       "2        814.0         30.0  176.260000  \n",
       "3       3744.0        114.0  176.180000  \n",
       "4        464.0         33.0  176.150000  \n",
       "...        ...          ...         ...  \n",
       "343433   209.0          8.0  171.305766  \n",
       "343434   810.0         16.0  171.305889  \n",
       "343435   439.0         20.0  171.330957  \n",
       "343436   532.0         11.0  171.282998  \n",
       "343437  3114.0         19.0  171.235370  \n",
       "\n",
       "[343438 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = df[df['tic']=='AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>tic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>177.830002</td>\n",
       "      <td>182.880005</td>\n",
       "      <td>177.710007</td>\n",
       "      <td>182.009995</td>\n",
       "      <td>104487900</td>\n",
       "      <td>-2.525743</td>\n",
       "      <td>3.722111</td>\n",
       "      <td>-3.922445</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>182.630005</td>\n",
       "      <td>182.940002</td>\n",
       "      <td>179.119995</td>\n",
       "      <td>179.699997</td>\n",
       "      <td>99310400</td>\n",
       "      <td>-2.752612</td>\n",
       "      <td>3.370780</td>\n",
       "      <td>-3.351379</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>179.610001</td>\n",
       "      <td>180.169998</td>\n",
       "      <td>174.639999</td>\n",
       "      <td>174.919998</td>\n",
       "      <td>94537600</td>\n",
       "      <td>-2.561095</td>\n",
       "      <td>3.561730</td>\n",
       "      <td>-3.588621</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>172.699997</td>\n",
       "      <td>175.300003</td>\n",
       "      <td>171.639999</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>96904000</td>\n",
       "      <td>-2.294448</td>\n",
       "      <td>3.207229</td>\n",
       "      <td>-3.612424</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>172.889999</td>\n",
       "      <td>174.139999</td>\n",
       "      <td>171.029999</td>\n",
       "      <td>172.169998</td>\n",
       "      <td>86709100</td>\n",
       "      <td>-2.325235</td>\n",
       "      <td>3.084295</td>\n",
       "      <td>-3.352122</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>2023-09-25</td>\n",
       "      <td>174.199997</td>\n",
       "      <td>176.970001</td>\n",
       "      <td>174.149994</td>\n",
       "      <td>176.080002</td>\n",
       "      <td>46172700</td>\n",
       "      <td>-2.361765</td>\n",
       "      <td>3.181928</td>\n",
       "      <td>-3.037302</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>2023-09-26</td>\n",
       "      <td>174.820007</td>\n",
       "      <td>175.199997</td>\n",
       "      <td>171.660004</td>\n",
       "      <td>171.960007</td>\n",
       "      <td>64588900</td>\n",
       "      <td>-1.893191</td>\n",
       "      <td>2.688069</td>\n",
       "      <td>-3.369864</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>2023-09-27</td>\n",
       "      <td>172.619995</td>\n",
       "      <td>173.039993</td>\n",
       "      <td>169.050003</td>\n",
       "      <td>170.429993</td>\n",
       "      <td>66921800</td>\n",
       "      <td>-3.139558</td>\n",
       "      <td>3.359877</td>\n",
       "      <td>-2.654129</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>2023-09-28</td>\n",
       "      <td>169.339996</td>\n",
       "      <td>172.029999</td>\n",
       "      <td>167.619995</td>\n",
       "      <td>170.690002</td>\n",
       "      <td>56294400</td>\n",
       "      <td>-2.045589</td>\n",
       "      <td>2.791628</td>\n",
       "      <td>-3.063628</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>172.020004</td>\n",
       "      <td>173.070007</td>\n",
       "      <td>170.339996</td>\n",
       "      <td>171.210007</td>\n",
       "      <td>51814200</td>\n",
       "      <td>-2.751975</td>\n",
       "      <td>3.445238</td>\n",
       "      <td>-3.111207</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date        open        high         low       close     volume  \\\n",
       "17    2022-01-03  177.830002  182.880005  177.710007  182.009995  104487900   \n",
       "21    2022-01-04  182.630005  182.940002  179.119995  179.699997   99310400   \n",
       "26    2022-01-05  179.610001  180.169998  174.639999  174.919998   94537600   \n",
       "35    2022-01-06  172.699997  175.300003  171.639999  172.000000   96904000   \n",
       "42    2022-01-07  172.889999  174.139999  171.029999  172.169998   86709100   \n",
       "...          ...         ...         ...         ...         ...        ...   \n",
       "3028  2023-09-25  174.199997  176.970001  174.149994  176.080002   46172700   \n",
       "3034  2023-09-26  174.820007  175.199997  171.660004  171.960007   64588900   \n",
       "3045  2023-09-27  172.619995  173.039993  169.050003  170.429993   66921800   \n",
       "3049  2023-09-28  169.339996  172.029999  167.619995  170.690002   56294400   \n",
       "3057  2023-09-29  172.020004  173.070007  170.339996  171.210007   51814200   \n",
       "\n",
       "      positive   neutral  negative   tic  \n",
       "17   -2.525743  3.722111 -3.922445  AAPL  \n",
       "21   -2.752612  3.370780 -3.351379  AAPL  \n",
       "26   -2.561095  3.561730 -3.588621  AAPL  \n",
       "35   -2.294448  3.207229 -3.612424  AAPL  \n",
       "42   -2.325235  3.084295 -3.352122  AAPL  \n",
       "...        ...       ...       ...   ...  \n",
       "3028 -2.361765  3.181928 -3.037302  AAPL  \n",
       "3034 -1.893191  2.688069 -3.369864  AAPL  \n",
       "3045 -3.139558  3.359877 -2.654129  AAPL  \n",
       "3049 -2.045589  2.791628 -3.063628  AAPL  \n",
       "3057 -2.751975  3.445238 -3.111207  AAPL  \n",
       "\n",
       "[438 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df['pctchange'] = (stock_df['close'] - stock_df['open'])/stock_df['open']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finta import TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df['SMA42'] = TA.SMA(stock_df, 42)\n",
    "stock_df['SMA5'] = TA.SMA(stock_df, 5)\n",
    "stock_df['SMA15'] = TA.SMA(stock_df, 15)\n",
    "stock_df['AO'] = TA.AO(stock_df)\n",
    "stock_df['OVB'] = TA.OBV(stock_df)\n",
    "stock_df[['VW_MACD','MACD_SIGNAL']] = TA.VW_MACD(stock_df)\n",
    "stock_df['RSI'] = TA.RSI(stock_df)\n",
    "stock_df['CMO'] = TA.CMO(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'open', 'high', 'low', 'close', 'volume', 'positive', 'neutral',\n",
       "       'negative', 'tic', 'SMA42', 'SMA5', 'SMA15', 'AO', 'OVB', 'VW_MACD',\n",
       "       'MACD_SIGNAL', 'RSI', 'CMO', 'pctchange'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df_train = stock_df[stock_df['date']<='2023-03-31']\n",
    "stock_df_test = stock_df[stock_df['date']>'2023-03-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([0], maxlen=5)\n",
      "deque([0, 1], maxlen=5)\n",
      "deque([0, 1, 2], maxlen=5)\n",
      "deque([0, 1, 2, 3], maxlen=5)\n",
      "deque([0, 1, 2, 3, 4], maxlen=5)\n",
      "deque([1, 2, 3, 4, 5], maxlen=5)\n",
      "deque([2, 3, 4, 5, 6], maxlen=5)\n",
      "deque([3, 4, 5, 6, 7], maxlen=5)\n",
      "deque([4, 5, 6, 7, 8], maxlen=5)\n",
      "deque([5, 6, 7, 8, 9], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "test = deque(maxlen=5)\n",
    "for i in range(10):\n",
    "    test.append(i)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from networks import *\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, N, Vmin, Vmax, device, visual=False):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.Vmin = Vmin\n",
    "        self.Vmax = Vmax\n",
    "        self.vals = torch.linspace(Vmin, Vmax, N).to(device)\n",
    "        self.unit = (Vmax - Vmin) / (N - 1)\n",
    "\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size, N, Vmin, Vmax).to(self.device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size, N, Vmin, Vmax).to(self.device)\n",
    "\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                _, action_values = self.Q_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        log_probs, _ = self.Q_local(states) #(batch_size, action_size, N)\n",
    "        log_probs = torch.gather(input=log_probs, dim=1, index=actions.unsqueeze(1).repeat(1, 1, self.N)) #(batch_size, 1, N)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            log_probs_targets, Q_targets = self.Q_target(next_states)\n",
    "            _, actions_target = torch.max(input=Q_targets, dim=1, keepdim=True)#(batch_size, 1) the same size as actions\n",
    "            log_probs_targets = torch.gather(input=log_probs_targets, dim=1, index=actions_target.unsqueeze(1).repeat(1, 1, self.N))\n",
    "            target_distribution = self.update_distribution(log_probs_targets.exp(), rewards, dones) #(batch_size, 1, N)\n",
    "\n",
    "        loss = -target_distribution*log_probs #D_KL(target||local)\n",
    "        #loss = -log_probs.exp()*((target_distribution+1e-9).log() - log_probs) #D_KL(local||target)\n",
    "\n",
    "        loss = loss.sum(dim=2, keepdims=False).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_distribution(self, old_distribution, reward, dones):\n",
    "        with torch.no_grad():\n",
    "            reward = reward.view(-1, 1)\n",
    "            batch_size = reward.size(0)\n",
    "            assert old_distribution.size(0) == batch_size\n",
    "            new_vals = self.vals.view(1, -1) * self.gamma * (1-dones) + reward\n",
    "            new_vals = torch.clamp(new_vals, self.Vmin, self.Vmax)\n",
    "            lower = torch.floor((new_vals - self.Vmin) / self.unit).long().to(self.device)\n",
    "            upper = torch.min(lower + 1, other=torch.tensor(self.N - 1)).to(self.device)\n",
    "            lower_vals = self.vals[lower]\n",
    "            lower_probs = 1 - torch.min((new_vals - lower_vals) / self.unit, other=torch.tensor(1, dtype=torch.float32)).to(self.device)\n",
    "            transit = torch.zeros((batch_size, self.N, self.N)).to(self.device)\n",
    "            first_dim = torch.tensor(range(batch_size), dtype=torch.long).view(-1, 1).repeat(1, self.N).view(-1).to(self.device)\n",
    "            second_dim = torch.tensor(range(self.N), dtype=torch.long).repeat(batch_size).to(self.device)\n",
    "            transit[first_dim, second_dim, lower.view(-1)] += lower_probs.view(-1)\n",
    "            transit[first_dim, second_dim, upper.view(-1)] += 1 - lower_probs.view(-1)\n",
    "            if len(old_distribution.size()) == 2:\n",
    "                old_distribution = old_distribution.unsqueeze(1)\n",
    "            return torch.bmm(old_distribution, transit)\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicators = ['open', 'high', 'low', 'close', 'volume', 'positive', 'neutral', 'negative','SMA42', 'SMA5', 'SMA15', 'AO', 'OVB','VW_MACD',\n",
    "#        'MACD_SIGNAL', 'RSI', 'CMO']\n",
    "\n",
    "indicators = ['pctchange', 'volume', 'positive', 'neutral', 'negative','SMA42', 'SMA5', 'SMA15', 'AO', 'OVB','VW_MACD',\n",
    "       'MACD_SIGNAL', 'RSI', 'CMO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stock_Env:\n",
    "    def __init__(self, initial_asset, data, cost):\n",
    "        self.asset = initial_asset\n",
    "        self.cash = initial_asset\n",
    "        self.stock = 0\n",
    "        self.data = data\n",
    "        self.time = data.iloc[time_period]['date']\n",
    "        self.cost = cost\n",
    "        self.history=[]\n",
    "        self.total_cost = 0\n",
    "        self.initial_asset = initial_asset\n",
    "        self.rowid = time_period\n",
    "        self.action_space = np.array(list(range(11)))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.asset = self.initial_asset\n",
    "        self.cash = self.initial_asset\n",
    "        self.stock = 0\n",
    "        self.time = self.data.iloc[100]['date']\n",
    "        self.history=[]\n",
    "        self.total_cost = 0    \n",
    "        self.rowid = time_period\n",
    "        return self.data[:time_period][indicators].values\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        states = self.data.iloc[self.rowid]        \n",
    "        self.rowid +=1\n",
    "        if self.rowid == len(self.data)-1:\n",
    "            done = True\n",
    "        next_state = self.data.iloc[self.rowid]\n",
    "        last_asset = self.asset\n",
    "        price = next_state['open']\n",
    "        old_asset = self.cash + self.stock*price\n",
    "        self.asset = old_asset\n",
    "        target_value = action*0.1*self.asset\n",
    "        distance = target_value - self.stock*price\n",
    "        stock_distance = int(distance/(price*(1+self.cost)))\n",
    "        self.stock += stock_distance\n",
    "        self.cash = self.cash - distance - np.abs(stock_distance*self.cost*price)\n",
    "        self.asset = self.cash+self.stock*price\n",
    "        market_value = self.stock * next_state['close']\n",
    "        self.asset = market_value + self.cash\n",
    "        reward = (self.asset - last_asset)/last_asset\n",
    "        self.time = next_state['date']\n",
    "        # self.stock = stock\n",
    "        return (self.data[self.rowid-time_period:self.rowid][indicators].values, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make()\n",
    "env = Stock_Env(1000000, stock_df_train, 0.002)\n",
    "env_test = Stock_Env(1000000, stock_df_test, 0.002)\n",
    "num_episode = 5\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "\n",
    "for _ in range(num_episode):\n",
    "    \n",
    "    # initialize\n",
    "    env.reset()\n",
    "    t = 0\n",
    "    episodic_reward = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        \n",
    "        #env.render()\n",
    "        action = np.random.randint(11) # random action\n",
    "        _, reward, done = env.step(action)\n",
    "        episodic_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    reward_log.append(episodic_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(1, len(env.action_space), 64, 0.001, 0.001, 0.99, 51, -0.1, 0.1, 'cuda', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 20000\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    state = env.reset()\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        next_state = frame\n",
    "        state = next_state.copy()\n",
    "        episodic_reward += reward\n",
    "    return env.asset\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "        \n",
    "        val_asset = validation(env_test, agent)\n",
    "\n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}'.format(i, episodic_reward, average_log[-1], env.asset, val_asset), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Reward -0.032, Average Reward -0.057, Asset 955555.39, Validation Asset 943615.57\n",
      "Episode 200, Reward -0.068, Average Reward 0.044, Asset 921639.72, Validation Asset 947519.4215\n",
      "Episode 300, Reward 0.218, Average Reward 0.119, Asset 1217947.13, Validation Asset 921452.086\n",
      "Episode 400, Reward 0.248, Average Reward 0.276, Asset 1267439.60, Validation Asset 879273.174\n",
      "Episode 500, Reward 0.552, Average Reward 0.410, Asset 1717098.16, Validation Asset 907589.867\n",
      "Episode 509, Reward 0.310, Average Reward 0.416, Asset 1342777.08, Validation Asset 886795.748"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46880\\4124558685.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_frame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46880\\1407229748.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame, constant)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m             \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mstate_deque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_46880\\3779655246.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, eps)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_init = eps\n",
    "# constant = C\n",
    "# num_frame =1\n",
    "\n",
    "# rewards_log = []\n",
    "# average_log = []\n",
    "# eps = eps_init\n",
    "\n",
    "# for i in range(1, 1 + num_episode):\n",
    "#     episodic_reward = 0\n",
    "#     done = False\n",
    "#     frame = env.reset()\n",
    "#     state_deque = deque(maxlen=num_frame)\n",
    "#     for _ in range(num_frame):\n",
    "#         state_deque.append(frame)\n",
    "#     state = np.stack(state_deque, axis=0)\n",
    "#     state = np.expand_dims(state, axis=0)\n",
    "#     t = 0\n",
    "\n",
    "#     while not done and t < max_t:\n",
    "\n",
    "#         t += 1\n",
    "#         action = agent.act(state, eps)\n",
    "#         frame, reward, done = env.step(action)\n",
    "#         state_deque.append(frame)\n",
    "#         next_state = np.stack(state_deque, axis=0)\n",
    "#         next_state = np.expand_dims(next_state, axis=0)\n",
    "#         agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "#         if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "#             agent.learn()\n",
    "#             agent.soft_update(agent.tau)\n",
    "\n",
    "#         state = next_state.copy()\n",
    "#         episodic_reward += reward\n",
    "\n",
    "#     rewards_log.append(episodic_reward)\n",
    "#     average_log.append(np.mean(rewards_log[-100:]))\n",
    "#     print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}'.format(i, episodic_reward, average_log[-1]), end='')\n",
    "#     if i % 100 == 0:\n",
    "#         print()\n",
    "\n",
    "#     eps = max(eps * eps_decay, eps_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
