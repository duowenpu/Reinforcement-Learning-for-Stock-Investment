{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "time_period = 15\n",
    "class Q_Network(nn.Module):\n",
    "    '''\n",
    "    The input of this network should have shape (num_frame, 80, 80)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_frame, num_action):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_frame, out_channels=32, kernel_size=(2,1), stride=1, padding=2)  # 16, 20, 20\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,2), stride=1)  # 32, 9, 9\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2,1))\n",
    "        self.fc1 = nn.Linear(672, 256)\n",
    "        self.fc2 = nn.Linear(256, num_action)\n",
    "        self.sf = nn.Softmax()\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = F.relu(self.pool(self.conv1(image)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = F.relu(self.pool(self.conv3(x)))\n",
    "        x = x.view(-1, 672)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sf(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "file = open('../../FinBert/stock_data_full.bin', 'rb')\n",
    "data = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['AAPL','AMZN','C','GOOG','JPM','NFLX','PLTR']\n",
    "for i in range(len(codes)):\n",
    "    data[i]['symbol'] = codes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../FinRL/concat_data.csv')\n",
    "df=df[['date', 'open', 'high', 'low', 'close', 'volume',\n",
    "       'positive', 'neutral', 'negative', 'tic']]\n",
    "df['date'] = [x[:10] for x in df['date']]\n",
    "df = df[(df['date']>='2022-01-01') & (df['date']<'2023-09-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../min_data_adjust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = data[data['symbol']=='AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>vwap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:00:00+00:00</td>\n",
       "      <td>176.23</td>\n",
       "      <td>176.23</td>\n",
       "      <td>176.1800</td>\n",
       "      <td>176.1800</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>176.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:02:00+00:00</td>\n",
       "      <td>176.30</td>\n",
       "      <td>176.31</td>\n",
       "      <td>176.2800</td>\n",
       "      <td>176.2800</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>176.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:03:00+00:00</td>\n",
       "      <td>176.25</td>\n",
       "      <td>176.27</td>\n",
       "      <td>176.2500</td>\n",
       "      <td>176.2700</td>\n",
       "      <td>814.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>176.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:04:00+00:00</td>\n",
       "      <td>176.20</td>\n",
       "      <td>176.20</td>\n",
       "      <td>176.1200</td>\n",
       "      <td>176.1200</td>\n",
       "      <td>3744.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>176.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:05:00+00:00</td>\n",
       "      <td>176.17</td>\n",
       "      <td>176.17</td>\n",
       "      <td>176.1700</td>\n",
       "      <td>176.1700</td>\n",
       "      <td>464.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>176.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343433</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:53:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>209.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>171.305766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343434</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:54:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>810.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>171.305889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343435</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:57:00+00:00</td>\n",
       "      <td>171.32</td>\n",
       "      <td>171.32</td>\n",
       "      <td>171.3200</td>\n",
       "      <td>171.3200</td>\n",
       "      <td>439.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>171.330957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343436</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:58:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.2699</td>\n",
       "      <td>171.2699</td>\n",
       "      <td>532.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>171.282998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343437</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:59:00+00:00</td>\n",
       "      <td>171.23</td>\n",
       "      <td>171.27</td>\n",
       "      <td>171.2300</td>\n",
       "      <td>171.2300</td>\n",
       "      <td>3114.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>171.235370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343438 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       symbol                  timestamp    open    high       low     close  \\\n",
       "0        AAPL  2022-01-03 09:00:00+00:00  176.23  176.23  176.1800  176.1800   \n",
       "1        AAPL  2022-01-03 09:02:00+00:00  176.30  176.31  176.2800  176.2800   \n",
       "2        AAPL  2022-01-03 09:03:00+00:00  176.25  176.27  176.2500  176.2700   \n",
       "3        AAPL  2022-01-03 09:04:00+00:00  176.20  176.20  176.1200  176.1200   \n",
       "4        AAPL  2022-01-03 09:05:00+00:00  176.17  176.17  176.1700  176.1700   \n",
       "...       ...                        ...     ...     ...       ...       ...   \n",
       "343433   AAPL  2023-09-29 23:53:00+00:00  171.30  171.30  171.3000  171.3000   \n",
       "343434   AAPL  2023-09-29 23:54:00+00:00  171.30  171.30  171.3000  171.3000   \n",
       "343435   AAPL  2023-09-29 23:57:00+00:00  171.32  171.32  171.3200  171.3200   \n",
       "343436   AAPL  2023-09-29 23:58:00+00:00  171.30  171.30  171.2699  171.2699   \n",
       "343437   AAPL  2023-09-29 23:59:00+00:00  171.23  171.27  171.2300  171.2300   \n",
       "\n",
       "        volume  trade_count        vwap  \n",
       "0       1118.0         65.0  176.210000  \n",
       "1       1218.0         26.0  176.300000  \n",
       "2        814.0         30.0  176.260000  \n",
       "3       3744.0        114.0  176.180000  \n",
       "4        464.0         33.0  176.150000  \n",
       "...        ...          ...         ...  \n",
       "343433   209.0          8.0  171.305766  \n",
       "343434   810.0         16.0  171.305889  \n",
       "343435   439.0         20.0  171.330957  \n",
       "343436   532.0         11.0  171.282998  \n",
       "343437  3114.0         19.0  171.235370  \n",
       "\n",
       "[343438 rows x 9 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = df[df['tic']=='AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>tic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>177.830002</td>\n",
       "      <td>182.880005</td>\n",
       "      <td>177.710007</td>\n",
       "      <td>182.009995</td>\n",
       "      <td>104487900</td>\n",
       "      <td>-2.525743</td>\n",
       "      <td>3.722111</td>\n",
       "      <td>-3.922445</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>182.630005</td>\n",
       "      <td>182.940002</td>\n",
       "      <td>179.119995</td>\n",
       "      <td>179.699997</td>\n",
       "      <td>99310400</td>\n",
       "      <td>-2.752612</td>\n",
       "      <td>3.370780</td>\n",
       "      <td>-3.351379</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>179.610001</td>\n",
       "      <td>180.169998</td>\n",
       "      <td>174.639999</td>\n",
       "      <td>174.919998</td>\n",
       "      <td>94537600</td>\n",
       "      <td>-2.561095</td>\n",
       "      <td>3.561730</td>\n",
       "      <td>-3.588621</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>172.699997</td>\n",
       "      <td>175.300003</td>\n",
       "      <td>171.639999</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>96904000</td>\n",
       "      <td>-2.294448</td>\n",
       "      <td>3.207229</td>\n",
       "      <td>-3.612424</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>172.889999</td>\n",
       "      <td>174.139999</td>\n",
       "      <td>171.029999</td>\n",
       "      <td>172.169998</td>\n",
       "      <td>86709100</td>\n",
       "      <td>-2.325235</td>\n",
       "      <td>3.084295</td>\n",
       "      <td>-3.352122</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>2023-09-25</td>\n",
       "      <td>174.199997</td>\n",
       "      <td>176.970001</td>\n",
       "      <td>174.149994</td>\n",
       "      <td>176.080002</td>\n",
       "      <td>46172700</td>\n",
       "      <td>-2.361765</td>\n",
       "      <td>3.181928</td>\n",
       "      <td>-3.037302</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>2023-09-26</td>\n",
       "      <td>174.820007</td>\n",
       "      <td>175.199997</td>\n",
       "      <td>171.660004</td>\n",
       "      <td>171.960007</td>\n",
       "      <td>64588900</td>\n",
       "      <td>-1.893191</td>\n",
       "      <td>2.688069</td>\n",
       "      <td>-3.369864</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>2023-09-27</td>\n",
       "      <td>172.619995</td>\n",
       "      <td>173.039993</td>\n",
       "      <td>169.050003</td>\n",
       "      <td>170.429993</td>\n",
       "      <td>66921800</td>\n",
       "      <td>-3.139558</td>\n",
       "      <td>3.359877</td>\n",
       "      <td>-2.654129</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>2023-09-28</td>\n",
       "      <td>169.339996</td>\n",
       "      <td>172.029999</td>\n",
       "      <td>167.619995</td>\n",
       "      <td>170.690002</td>\n",
       "      <td>56294400</td>\n",
       "      <td>-2.045589</td>\n",
       "      <td>2.791628</td>\n",
       "      <td>-3.063628</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>172.020004</td>\n",
       "      <td>173.070007</td>\n",
       "      <td>170.339996</td>\n",
       "      <td>171.210007</td>\n",
       "      <td>51814200</td>\n",
       "      <td>-2.751975</td>\n",
       "      <td>3.445238</td>\n",
       "      <td>-3.111207</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date        open        high         low       close     volume  \\\n",
       "17    2022-01-03  177.830002  182.880005  177.710007  182.009995  104487900   \n",
       "21    2022-01-04  182.630005  182.940002  179.119995  179.699997   99310400   \n",
       "26    2022-01-05  179.610001  180.169998  174.639999  174.919998   94537600   \n",
       "35    2022-01-06  172.699997  175.300003  171.639999  172.000000   96904000   \n",
       "42    2022-01-07  172.889999  174.139999  171.029999  172.169998   86709100   \n",
       "...          ...         ...         ...         ...         ...        ...   \n",
       "3028  2023-09-25  174.199997  176.970001  174.149994  176.080002   46172700   \n",
       "3034  2023-09-26  174.820007  175.199997  171.660004  171.960007   64588900   \n",
       "3045  2023-09-27  172.619995  173.039993  169.050003  170.429993   66921800   \n",
       "3049  2023-09-28  169.339996  172.029999  167.619995  170.690002   56294400   \n",
       "3057  2023-09-29  172.020004  173.070007  170.339996  171.210007   51814200   \n",
       "\n",
       "      positive   neutral  negative   tic  \n",
       "17   -2.525743  3.722111 -3.922445  AAPL  \n",
       "21   -2.752612  3.370780 -3.351379  AAPL  \n",
       "26   -2.561095  3.561730 -3.588621  AAPL  \n",
       "35   -2.294448  3.207229 -3.612424  AAPL  \n",
       "42   -2.325235  3.084295 -3.352122  AAPL  \n",
       "...        ...       ...       ...   ...  \n",
       "3028 -2.361765  3.181928 -3.037302  AAPL  \n",
       "3034 -1.893191  2.688069 -3.369864  AAPL  \n",
       "3045 -3.139558  3.359877 -2.654129  AAPL  \n",
       "3049 -2.045589  2.791628 -3.063628  AAPL  \n",
       "3057 -2.751975  3.445238 -3.111207  AAPL  \n",
       "\n",
       "[438 rows x 10 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finta import TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df['SMA42'] = TA.SMA(stock_df, 42)\n",
    "stock_df['SMA5'] = TA.SMA(stock_df, 5)\n",
    "stock_df['SMA15'] = TA.SMA(stock_df, 15)\n",
    "stock_df['AO'] = TA.AO(stock_df)\n",
    "stock_df['OVB'] = TA.OBV(stock_df)\n",
    "stock_df[['VW_MACD','MACD_SIGNAL']] = TA.VW_MACD(stock_df)\n",
    "stock_df['RSI'] = TA.RSI(stock_df)\n",
    "stock_df['CMO'] = TA.CMO(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'open', 'high', 'low', 'close', 'volume', 'positive', 'neutral',\n",
       "       'negative', 'tic', 'SMA42', 'SMA5', 'SMA15', 'AO', 'OVB', 'VW_MACD',\n",
       "       'MACD_SIGNAL', 'RSI', 'CMO'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([0], maxlen=5)\n",
      "deque([0, 1], maxlen=5)\n",
      "deque([0, 1, 2], maxlen=5)\n",
      "deque([0, 1, 2, 3], maxlen=5)\n",
      "deque([0, 1, 2, 3, 4], maxlen=5)\n",
      "deque([1, 2, 3, 4, 5], maxlen=5)\n",
      "deque([2, 3, 4, 5, 6], maxlen=5)\n",
      "deque([3, 4, 5, 6, 7], maxlen=5)\n",
      "deque([4, 5, 6, 7, 8], maxlen=5)\n",
      "deque([5, 6, 7, 8, 9], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "test = deque(maxlen=5)\n",
    "for i in range(10):\n",
    "    test.append(i)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = None\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# from networks import *\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, device, visual=False):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size).to(device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size).to(device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.tst = None\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "        self.tst = states\n",
    "        Q_values = self.Q_local(states)\n",
    "        Q_values = torch.gather(input=Q_values, dim=-1, index=actions)\n",
    "        with torch.no_grad():\n",
    "            Q_targets = self.Q_target(next_states)\n",
    "            Q_targets, _ = torch.max(input=Q_targets, dim=-1, keepdim=True)\n",
    "            Q_targets = rewards + self.gamma * (1 - dones) * Q_targets\n",
    "\n",
    "        loss = (Q_values - Q_targets).pow(2).mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = ['open', 'high', 'low', 'close', 'volume', 'positive', 'neutral', 'negative','SMA42', 'SMA5', 'SMA15', 'AO', 'OVB','VW_MACD',\n",
    "       'MACD_SIGNAL', 'RSI', 'CMO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stock_Env:\n",
    "    def __init__(self, initial_asset, data, cost):\n",
    "        self.asset = initial_asset\n",
    "        self.cash = initial_asset\n",
    "        self.stock = 0\n",
    "        self.data = data\n",
    "        self.time = data.iloc[time_period]['date']\n",
    "        self.cost = cost\n",
    "        self.history=[]\n",
    "        self.total_cost = 0\n",
    "        self.initial_asset = initial_asset\n",
    "        self.rowid = time_period\n",
    "        self.action_space = np.array(list(range(11)))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.asset = self.initial_asset\n",
    "        self.cash = self.initial_asset\n",
    "        self.stock = 0\n",
    "        self.time = self.data.iloc[100]['date']\n",
    "        self.history=[]\n",
    "        self.total_cost = 0    \n",
    "        self.rowid = time_period\n",
    "        return self.data[:time_period][indicators].values\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        states = self.data.iloc[self.rowid]        \n",
    "        self.rowid +=1\n",
    "        if self.rowid == len(self.data)-1:\n",
    "            done = True\n",
    "        next_state = self.data.iloc[self.rowid]\n",
    "        last_asset = self.asset\n",
    "        price = next_state['open']\n",
    "        old_asset = self.cash + self.stock*price\n",
    "        self.asset = old_asset\n",
    "        target_value = action*0.1*self.asset\n",
    "        distance = target_value - self.stock*price\n",
    "        stock_distance = int(distance/(price*(1+self.cost)))\n",
    "        self.stock += stock_distance\n",
    "        self.cash = self.cash - distance - np.abs(stock_distance*self.cost*price)\n",
    "        self.asset = self.cash+self.stock*price\n",
    "        market_value = self.stock * next_state['close']\n",
    "        self.asset = market_value + self.cash\n",
    "        reward = self.asset - last_asset\n",
    "        self.time = next_state['date']\n",
    "        # self.stock = stock\n",
    "        return (self.data[self.rowid-time_period:self.rowid][indicators].values, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make()\n",
    "env = Stock_Env(1000000, stock_df, 0.002)\n",
    "num_episode = 5\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "\n",
    "for _ in range(num_episode):\n",
    "    \n",
    "    # initialize\n",
    "    env.reset()\n",
    "    t = 0\n",
    "    episodic_reward = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        \n",
    "        #env.render()\n",
    "        action = np.random.randint(11) # random action\n",
    "        _, reward, done = env.step(action)\n",
    "        episodic_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    reward_log.append(episodic_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(1, len(env.action_space), 64, 0.001, 0.001, 0.99, 'cuda',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 20000\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "\n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}'.format(i, episodic_reward, average_log[-1]), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Reward -158293.017, Average Reward -214143.315\n",
      "Episode 200, Reward -219081.606, Average Reward -194950.177\n",
      "Episode 300, Reward -171535.242, Average Reward -153084.563\n",
      "Episode 400, Reward -7701.033, Average Reward -122730.15938\n",
      "Episode 500, Reward -63621.570, Average Reward -100781.8433\n",
      "Episode 600, Reward -43233.783, Average Reward -72651.46751\n",
      "Episode 700, Reward -31046.109, Average Reward -57316.6849\n",
      "Episode 800, Reward 56621.490, Average Reward -42087.85283\n",
      "Episode 900, Reward -79004.737, Average Reward -31984.3725\n",
      "Episode 1000, Reward -22175.508, Average Reward -21836.425\n",
      "Episode 1100, Reward 20945.377, Average Reward -17013.1324\n",
      "Episode 1200, Reward -20187.514, Average Reward -12304.824\n",
      "Episode 1300, Reward -5267.051, Average Reward -11603.9994\n",
      "Episode 1400, Reward -6705.901, Average Reward -6337.27919\n",
      "Episode 1500, Reward 1859.653, Average Reward -4635.49832\n",
      "Episode 1600, Reward 3272.370, Average Reward -2534.43855\n",
      "Episode 1700, Reward 22179.980, Average Reward -1342.1906\n",
      "Episode 1800, Reward -7699.587, Average Reward -2138.8187\n",
      "Episode 1900, Reward -1566.569, Average Reward -2835.6258\n",
      "Episode 2000, Reward 11392.884, Average Reward -2960.8632\n",
      "Episode 2100, Reward 14099.681, Average Reward -1684.9920\n",
      "Episode 2200, Reward 5979.450, Average Reward -691.387363\n",
      "Episode 2300, Reward 11838.011, Average Reward -2262.8251\n",
      "Episode 2400, Reward 8686.000, Average Reward -4049.29965\n",
      "Episode 2500, Reward -16812.876, Average Reward -1288.798\n",
      "Episode 2600, Reward -1566.569, Average Reward -82.129251\n",
      "Episode 2700, Reward -57403.126, Average Reward 1157.448\n",
      "Episode 2800, Reward 3272.370, Average Reward -2369.43218\n",
      "Episode 2900, Reward 28758.126, Average Reward -2531.1075\n",
      "Episode 3000, Reward -127.719, Average Reward 556.5056439\n",
      "Episode 3100, Reward -1566.569, Average Reward -2146.1601\n",
      "Episode 3200, Reward 12386.570, Average Reward -702.10906\n",
      "Episode 3300, Reward -13386.618, Average Reward -155.5497\n",
      "Episode 3400, Reward -22174.923, Average Reward -617.476\n",
      "Episode 3500, Reward 2107.906, Average Reward -770.082152\n",
      "Episode 3600, Reward -4992.790, Average Reward -3732.1514\n",
      "Episode 3700, Reward -2560.255, Average Reward -2930.9116\n",
      "Episode 3800, Reward 8960.334, Average Reward -1808.24021\n",
      "Episode 3900, Reward -7254.423, Average Reward -2609.6376\n",
      "Episode 4000, Reward -25601.108, Average Reward -38.29826\n",
      "Episode 4100, Reward 10399.109, Average Reward 810.883002\n",
      "Episode 4200, Reward -1566.569, Average Reward -806.5898\n",
      "Episode 4300, Reward 8960.261, Average Reward -4237.71987\n",
      "Episode 4400, Reward 14099.681, Average Reward -1108.3752\n",
      "Episode 4500, Reward -13831.782, Average Reward -2852.329\n",
      "Episode 4600, Reward 9679.774, Average Reward -3077.41530\n",
      "Episode 4647, Reward -6980.162, Average Reward -1075.3504"
     ]
    }
   ],
   "source": [
    "train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_init = eps\n",
    "# constant = C\n",
    "# num_frame =1\n",
    "\n",
    "# rewards_log = []\n",
    "# average_log = []\n",
    "# eps = eps_init\n",
    "\n",
    "# for i in range(1, 1 + num_episode):\n",
    "#     episodic_reward = 0\n",
    "#     done = False\n",
    "#     frame = env.reset()\n",
    "#     state_deque = deque(maxlen=num_frame)\n",
    "#     for _ in range(num_frame):\n",
    "#         state_deque.append(frame)\n",
    "#     state = np.stack(state_deque, axis=0)\n",
    "#     state = np.expand_dims(state, axis=0)\n",
    "#     t = 0\n",
    "\n",
    "#     while not done and t < max_t:\n",
    "\n",
    "#         t += 1\n",
    "#         action = agent.act(state, eps)\n",
    "#         frame, reward, done = env.step(action)\n",
    "#         state_deque.append(frame)\n",
    "#         next_state = np.stack(state_deque, axis=0)\n",
    "#         next_state = np.expand_dims(next_state, axis=0)\n",
    "#         agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "#         if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "#             agent.learn()\n",
    "#             agent.soft_update(agent.tau)\n",
    "\n",
    "#         state = next_state.copy()\n",
    "#         episodic_reward += reward\n",
    "\n",
    "#     rewards_log.append(episodic_reward)\n",
    "#     average_log.append(np.mean(rewards_log[-100:]))\n",
    "#     print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}'.format(i, episodic_reward, average_log[-1]), end='')\n",
    "#     if i % 100 == 0:\n",
    "#         print()\n",
    "\n",
    "#     eps = max(eps * eps_decay, eps_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
