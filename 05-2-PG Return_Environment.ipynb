{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "time_period = 15\n",
    "class Actor_Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, hidden=[128, 16]):\n",
    "        super(Actor_Critic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=state_size, out_channels=32, kernel_size=(2,1), stride=1, padding=2)  # 16, 20, 20\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,2), stride=1)  # 32, 9, 9\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2,1))\n",
    "        self.fc1 = nn.Linear(576, 256)\n",
    "        self.fc2 = nn.Linear(256, action_size)\n",
    "        self.fc3 = nn.Linear(256,1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.pool(self.conv1(state)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = F.relu(self.pool(self.conv3(x)))\n",
    "        x = x.view(-1, 576)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        log_probs = F.log_softmax(self.fc2(x), dim=1)\n",
    "        values = self.fc3(x)\n",
    "        return log_probs, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\lab\\\\CP\\\\Reinforcement\\\\Basic Actor-Critic'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "file = open('../../FinBert/stock_data_full.bin', 'rb')\n",
    "data = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['AAPL','AMZN','C','GOOG','JPM','NFLX','PLTR']\n",
    "for i in range(len(codes)):\n",
    "    data[i]['symbol'] = codes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../FinRL/concat_data.csv')\n",
    "df=df[['date', 'open', 'high', 'low', 'close', 'volume',\n",
    "       'positive', 'neutral', 'negative', 'tic']]\n",
    "df['date'] = [x[:10] for x in df['date']]\n",
    "df = df[(df['date']>='2022-01-01') & (df['date']<'2023-09-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../min_data_adjust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = data[data['symbol']=='AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>vwap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:00:00+00:00</td>\n",
       "      <td>176.23</td>\n",
       "      <td>176.23</td>\n",
       "      <td>176.1800</td>\n",
       "      <td>176.1800</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>176.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:02:00+00:00</td>\n",
       "      <td>176.30</td>\n",
       "      <td>176.31</td>\n",
       "      <td>176.2800</td>\n",
       "      <td>176.2800</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>176.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:03:00+00:00</td>\n",
       "      <td>176.25</td>\n",
       "      <td>176.27</td>\n",
       "      <td>176.2500</td>\n",
       "      <td>176.2700</td>\n",
       "      <td>814.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>176.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:04:00+00:00</td>\n",
       "      <td>176.20</td>\n",
       "      <td>176.20</td>\n",
       "      <td>176.1200</td>\n",
       "      <td>176.1200</td>\n",
       "      <td>3744.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>176.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:05:00+00:00</td>\n",
       "      <td>176.17</td>\n",
       "      <td>176.17</td>\n",
       "      <td>176.1700</td>\n",
       "      <td>176.1700</td>\n",
       "      <td>464.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>176.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343433</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:53:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>209.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>171.305766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343434</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:54:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>810.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>171.305889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343435</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:57:00+00:00</td>\n",
       "      <td>171.32</td>\n",
       "      <td>171.32</td>\n",
       "      <td>171.3200</td>\n",
       "      <td>171.3200</td>\n",
       "      <td>439.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>171.330957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343436</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:58:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.2699</td>\n",
       "      <td>171.2699</td>\n",
       "      <td>532.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>171.282998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343437</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:59:00+00:00</td>\n",
       "      <td>171.23</td>\n",
       "      <td>171.27</td>\n",
       "      <td>171.2300</td>\n",
       "      <td>171.2300</td>\n",
       "      <td>3114.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>171.235370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343438 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       symbol                  timestamp    open    high       low     close  \\\n",
       "0        AAPL  2022-01-03 09:00:00+00:00  176.23  176.23  176.1800  176.1800   \n",
       "1        AAPL  2022-01-03 09:02:00+00:00  176.30  176.31  176.2800  176.2800   \n",
       "2        AAPL  2022-01-03 09:03:00+00:00  176.25  176.27  176.2500  176.2700   \n",
       "3        AAPL  2022-01-03 09:04:00+00:00  176.20  176.20  176.1200  176.1200   \n",
       "4        AAPL  2022-01-03 09:05:00+00:00  176.17  176.17  176.1700  176.1700   \n",
       "...       ...                        ...     ...     ...       ...       ...   \n",
       "343433   AAPL  2023-09-29 23:53:00+00:00  171.30  171.30  171.3000  171.3000   \n",
       "343434   AAPL  2023-09-29 23:54:00+00:00  171.30  171.30  171.3000  171.3000   \n",
       "343435   AAPL  2023-09-29 23:57:00+00:00  171.32  171.32  171.3200  171.3200   \n",
       "343436   AAPL  2023-09-29 23:58:00+00:00  171.30  171.30  171.2699  171.2699   \n",
       "343437   AAPL  2023-09-29 23:59:00+00:00  171.23  171.27  171.2300  171.2300   \n",
       "\n",
       "        volume  trade_count        vwap  \n",
       "0       1118.0         65.0  176.210000  \n",
       "1       1218.0         26.0  176.300000  \n",
       "2        814.0         30.0  176.260000  \n",
       "3       3744.0        114.0  176.180000  \n",
       "4        464.0         33.0  176.150000  \n",
       "...        ...          ...         ...  \n",
       "343433   209.0          8.0  171.305766  \n",
       "343434   810.0         16.0  171.305889  \n",
       "343435   439.0         20.0  171.330957  \n",
       "343436   532.0         11.0  171.282998  \n",
       "343437  3114.0         19.0  171.235370  \n",
       "\n",
       "[343438 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = df[df['tic']=='AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>tic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>177.830002</td>\n",
       "      <td>182.880005</td>\n",
       "      <td>177.710007</td>\n",
       "      <td>182.009995</td>\n",
       "      <td>104487900</td>\n",
       "      <td>-2.525743</td>\n",
       "      <td>3.722111</td>\n",
       "      <td>-3.922445</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>182.630005</td>\n",
       "      <td>182.940002</td>\n",
       "      <td>179.119995</td>\n",
       "      <td>179.699997</td>\n",
       "      <td>99310400</td>\n",
       "      <td>-2.752612</td>\n",
       "      <td>3.370780</td>\n",
       "      <td>-3.351379</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>179.610001</td>\n",
       "      <td>180.169998</td>\n",
       "      <td>174.639999</td>\n",
       "      <td>174.919998</td>\n",
       "      <td>94537600</td>\n",
       "      <td>-2.561095</td>\n",
       "      <td>3.561730</td>\n",
       "      <td>-3.588621</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>172.699997</td>\n",
       "      <td>175.300003</td>\n",
       "      <td>171.639999</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>96904000</td>\n",
       "      <td>-2.294448</td>\n",
       "      <td>3.207229</td>\n",
       "      <td>-3.612424</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>172.889999</td>\n",
       "      <td>174.139999</td>\n",
       "      <td>171.029999</td>\n",
       "      <td>172.169998</td>\n",
       "      <td>86709100</td>\n",
       "      <td>-2.325235</td>\n",
       "      <td>3.084295</td>\n",
       "      <td>-3.352122</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>2023-09-25</td>\n",
       "      <td>174.199997</td>\n",
       "      <td>176.970001</td>\n",
       "      <td>174.149994</td>\n",
       "      <td>176.080002</td>\n",
       "      <td>46172700</td>\n",
       "      <td>-2.361765</td>\n",
       "      <td>3.181928</td>\n",
       "      <td>-3.037302</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>2023-09-26</td>\n",
       "      <td>174.820007</td>\n",
       "      <td>175.199997</td>\n",
       "      <td>171.660004</td>\n",
       "      <td>171.960007</td>\n",
       "      <td>64588900</td>\n",
       "      <td>-1.893191</td>\n",
       "      <td>2.688069</td>\n",
       "      <td>-3.369864</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>2023-09-27</td>\n",
       "      <td>172.619995</td>\n",
       "      <td>173.039993</td>\n",
       "      <td>169.050003</td>\n",
       "      <td>170.429993</td>\n",
       "      <td>66921800</td>\n",
       "      <td>-3.139558</td>\n",
       "      <td>3.359877</td>\n",
       "      <td>-2.654129</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>2023-09-28</td>\n",
       "      <td>169.339996</td>\n",
       "      <td>172.029999</td>\n",
       "      <td>167.619995</td>\n",
       "      <td>170.690002</td>\n",
       "      <td>56294400</td>\n",
       "      <td>-2.045589</td>\n",
       "      <td>2.791628</td>\n",
       "      <td>-3.063628</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>172.020004</td>\n",
       "      <td>173.070007</td>\n",
       "      <td>170.339996</td>\n",
       "      <td>171.210007</td>\n",
       "      <td>51814200</td>\n",
       "      <td>-2.751975</td>\n",
       "      <td>3.445238</td>\n",
       "      <td>-3.111207</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date        open        high         low       close     volume  \\\n",
       "17    2022-01-03  177.830002  182.880005  177.710007  182.009995  104487900   \n",
       "21    2022-01-04  182.630005  182.940002  179.119995  179.699997   99310400   \n",
       "26    2022-01-05  179.610001  180.169998  174.639999  174.919998   94537600   \n",
       "35    2022-01-06  172.699997  175.300003  171.639999  172.000000   96904000   \n",
       "42    2022-01-07  172.889999  174.139999  171.029999  172.169998   86709100   \n",
       "...          ...         ...         ...         ...         ...        ...   \n",
       "3028  2023-09-25  174.199997  176.970001  174.149994  176.080002   46172700   \n",
       "3034  2023-09-26  174.820007  175.199997  171.660004  171.960007   64588900   \n",
       "3045  2023-09-27  172.619995  173.039993  169.050003  170.429993   66921800   \n",
       "3049  2023-09-28  169.339996  172.029999  167.619995  170.690002   56294400   \n",
       "3057  2023-09-29  172.020004  173.070007  170.339996  171.210007   51814200   \n",
       "\n",
       "      positive   neutral  negative   tic  \n",
       "17   -2.525743  3.722111 -3.922445  AAPL  \n",
       "21   -2.752612  3.370780 -3.351379  AAPL  \n",
       "26   -2.561095  3.561730 -3.588621  AAPL  \n",
       "35   -2.294448  3.207229 -3.612424  AAPL  \n",
       "42   -2.325235  3.084295 -3.352122  AAPL  \n",
       "...        ...       ...       ...   ...  \n",
       "3028 -2.361765  3.181928 -3.037302  AAPL  \n",
       "3034 -1.893191  2.688069 -3.369864  AAPL  \n",
       "3045 -3.139558  3.359877 -2.654129  AAPL  \n",
       "3049 -2.045589  2.791628 -3.063628  AAPL  \n",
       "3057 -2.751975  3.445238 -3.111207  AAPL  \n",
       "\n",
       "[438 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_30632\\1920598173.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['pctchange'] = (stock_df['close'] - stock_df['open'])/stock_df['open']\n"
     ]
    }
   ],
   "source": [
    "stock_df['pctchange'] = (stock_df['close'] - stock_df['open'])/stock_df['open']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finta import TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_30632\\1998282740.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['SMA42'] = TA.SMA(stock_df, 42)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_30632\\1998282740.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['SMA5'] = TA.SMA(stock_df, 5)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_30632\\1998282740.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['SMA15'] = TA.SMA(stock_df, 15)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_30632\\1998282740.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['AO'] = TA.AO(stock_df)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_30632\\1998282740.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['OVB'] = TA.OBV(stock_df)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_30632\\1998282740.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df[['VW_MACD','MACD_SIGNAL']] = TA.VW_MACD(stock_df)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_30632\\1998282740.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df[['VW_MACD','MACD_SIGNAL']] = TA.VW_MACD(stock_df)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_30632\\1998282740.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['RSI'] = TA.RSI(stock_df)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_30632\\1998282740.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['CMO'] = TA.CMO(stock_df)\n"
     ]
    }
   ],
   "source": [
    "stock_df['SMA42'] = TA.SMA(stock_df, 42)\n",
    "stock_df['SMA5'] = TA.SMA(stock_df, 5)\n",
    "stock_df['SMA15'] = TA.SMA(stock_df, 15)\n",
    "stock_df['AO'] = TA.AO(stock_df)\n",
    "stock_df['OVB'] = TA.OBV(stock_df)\n",
    "stock_df[['VW_MACD','MACD_SIGNAL']] = TA.VW_MACD(stock_df)\n",
    "stock_df['RSI'] = TA.RSI(stock_df)\n",
    "stock_df['CMO'] = TA.CMO(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'open', 'high', 'low', 'close', 'volume', 'positive', 'neutral',\n",
       "       'negative', 'tic', 'pctchange', 'SMA42', 'SMA5', 'SMA15', 'AO', 'OVB',\n",
       "       'VW_MACD', 'MACD_SIGNAL', 'RSI', 'CMO'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df_train = stock_df[stock_df['date']<='2023-03-31']\n",
    "stock_df_test = stock_df[stock_df['date']>'2023-03-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "class Agent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size, lr, gamma, device, mode='MC', use_critic=False, normalize=False):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.mode = mode\n",
    "        self.use_critic = use_critic\n",
    "        self.normalize = normalize\n",
    "        \n",
    "\n",
    "        self.Actor_Critic = Actor_Critic(self.state_size, self.action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.Actor_Critic.parameters(), lr)\n",
    "            \n",
    "    def act(self, states):\n",
    "        with torch.no_grad():\n",
    "            states = torch.tensor(states.astype(np.float32)).to(self.device)\n",
    "\n",
    "            log_probs, _ = self.Actor_Critic(states)\n",
    "            probs = log_probs.exp().view(-1).cpu().numpy()\n",
    "            action = np.random.choice(a=self.action_size, size=1, replace=False, p=probs)[0]\n",
    "        return action\n",
    "    \n",
    "    def process_data(self, states, actions, rewards, dones, batch_size):\n",
    "        \n",
    "        states = torch.tensor(states, dtype=torch.float).to(self.device)\n",
    "        states = states.reshape([states.shape[0]]+list(states.shape[2:]))\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device).view(-1, 1)\n",
    "        dones = torch.tensor(dones, dtype=torch.float).to(self.device).view(-1,1)\n",
    "        #calculate log probabilities and state values\n",
    "        N = states.size(0) # N-1 is the length of actions, rewards and dones\n",
    "        log_probs = torch.zeros((N, self.action_size)).to(self.device)\n",
    "        state_values = torch.zeros((N, 1)).to(self.device)\n",
    "        step = math.ceil(N/batch_size)\n",
    "        \n",
    "        for ind in range(step):\n",
    "            output1, output2 = self.Actor_Critic(states[ind*batch_size:(ind+1)*batch_size, :])\n",
    "\n",
    "            log_probs[ind*batch_size:(ind+1)*batch_size, :] = output1\n",
    "            state_values[ind*batch_size:(ind+1)*batch_size, :] = output2 \n",
    "        \n",
    "        log_probs = log_probs[:-1, :]# remove the last one, which corresponds to no actions\n",
    "        log_probs = torch.gather(log_probs, dim=1, index=actions)\n",
    "        \n",
    "        #calculate discounted rewards, gamma^t r_t\n",
    "        L = len(rewards)\n",
    "        rewards = np.array(rewards) #r_t\n",
    "        discounts = self.gamma ** np.arange(L)\n",
    "        discounted_rewards = rewards * discounts # this is gamma^t r_t\n",
    "        \n",
    "        return state_values, log_probs, rewards, discounted_rewards, dones\n",
    "    \n",
    "    def learn(self, state_values, log_probs, rewards, discounted_rewards, dones):\n",
    "\n",
    "        # Update Critic use MSE\n",
    "        # Update Actor by maximizing A_t * log(a_t|s_t)\n",
    "\n",
    "        L = len(discounted_rewards)\n",
    "        with torch.no_grad():\n",
    "            G = []\n",
    "            return_value = 0\n",
    "            if self.mode == 'MC':\n",
    "                for i in range(L-1, -1, -1):\n",
    "                    return_value = rewards[i] + self.gamma * (1-dones[i].cpu().detach().numpy()) * return_value\n",
    "                    G.append(return_value)\n",
    "                G = G[::-1]\n",
    "                G = torch.tensor(G, dtype=torch.float).view(-1, 1).to(self.device)\n",
    "            else:\n",
    "                rewards = torch.tensor(rewards, dtype=torch.float).view(-1, 1).to(self.device)\n",
    "                G = rewards + self.gamma * (1-dones) * state_values[1:, :]\n",
    "            \n",
    "        Critic_Loss = 0.5*(state_values[:-1, :] - G).pow(2).mean()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.use_critic:\n",
    "                G = G - state_values[:-1, :] # advantage\n",
    "            if self.normalize:\n",
    "                G = (G - G.mean()) / (G.std() + 0.00001) # normalized advantage\n",
    "                \n",
    "        Actor_Loss = -log_probs * G\n",
    "        Actor_Loss = Actor_Loss.mean()\n",
    "        \n",
    "\n",
    "        Loss = Actor_Loss + Critic_Loss\n",
    "        self.optimizer.zero_grad()\n",
    "        Loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicators = ['open', 'high', 'low', 'close', 'volume', 'positive', 'neutral', 'negative','SMA42', 'SMA5', 'SMA15', 'AO', 'OVB','VW_MACD',\n",
    "#        'MACD_SIGNAL', 'RSI', 'CMO']\n",
    "\n",
    "indicators = ['pctchange', 'volume', 'positive', 'neutral', 'negative','SMA42', 'SMA5', 'SMA15', 'AO', 'OVB','VW_MACD',\n",
    "       'MACD_SIGNAL', 'RSI', 'CMO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stock_Env:\n",
    "    def __init__(self, initial_asset, data, cost):\n",
    "        self.asset = initial_asset\n",
    "        self.cash = initial_asset\n",
    "        self.stock = 0\n",
    "        self.data = data\n",
    "        self.time = data.iloc[time_period]['date']\n",
    "        self.cost = cost\n",
    "        self.history=[]\n",
    "        self.total_cost = 0\n",
    "        self.initial_asset = initial_asset\n",
    "        self.rowid = time_period\n",
    "        self.action_space = np.array(list(range(11)))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.asset = self.initial_asset\n",
    "        self.cash = self.initial_asset\n",
    "        self.stock = 0\n",
    "        self.time = self.data.iloc[100]['date']\n",
    "        self.history=[]\n",
    "        self.total_cost = 0    \n",
    "        temp_time = np.random.randint(15,100)\n",
    "        self.rowid = temp_time\n",
    "        return self.data[temp_time-time_period:temp_time][indicators].values\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        states = self.data.iloc[self.rowid]        \n",
    "        self.rowid +=1\n",
    "        if self.rowid == len(self.data)-1:\n",
    "            done = True\n",
    "        next_state = self.data.iloc[self.rowid]\n",
    "        last_asset = self.asset\n",
    "        price = next_state['open']\n",
    "        old_asset = self.cash + self.stock*price\n",
    "        self.asset = old_asset\n",
    "        target_value = action*0.1*self.asset\n",
    "        distance = target_value - self.stock*price\n",
    "        stock_distance = int(distance/(price*(1+self.cost)))\n",
    "        self.stock += stock_distance\n",
    "        self.cash = self.cash - distance - np.abs(stock_distance*self.cost*price)\n",
    "        self.asset = self.cash+self.stock*price\n",
    "        market_value = self.stock * next_state['close']\n",
    "        self.asset = market_value + self.cash\n",
    "        reward = (self.asset - last_asset)/last_asset\n",
    "        self.time = next_state['date']\n",
    "        # self.stock = stock\n",
    "        return (self.data[self.rowid-time_period:self.rowid][indicators].values, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make()\n",
    "env = Stock_Env(1000000, stock_df_train, 0.002)\n",
    "env_test = Stock_Env(1000000, stock_df_test, 0.002)\n",
    "num_episode = 5\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "\n",
    "for _ in range(num_episode):\n",
    "    \n",
    "    # initialize\n",
    "    env.reset()\n",
    "    t = 0\n",
    "    episodic_reward = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        \n",
    "        #env.render()\n",
    "        action = np.random.randint(11) # random action\n",
    "        _, reward, done = env.step(action)\n",
    "        episodic_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    reward_log.append(episodic_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(1, len(env.action_space), 0.0005, 0.99, 'cuda', 'TD', True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 20000\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.997\n",
    "eps_min = 0.01\n",
    "num_frame = 1\n",
    "C = 4 # update weights every C steps\n",
    "state_deque = deque(maxlen=num_frame)\n",
    "\n",
    "def validation(env, agent):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    state = env.reset()\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state.reshape([1]+list(state.shape)))\n",
    "        frame, reward, done = env.step(action)\n",
    "        next_state = frame\n",
    "        state = next_state.copy()\n",
    "        episodic_reward += reward\n",
    "    return env.asset\n",
    "\n",
    "def train(agent, env, n_episode, max_t, scale=1):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        # state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        \n",
    "        done = False\n",
    "        t = 0\n",
    "        state_history = [list(state)]\n",
    "        action_history = []\n",
    "        done_history = []\n",
    "        reward_history = []\n",
    "        episodic_reward = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            \n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "           \n",
    "            episodic_reward += reward\n",
    "            action_history.append(action)\n",
    "            done_history.append(done)\n",
    "            reward_history.append(reward * scale)\n",
    "            state = next_state.copy()\n",
    "            state_history.append(state)\n",
    "            t += 1\n",
    "\n",
    "        state_values, log_probs, rewards, discounted_rewards, dones = agent.process_data(state_history, action_history, reward_history, done_history, 64)\n",
    "        agent.learn(state_values, log_probs, rewards, discounted_rewards, dones)\n",
    "        \n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        \n",
    "        val_asset = validation(env_test, agent)\n",
    "\n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}'.format(i, episodic_reward, average_log[-1], env.asset, val_asset), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "            \n",
    "    return rewards, average_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Reward 0.033, Average Reward 0.025, Asset 1032096.49, Validation Asset 918107.65\n",
      "Episode 200, Reward 0.040, Average Reward 0.020, Asset 1038519.14, Validation Asset 1004900.58\n",
      "Episode 300, Reward 0.035, Average Reward 0.024, Asset 1033699.81, Validation Asset 895081.850\n",
      "Episode 400, Reward 0.022, Average Reward 0.023, Asset 1020592.28, Validation Asset 881570.747\n",
      "Episode 500, Reward 0.043, Average Reward 0.024, Asset 1041622.11, Validation Asset 1029258.18\n",
      "Episode 600, Reward 0.025, Average Reward 0.022, Asset 1023800.26, Validation Asset 990699.000\n",
      "Episode 700, Reward 0.001, Average Reward 0.022, Asset 998219.82, Validation Asset 993288.3942\n",
      "Episode 800, Reward 0.037, Average Reward 0.021, Asset 1036133.66, Validation Asset 1008594.27\n",
      "Episode 900, Reward 0.036, Average Reward 0.026, Asset 1034510.83, Validation Asset 978153.640\n",
      "Episode 1000, Reward 0.043, Average Reward 0.029, Asset 1041622.11, Validation Asset 998751.80\n",
      "Episode 1100, Reward 0.016, Average Reward 0.024, Asset 1013777.71, Validation Asset 989825.790\n",
      "Episode 1200, Reward 0.014, Average Reward 0.023, Asset 1011372.33, Validation Asset 982899.037\n",
      "Episode 1300, Reward 0.007, Average Reward 0.027, Asset 1004262.65, Validation Asset 986361.040\n",
      "Episode 1400, Reward -0.006, Average Reward 0.025, Asset 991800.11, Validation Asset 1002534.70\n",
      "Episode 1500, Reward 0.050, Average Reward 0.026, Asset 1050022.04, Validation Asset 980273.642\n",
      "Episode 1600, Reward 0.013, Average Reward 0.025, Asset 1010570.70, Validation Asset 973442.684\n",
      "Episode 1700, Reward 0.043, Average Reward 0.021, Asset 1042423.74, Validation Asset 1002534.70\n",
      "Episode 1800, Reward 0.040, Average Reward 0.027, Asset 1039320.45, Validation Asset 988959.694\n",
      "Episode 1900, Reward 0.018, Average Reward 0.024, Asset 1016979.90, Validation Asset 988959.977\n",
      "Episode 2000, Reward 0.008, Average Reward 0.020, Asset 1006083.18, Validation Asset 999761.344\n",
      "Episode 2100, Reward 0.010, Average Reward 0.026, Asset 1007686.35, Validation Asset 988959.590\n",
      "Episode 2200, Reward 0.002, Average Reward 0.025, Asset 999813.65, Validation Asset 991555.9462\n",
      "Episode 2300, Reward 0.028, Average Reward 0.027, Asset 1026668.01, Validation Asset 992422.190\n",
      "Episode 2400, Reward 0.007, Average Reward 0.023, Asset 1004262.65, Validation Asset 980273.644\n",
      "Episode 2500, Reward 0.018, Average Reward 0.024, Asset 1016979.90, Validation Asset 991555.947\n",
      "Episode 2600, Reward 0.001, Average Reward 0.023, Asset 998219.82, Validation Asset 998751.8070\n",
      "Episode 2700, Reward 0.051, Average Reward 0.024, Asset 1050823.69, Validation Asset 998751.804\n",
      "Episode 2800, Reward 0.025, Average Reward 0.024, Asset 1023800.26, Validation Asset 975175.150\n",
      "Episode 2900, Reward 0.038, Average Reward 0.024, Asset 1036133.66, Validation Asset 988959.590\n",
      "Episode 3000, Reward 0.040, Average Reward 0.027, Asset 1038519.14, Validation Asset 1008594.27\n",
      "Episode 3100, Reward 0.044, Average Reward 0.026, Asset 1043225.40, Validation Asset 993288.394\n",
      "Episode 3200, Reward 0.019, Average Reward 0.026, Asset 1017788.05, Validation Asset 997885.707\n",
      "Episode 3300, Reward 0.043, Average Reward 0.024, Asset 1042423.74, Validation Asset 993288.272\n",
      "Episode 3400, Reward 0.029, Average Reward 0.023, Asset 1027452.16, Validation Asset 998751.802\n",
      "Episode 3500, Reward 0.029, Average Reward 0.031, Asset 1027452.16, Validation Asset 1011192.84\n",
      "Episode 3600, Reward 0.032, Average Reward 0.025, Asset 1030493.45, Validation Asset 974308.884\n",
      "Episode 3700, Reward 0.050, Average Reward 0.024, Asset 1050022.04, Validation Asset 990691.994\n",
      "Episode 3800, Reward 0.032, Average Reward 0.023, Asset 1030493.45, Validation Asset 995307.527\n",
      "Episode 3900, Reward 0.002, Average Reward 0.027, Asset 999813.65, Validation Asset 1001482.655\n",
      "Episode 4000, Reward 0.024, Average Reward 0.027, Asset 1022997.24, Validation Asset 1001482.65\n",
      "Episode 4100, Reward 0.001, Average Reward 0.021, Asset 998219.82, Validation Asset 1006867.115\n",
      "Episode 4200, Reward 0.040, Average Reward 0.022, Asset 1038519.14, Validation Asset 878839.700\n",
      "Episode 4300, Reward 0.036, Average Reward 0.022, Asset 1034510.83, Validation Asset 994110.163\n",
      "Episode 4400, Reward 0.029, Average Reward 0.024, Asset 1027452.16, Validation Asset 994354.874\n",
      "Episode 4500, Reward 0.010, Average Reward 0.025, Asset 1007686.35, Validation Asset 969886.336\n",
      "Episode 4600, Reward 0.003, Average Reward 0.024, Asset 1000615.31, Validation Asset 923356.856\n",
      "Episode 4700, Reward 0.051, Average Reward 0.026, Asset 1050022.04, Validation Asset 922133.163\n",
      "Episode 4800, Reward 0.009, Average Reward 0.024, Asset 1006884.69, Validation Asset 974536.379\n",
      "Episode 4900, Reward 0.022, Average Reward 0.030, Asset 1020592.28, Validation Asset 951552.995\n",
      "Episode 5000, Reward 0.028, Average Reward 0.027, Asset 1026668.01, Validation Asset 972088.983\n",
      "Episode 5100, Reward 0.002, Average Reward 0.028, Asset 999813.65, Validation Asset 954734.6036\n",
      "Episode 5200, Reward 0.024, Average Reward 0.024, Asset 1022997.24, Validation Asset 993131.175\n",
      "Episode 5300, Reward 0.031, Average Reward 0.024, Asset 1029691.79, Validation Asset 959384.584\n",
      "Episode 5400, Reward 0.044, Average Reward 0.026, Asset 1043225.40, Validation Asset 946340.296\n",
      "Episode 5500, Reward 0.007, Average Reward 0.029, Asset 1004262.65, Validation Asset 917483.166\n",
      "Episode 5600, Reward 0.043, Average Reward 0.022, Asset 1041622.11, Validation Asset 936061.255\n",
      "Episode 5700, Reward 0.021, Average Reward 0.027, Asset 1020188.17, Validation Asset 917000.395\n",
      "Episode 5800, Reward 0.044, Average Reward 0.024, Asset 1043225.40, Validation Asset 990437.645\n",
      "Episode 5900, Reward -0.001, Average Reward 0.026, Asset 996616.12, Validation Asset 992152.228\n",
      "Episode 6000, Reward 0.003, Average Reward 0.025, Asset 1000615.31, Validation Asset 995333.828\n",
      "Episode 6100, Reward 0.007, Average Reward 0.024, Asset 1004262.65, Validation Asset 943158.686\n",
      "Episode 6200, Reward 0.030, Average Reward 0.022, Asset 1028890.13, Validation Asset 963789.888\n",
      "Episode 6300, Reward 0.055, Average Reward 0.025, Asset 1054677.99, Validation Asset 932145.435\n",
      "Episode 6400, Reward 0.021, Average Reward 0.023, Asset 1019390.77, Validation Asset 936061.253\n",
      "Episode 6500, Reward 0.030, Average Reward 0.023, Asset 1028890.13, Validation Asset 993131.179\n",
      "Episode 6600, Reward 0.018, Average Reward 0.026, Asset 1016178.25, Validation Asset 977673.822\n",
      "Episode 6700, Reward 0.044, Average Reward 0.025, Asset 1043225.40, Validation Asset 975943.574\n",
      "Episode 6800, Reward 0.043, Average Reward 0.025, Asset 1042423.74, Validation Asset 987227.242\n",
      "Episode 6900, Reward 0.051, Average Reward 0.028, Asset 1050823.69, Validation Asset 993288.230\n",
      "Episode 7000, Reward 0.019, Average Reward 0.029, Asset 1017788.05, Validation Asset 975943.572\n",
      "Episode 7100, Reward 0.043, Average Reward 0.025, Asset 1041622.11, Validation Asset 990691.997\n",
      "Episode 7200, Reward 0.007, Average Reward 0.024, Asset 1004262.65, Validation Asset 987227.307\n",
      "Episode 7300, Reward -0.004, Average Reward 0.024, Asset 993403.42, Validation Asset 982899.032\n",
      "Episode 7400, Reward 0.038, Average Reward 0.023, Asset 1036133.66, Validation Asset 988959.692\n",
      "Episode 7500, Reward 0.028, Average Reward 0.026, Asset 1026668.01, Validation Asset 975943.570\n",
      "Episode 7600, Reward 0.029, Average Reward 0.029, Asset 1027452.16, Validation Asset 976807.612\n",
      "Episode 7700, Reward 0.006, Average Reward 0.027, Asset 1003818.51, Validation Asset 1011192.84\n",
      "Episode 7800, Reward 0.034, Average Reward 0.024, Asset 1032096.49, Validation Asset 973442.174\n",
      "Episode 7900, Reward 0.034, Average Reward 0.024, Asset 1032096.49, Validation Asset 993269.340\n",
      "Episode 8000, Reward 0.021, Average Reward 0.027, Asset 1019390.77, Validation Asset 989825.782\n",
      "Episode 8100, Reward 0.021, Average Reward 0.026, Asset 1019390.77, Validation Asset 986361.100\n",
      "Episode 8200, Reward 0.031, Average Reward 0.029, Asset 1029691.79, Validation Asset 1004267.10\n",
      "Episode 8300, Reward 0.015, Average Reward 0.023, Asset 1012976.07, Validation Asset 1001667.92\n",
      "Episode 8400, Reward 0.039, Average Reward 0.028, Asset 1038519.14, Validation Asset 988959.977\n",
      "Episode 8500, Reward 0.035, Average Reward 0.023, Asset 1034510.83, Validation Asset 987227.300\n",
      "Episode 8600, Reward 0.040, Average Reward 0.024, Asset 1039320.45, Validation Asset 980289.780\n",
      "Episode 8700, Reward 0.032, Average Reward 0.027, Asset 1030493.45, Validation Asset 1001667.92\n",
      "Episode 8800, Reward 0.043, Average Reward 0.025, Asset 1041622.11, Validation Asset 993269.340\n",
      "Episode 8900, Reward 0.005, Average Reward 0.026, Asset 1003016.85, Validation Asset 991555.944\n",
      "Episode 9000, Reward 0.010, Average Reward 0.026, Asset 1007686.35, Validation Asset 978153.644\n",
      "Episode 9100, Reward 0.028, Average Reward 0.025, Asset 1026668.01, Validation Asset 992422.147\n",
      "Episode 9200, Reward 0.021, Average Reward 0.024, Asset 1019390.77, Validation Asset 987227.302\n",
      "Episode 9300, Reward 0.014, Average Reward 0.025, Asset 1011372.33, Validation Asset 1011192.84\n",
      "Episode 9400, Reward 0.033, Average Reward 0.028, Asset 1032096.49, Validation Asset 975176.684\n",
      "Episode 9500, Reward 0.002, Average Reward 0.022, Asset 999813.65, Validation Asset 978540.0182\n",
      "Episode 9600, Reward 0.036, Average Reward 0.024, Asset 1034510.83, Validation Asset 1002534.70\n",
      "Episode 9700, Reward 0.045, Average Reward 0.023, Asset 1044027.06, Validation Asset 978153.642\n",
      "Episode 9800, Reward 0.007, Average Reward 0.024, Asset 1004262.65, Validation Asset 989825.794\n",
      "Episode 9900, Reward 0.050, Average Reward 0.028, Asset 1050022.04, Validation Asset 993269.342\n",
      "Episode 10000, Reward 0.008, Average Reward 0.022, Asset 1006083.18, Validation Asset 983765.23\n",
      "Episode 10100, Reward 0.044, Average Reward 0.027, Asset 1043225.40, Validation Asset 988959.974\n",
      "Episode 10200, Reward 0.030, Average Reward 0.025, Asset 1028890.13, Validation Asset 992422.194\n",
      "Episode 10300, Reward 0.029, Average Reward 0.022, Asset 1027452.16, Validation Asset 987227.487\n",
      "Episode 10400, Reward 0.030, Average Reward 0.030, Asset 1028890.13, Validation Asset 1000801.74\n",
      "Episode 10500, Reward 0.036, Average Reward 0.026, Asset 1034510.83, Validation Asset 998751.817\n",
      "Episode 10600, Reward 0.031, Average Reward 0.031, Asset 1029691.79, Validation Asset 984596.904\n",
      "Episode 10700, Reward 0.029, Average Reward 0.026, Asset 1027452.16, Validation Asset 978540.014\n",
      "Episode 10800, Reward 0.014, Average Reward 0.027, Asset 1011372.33, Validation Asset 982899.034\n",
      "Episode 10900, Reward 0.040, Average Reward 0.027, Asset 1039320.45, Validation Asset 999761.344\n",
      "Episode 11000, Reward 0.050, Average Reward 0.029, Asset 1050022.04, Validation Asset 976807.602\n",
      "Episode 11100, Reward 0.024, Average Reward 0.023, Asset 1022997.24, Validation Asset 975943.577\n",
      "Episode 11200, Reward 0.004, Average Reward 0.023, Asset 1001413.53, Validation Asset 974308.887\n",
      "Episode 11300, Reward 0.051, Average Reward 0.026, Asset 1050823.69, Validation Asset 998758.820\n",
      "Episode 11400, Reward -0.006, Average Reward 0.023, Asset 991800.11, Validation Asset 1008594.27\n",
      "Episode 11500, Reward 0.018, Average Reward 0.024, Asset 1016178.25, Validation Asset 975176.682\n",
      "Episode 11600, Reward 0.031, Average Reward 0.023, Asset 1029691.79, Validation Asset 990692.042\n",
      "Episode 11700, Reward 0.002, Average Reward 0.026, Asset 999813.65, Validation Asset 991555.9480\n",
      "Episode 11800, Reward 0.014, Average Reward 0.026, Asset 1011372.33, Validation Asset 991555.942\n",
      "Episode 11900, Reward 0.003, Average Reward 0.025, Asset 1000615.31, Validation Asset 986361.040\n",
      "Episode 12000, Reward 0.024, Average Reward 0.025, Asset 1022997.24, Validation Asset 993269.340\n",
      "Episode 12100, Reward 0.013, Average Reward 0.024, Asset 1010570.70, Validation Asset 980289.812\n",
      "Episode 12200, Reward 0.015, Average Reward 0.023, Asset 1013777.71, Validation Asset 976807.600\n",
      "Episode 12300, Reward 0.005, Average Reward 0.026, Asset 1003016.85, Validation Asset 982021.504\n",
      "Episode 12400, Reward 0.016, Average Reward 0.025, Asset 1013777.71, Validation Asset 991555.942\n",
      "Episode 12500, Reward 0.043, Average Reward 0.021, Asset 1041622.11, Validation Asset 986361.100\n",
      "Episode 12600, Reward 0.043, Average Reward 0.023, Asset 1042423.74, Validation Asset 1001667.92\n",
      "Episode 12700, Reward 0.005, Average Reward 0.025, Asset 1003016.85, Validation Asset 990692.040\n",
      "Episode 12800, Reward 0.022, Average Reward 0.024, Asset 1020592.28, Validation Asset 993265.272\n",
      "Episode 12900, Reward 0.023, Average Reward 0.022, Asset 1021393.94, Validation Asset 997885.707\n",
      "Episode 13000, Reward 0.029, Average Reward 0.023, Asset 1027452.16, Validation Asset 978540.012\n",
      "Episode 13100, Reward 0.038, Average Reward 0.020, Asset 1036935.30, Validation Asset 975176.687\n",
      "Episode 13200, Reward 0.022, Average Reward 0.023, Asset 1020188.17, Validation Asset 976807.617\n",
      "Episode 13300, Reward 0.052, Average Reward 0.022, Asset 1051471.34, Validation Asset 985494.720\n",
      "Episode 13400, Reward 0.013, Average Reward 0.019, Asset 1010570.70, Validation Asset 993288.232\n",
      "Episode 13500, Reward 0.021, Average Reward 0.028, Asset 1019390.77, Validation Asset 998751.802\n",
      "Episode 13600, Reward 0.021, Average Reward 0.025, Asset 1020188.17, Validation Asset 984596.902\n",
      "Episode 13700, Reward 0.013, Average Reward 0.025, Asset 1010570.70, Validation Asset 978540.017\n",
      "Episode 13800, Reward 0.051, Average Reward 0.024, Asset 1050022.04, Validation Asset 991555.947\n",
      "Episode 13900, Reward 0.035, Average Reward 0.024, Asset 1034510.83, Validation Asset 976807.602\n",
      "Episode 14000, Reward 0.007, Average Reward 0.026, Asset 1004262.65, Validation Asset 989822.882\n",
      "Episode 14100, Reward 0.019, Average Reward 0.026, Asset 1017788.05, Validation Asset 1008594.27\n",
      "Episode 14200, Reward 0.035, Average Reward 0.025, Asset 1034510.83, Validation Asset 977673.824\n",
      "Episode 14300, Reward 0.006, Average Reward 0.023, Asset 1003818.51, Validation Asset 976807.612\n",
      "Episode 14400, Reward 0.050, Average Reward 0.026, Asset 1050022.04, Validation Asset 998751.800\n",
      "Episode 14500, Reward 0.038, Average Reward 0.022, Asset 1036935.30, Validation Asset 987227.484\n",
      "Episode 14600, Reward 0.045, Average Reward 0.028, Asset 1044027.06, Validation Asset 998751.802\n",
      "Episode 14700, Reward 0.019, Average Reward 0.024, Asset 1016979.90, Validation Asset 997020.020\n",
      "Episode 14800, Reward 0.033, Average Reward 0.028, Asset 1032096.49, Validation Asset 991555.947\n",
      "Episode 14900, Reward 0.038, Average Reward 0.026, Asset 1036133.66, Validation Asset 992422.197\n",
      "Episode 15000, Reward 0.042, Average Reward 0.023, Asset 1041622.11, Validation Asset 993288.394\n",
      "Episode 15100, Reward 0.046, Average Reward 0.027, Asset 1045035.02, Validation Asset 995306.930\n",
      "Episode 15200, Reward 0.031, Average Reward 0.027, Asset 1029691.79, Validation Asset 991564.824\n",
      "Episode 15300, Reward 0.045, Average Reward 0.022, Asset 1044027.06, Validation Asset 985478.653\n",
      "Episode 15400, Reward 0.032, Average Reward 0.024, Asset 1030493.45, Validation Asset 984612.235\n",
      "Episode 15500, Reward 0.021, Average Reward 0.024, Asset 1020188.17, Validation Asset 982000.534\n",
      "Episode 15600, Reward 0.025, Average Reward 0.025, Asset 1023800.26, Validation Asset 998751.747\n",
      "Episode 15700, Reward -0.001, Average Reward 0.024, Asset 996616.12, Validation Asset 984612.235\n",
      "Episode 15800, Reward 0.014, Average Reward 0.020, Asset 1011372.33, Validation Asset 978540.055\n",
      "Episode 15900, Reward 0.044, Average Reward 0.023, Asset 1043225.40, Validation Asset 991564.835\n",
      "Episode 16000, Reward 0.004, Average Reward 0.025, Asset 1001413.53, Validation Asset 999083.385\n",
      "Episode 16100, Reward 0.034, Average Reward 0.026, Asset 1032096.49, Validation Asset 1007727.13\n",
      "Episode 16200, Reward 0.035, Average Reward 0.023, Asset 1034510.83, Validation Asset 997886.294\n",
      "Episode 16300, Reward 0.033, Average Reward 0.028, Asset 1032096.49, Validation Asset 1004267.37\n",
      "Episode 16400, Reward 0.015, Average Reward 0.021, Asset 1013777.71, Validation Asset 980268.165\n",
      "Episode 16500, Reward -0.006, Average Reward 0.020, Asset 991800.11, Validation Asset 988070.104\n",
      "Episode 16600, Reward 0.009, Average Reward 0.023, Asset 1006884.69, Validation Asset 991564.834\n",
      "Episode 16700, Reward 0.007, Average Reward 0.025, Asset 1004262.65, Validation Asset 1013788.02\n",
      "Episode 16800, Reward 0.024, Average Reward 0.027, Asset 1022997.24, Validation Asset 993294.472\n",
      "Episode 16900, Reward -0.004, Average Reward 0.025, Asset 993403.42, Validation Asset 978153.971\n",
      "Episode 17000, Reward 0.035, Average Reward 0.027, Asset 1034510.83, Validation Asset 993265.272\n",
      "Episode 17100, Reward 0.002, Average Reward 0.024, Asset 999813.65, Validation Asset 1005133.774\n",
      "Episode 17200, Reward 0.045, Average Reward 0.022, Asset 1044027.06, Validation Asset 992422.263\n",
      "Episode 17300, Reward 0.044, Average Reward 0.025, Asset 1043225.40, Validation Asset 1003400.95\n",
      "Episode 17400, Reward 0.013, Average Reward 0.021, Asset 1010570.70, Validation Asset 995301.172\n",
      "Episode 17500, Reward 0.033, Average Reward 0.027, Asset 1032096.49, Validation Asset 997034.697\n",
      "Episode 17600, Reward 0.018, Average Reward 0.027, Asset 1016979.90, Validation Asset 984636.595\n",
      "Episode 17700, Reward 0.002, Average Reward 0.025, Asset 999813.65, Validation Asset 1007727.137\n",
      "Episode 17800, Reward 0.042, Average Reward 0.027, Asset 1041622.11, Validation Asset 985478.655\n",
      "Episode 17900, Reward 0.006, Average Reward 0.024, Asset 1003818.51, Validation Asset 988959.973\n",
      "Episode 18000, Reward 0.024, Average Reward 0.025, Asset 1022997.24, Validation Asset 999083.385\n",
      "Episode 18100, Reward 0.036, Average Reward 0.026, Asset 1034510.83, Validation Asset 997034.690\n",
      "Episode 18200, Reward 0.024, Average Reward 0.021, Asset 1022997.24, Validation Asset 982000.534\n",
      "Episode 18300, Reward 0.052, Average Reward 0.029, Asset 1051471.34, Validation Asset 1013788.02\n",
      "Episode 18400, Reward 0.018, Average Reward 0.025, Asset 1016178.25, Validation Asset 993294.474\n",
      "Episode 18500, Reward 0.035, Average Reward 0.022, Asset 1034510.83, Validation Asset 988959.973\n",
      "Episode 18600, Reward 0.031, Average Reward 0.026, Asset 1029691.79, Validation Asset 984636.597\n",
      "Episode 18700, Reward 0.030, Average Reward 0.025, Asset 1028890.13, Validation Asset 990698.874\n",
      "Episode 18800, Reward 0.035, Average Reward 0.026, Asset 1034510.83, Validation Asset 998751.745\n",
      "Episode 18900, Reward 0.016, Average Reward 0.023, Asset 1013777.71, Validation Asset 1009459.50\n",
      "Episode 19000, Reward 0.033, Average Reward 0.021, Asset 1032096.49, Validation Asset 996167.600\n",
      "Episode 19100, Reward 0.019, Average Reward 0.024, Asset 1017788.05, Validation Asset 994155.247\n",
      "Episode 19200, Reward 0.024, Average Reward 0.022, Asset 1022997.24, Validation Asset 1003400.95\n",
      "Episode 19300, Reward 0.031, Average Reward 0.027, Asset 1029691.79, Validation Asset 998216.923\n",
      "Episode 19400, Reward 0.021, Average Reward 0.024, Asset 1019390.77, Validation Asset 988959.977\n",
      "Episode 19500, Reward 0.042, Average Reward 0.027, Asset 1041622.11, Validation Asset 983765.232\n",
      "Episode 19600, Reward 0.046, Average Reward 0.022, Asset 1045035.02, Validation Asset 980289.812\n",
      "Episode 19700, Reward 0.030, Average Reward 0.023, Asset 1028890.13, Validation Asset 974308.882\n",
      "Episode 19800, Reward 0.023, Average Reward 0.027, Asset 1021393.94, Validation Asset 988959.974\n",
      "Episode 19900, Reward 0.040, Average Reward 0.027, Asset 1039320.45, Validation Asset 978540.014\n",
      "Episode 20000, Reward 0.002, Average Reward 0.025, Asset 999813.65, Validation Asset 986361.0492\n"
     ]
    }
   ],
   "source": [
    "rewards_log, _ = train(agent, env, 2000, max_t, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_init = eps\n",
    "# constant = C\n",
    "# num_frame =1\n",
    "\n",
    "# rewards_log = []\n",
    "# average_log = []\n",
    "# eps = eps_init\n",
    "\n",
    "# for i in range(1, 1 + num_episode):\n",
    "#     episodic_reward = 0\n",
    "#     done = False\n",
    "#     frame = env.reset()\n",
    "#     state_deque = deque(maxlen=num_frame)\n",
    "#     for _ in range(num_frame):\n",
    "#         state_deque.append(frame)\n",
    "#     state = np.stack(state_deque, axis=0)\n",
    "#     state = np.expand_dims(state, axis=0)\n",
    "#     t = 0\n",
    "\n",
    "#     while not done and t < max_t:\n",
    "\n",
    "#         t += 1\n",
    "#         action = agent.act(state, eps)\n",
    "#         frame, reward, done = env.step(action)\n",
    "#         state_deque.append(frame)\n",
    "#         next_state = np.stack(state_deque, axis=0)\n",
    "#         next_state = np.expand_dims(next_state, axis=0)\n",
    "#         agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "#         if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "#             agent.learn()\n",
    "#             agent.soft_update(agent.tau)\n",
    "\n",
    "#         state = next_state.copy()\n",
    "#         episodic_reward += reward\n",
    "\n",
    "#     rewards_log.append(episodic_reward)\n",
    "#     average_log.append(np.mean(rewards_log[-100:]))\n",
    "#     print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}'.format(i, episodic_reward, average_log[-1]), end='')\n",
    "#     if i % 100 == 0:\n",
    "#         print()\n",
    "\n",
    "#     eps = max(eps * eps_decay, eps_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
