{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "time_period = 15\n",
    "class Q_Network(nn.Module):\n",
    "    '''\n",
    "    The input of this network should have shape (num_frame, 80, 80)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_frame, num_action, N):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_frame, out_channels=32, kernel_size=(2,1), stride=1, padding=2)  # 16, 20, 20\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(2,1), stride=1)  # 32, 9, 9\n",
    "        self.conv5 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=(2,2), stride=1)  # 32, 9, 9\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2,1))\n",
    "        self.fc1 = nn.Linear(576, 256)\n",
    "        self.fc2 = nn.Linear(256, num_action*N)\n",
    "        self.action_size = num_action\n",
    "        self.N = N\n",
    "\n",
    "    def forward(self, image):\n",
    "        x = F.relu(self.pool(self.conv1(image)))\n",
    "        x = F.relu(self.pool(self.conv2(x)))\n",
    "        x = F.relu(self.pool(self.conv3(x)))\n",
    "        x = x.view(-1, 576)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = x.view(-1, self.action_size, self.N)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "file = open('../../FinBert/stock_data_full.bin', 'rb')\n",
    "data = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = ['AAPL','AMZN','C','GOOG','JPM','NFLX','PLTR']\n",
    "for i in range(len(codes)):\n",
    "    data[i]['symbol'] = codes[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../FinRL/concat_data.csv')\n",
    "df=df[['date', 'open', 'high', 'low', 'close', 'volume',\n",
    "       'positive', 'neutral', 'negative', 'tic']]\n",
    "df['date'] = [x[:10] for x in df['date']]\n",
    "df = df[(df['date']>='2022-01-01') & (df['date']<'2023-09-30')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../min_data_adjust.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = data[data['symbol']=='AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>vwap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:00:00+00:00</td>\n",
       "      <td>176.23</td>\n",
       "      <td>176.23</td>\n",
       "      <td>176.1800</td>\n",
       "      <td>176.1800</td>\n",
       "      <td>1118.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>176.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:02:00+00:00</td>\n",
       "      <td>176.30</td>\n",
       "      <td>176.31</td>\n",
       "      <td>176.2800</td>\n",
       "      <td>176.2800</td>\n",
       "      <td>1218.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>176.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:03:00+00:00</td>\n",
       "      <td>176.25</td>\n",
       "      <td>176.27</td>\n",
       "      <td>176.2500</td>\n",
       "      <td>176.2700</td>\n",
       "      <td>814.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>176.260000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:04:00+00:00</td>\n",
       "      <td>176.20</td>\n",
       "      <td>176.20</td>\n",
       "      <td>176.1200</td>\n",
       "      <td>176.1200</td>\n",
       "      <td>3744.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>176.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2022-01-03 09:05:00+00:00</td>\n",
       "      <td>176.17</td>\n",
       "      <td>176.17</td>\n",
       "      <td>176.1700</td>\n",
       "      <td>176.1700</td>\n",
       "      <td>464.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>176.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343433</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:53:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>209.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>171.305766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343434</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:54:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>171.3000</td>\n",
       "      <td>810.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>171.305889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343435</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:57:00+00:00</td>\n",
       "      <td>171.32</td>\n",
       "      <td>171.32</td>\n",
       "      <td>171.3200</td>\n",
       "      <td>171.3200</td>\n",
       "      <td>439.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>171.330957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343436</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:58:00+00:00</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.30</td>\n",
       "      <td>171.2699</td>\n",
       "      <td>171.2699</td>\n",
       "      <td>532.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>171.282998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343437</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2023-09-29 23:59:00+00:00</td>\n",
       "      <td>171.23</td>\n",
       "      <td>171.27</td>\n",
       "      <td>171.2300</td>\n",
       "      <td>171.2300</td>\n",
       "      <td>3114.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>171.235370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>343438 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       symbol                  timestamp    open    high       low     close  \\\n",
       "0        AAPL  2022-01-03 09:00:00+00:00  176.23  176.23  176.1800  176.1800   \n",
       "1        AAPL  2022-01-03 09:02:00+00:00  176.30  176.31  176.2800  176.2800   \n",
       "2        AAPL  2022-01-03 09:03:00+00:00  176.25  176.27  176.2500  176.2700   \n",
       "3        AAPL  2022-01-03 09:04:00+00:00  176.20  176.20  176.1200  176.1200   \n",
       "4        AAPL  2022-01-03 09:05:00+00:00  176.17  176.17  176.1700  176.1700   \n",
       "...       ...                        ...     ...     ...       ...       ...   \n",
       "343433   AAPL  2023-09-29 23:53:00+00:00  171.30  171.30  171.3000  171.3000   \n",
       "343434   AAPL  2023-09-29 23:54:00+00:00  171.30  171.30  171.3000  171.3000   \n",
       "343435   AAPL  2023-09-29 23:57:00+00:00  171.32  171.32  171.3200  171.3200   \n",
       "343436   AAPL  2023-09-29 23:58:00+00:00  171.30  171.30  171.2699  171.2699   \n",
       "343437   AAPL  2023-09-29 23:59:00+00:00  171.23  171.27  171.2300  171.2300   \n",
       "\n",
       "        volume  trade_count        vwap  \n",
       "0       1118.0         65.0  176.210000  \n",
       "1       1218.0         26.0  176.300000  \n",
       "2        814.0         30.0  176.260000  \n",
       "3       3744.0        114.0  176.180000  \n",
       "4        464.0         33.0  176.150000  \n",
       "...        ...          ...         ...  \n",
       "343433   209.0          8.0  171.305766  \n",
       "343434   810.0         16.0  171.305889  \n",
       "343435   439.0         20.0  171.330957  \n",
       "343436   532.0         11.0  171.282998  \n",
       "343437  3114.0         19.0  171.235370  \n",
       "\n",
       "[343438 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = df[df['tic']=='AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>positive</th>\n",
       "      <th>neutral</th>\n",
       "      <th>negative</th>\n",
       "      <th>tic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>177.830002</td>\n",
       "      <td>182.880005</td>\n",
       "      <td>177.710007</td>\n",
       "      <td>182.009995</td>\n",
       "      <td>104487900</td>\n",
       "      <td>-2.525743</td>\n",
       "      <td>3.722111</td>\n",
       "      <td>-3.922445</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>182.630005</td>\n",
       "      <td>182.940002</td>\n",
       "      <td>179.119995</td>\n",
       "      <td>179.699997</td>\n",
       "      <td>99310400</td>\n",
       "      <td>-2.752612</td>\n",
       "      <td>3.370780</td>\n",
       "      <td>-3.351379</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>179.610001</td>\n",
       "      <td>180.169998</td>\n",
       "      <td>174.639999</td>\n",
       "      <td>174.919998</td>\n",
       "      <td>94537600</td>\n",
       "      <td>-2.561095</td>\n",
       "      <td>3.561730</td>\n",
       "      <td>-3.588621</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>172.699997</td>\n",
       "      <td>175.300003</td>\n",
       "      <td>171.639999</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>96904000</td>\n",
       "      <td>-2.294448</td>\n",
       "      <td>3.207229</td>\n",
       "      <td>-3.612424</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>172.889999</td>\n",
       "      <td>174.139999</td>\n",
       "      <td>171.029999</td>\n",
       "      <td>172.169998</td>\n",
       "      <td>86709100</td>\n",
       "      <td>-2.325235</td>\n",
       "      <td>3.084295</td>\n",
       "      <td>-3.352122</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3028</th>\n",
       "      <td>2023-09-25</td>\n",
       "      <td>174.199997</td>\n",
       "      <td>176.970001</td>\n",
       "      <td>174.149994</td>\n",
       "      <td>176.080002</td>\n",
       "      <td>46172700</td>\n",
       "      <td>-2.361765</td>\n",
       "      <td>3.181928</td>\n",
       "      <td>-3.037302</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>2023-09-26</td>\n",
       "      <td>174.820007</td>\n",
       "      <td>175.199997</td>\n",
       "      <td>171.660004</td>\n",
       "      <td>171.960007</td>\n",
       "      <td>64588900</td>\n",
       "      <td>-1.893191</td>\n",
       "      <td>2.688069</td>\n",
       "      <td>-3.369864</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>2023-09-27</td>\n",
       "      <td>172.619995</td>\n",
       "      <td>173.039993</td>\n",
       "      <td>169.050003</td>\n",
       "      <td>170.429993</td>\n",
       "      <td>66921800</td>\n",
       "      <td>-3.139558</td>\n",
       "      <td>3.359877</td>\n",
       "      <td>-2.654129</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3049</th>\n",
       "      <td>2023-09-28</td>\n",
       "      <td>169.339996</td>\n",
       "      <td>172.029999</td>\n",
       "      <td>167.619995</td>\n",
       "      <td>170.690002</td>\n",
       "      <td>56294400</td>\n",
       "      <td>-2.045589</td>\n",
       "      <td>2.791628</td>\n",
       "      <td>-3.063628</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3057</th>\n",
       "      <td>2023-09-29</td>\n",
       "      <td>172.020004</td>\n",
       "      <td>173.070007</td>\n",
       "      <td>170.339996</td>\n",
       "      <td>171.210007</td>\n",
       "      <td>51814200</td>\n",
       "      <td>-2.751975</td>\n",
       "      <td>3.445238</td>\n",
       "      <td>-3.111207</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date        open        high         low       close     volume  \\\n",
       "17    2022-01-03  177.830002  182.880005  177.710007  182.009995  104487900   \n",
       "21    2022-01-04  182.630005  182.940002  179.119995  179.699997   99310400   \n",
       "26    2022-01-05  179.610001  180.169998  174.639999  174.919998   94537600   \n",
       "35    2022-01-06  172.699997  175.300003  171.639999  172.000000   96904000   \n",
       "42    2022-01-07  172.889999  174.139999  171.029999  172.169998   86709100   \n",
       "...          ...         ...         ...         ...         ...        ...   \n",
       "3028  2023-09-25  174.199997  176.970001  174.149994  176.080002   46172700   \n",
       "3034  2023-09-26  174.820007  175.199997  171.660004  171.960007   64588900   \n",
       "3045  2023-09-27  172.619995  173.039993  169.050003  170.429993   66921800   \n",
       "3049  2023-09-28  169.339996  172.029999  167.619995  170.690002   56294400   \n",
       "3057  2023-09-29  172.020004  173.070007  170.339996  171.210007   51814200   \n",
       "\n",
       "      positive   neutral  negative   tic  \n",
       "17   -2.525743  3.722111 -3.922445  AAPL  \n",
       "21   -2.752612  3.370780 -3.351379  AAPL  \n",
       "26   -2.561095  3.561730 -3.588621  AAPL  \n",
       "35   -2.294448  3.207229 -3.612424  AAPL  \n",
       "42   -2.325235  3.084295 -3.352122  AAPL  \n",
       "...        ...       ...       ...   ...  \n",
       "3028 -2.361765  3.181928 -3.037302  AAPL  \n",
       "3034 -1.893191  2.688069 -3.369864  AAPL  \n",
       "3045 -3.139558  3.359877 -2.654129  AAPL  \n",
       "3049 -2.045589  2.791628 -3.063628  AAPL  \n",
       "3057 -2.751975  3.445238 -3.111207  AAPL  \n",
       "\n",
       "[438 rows x 10 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_22072\\1920598173.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['pctchange'] = (stock_df['close'] - stock_df['open'])/stock_df['open']\n"
     ]
    }
   ],
   "source": [
    "stock_df['pctchange'] = (stock_df['close'] - stock_df['open'])/stock_df['open']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finta import TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_22072\\1998282740.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['SMA42'] = TA.SMA(stock_df, 42)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_22072\\1998282740.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['SMA5'] = TA.SMA(stock_df, 5)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_22072\\1998282740.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['SMA15'] = TA.SMA(stock_df, 15)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_22072\\1998282740.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['AO'] = TA.AO(stock_df)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_22072\\1998282740.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['OVB'] = TA.OBV(stock_df)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_22072\\1998282740.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df[['VW_MACD','MACD_SIGNAL']] = TA.VW_MACD(stock_df)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_22072\\1998282740.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df[['VW_MACD','MACD_SIGNAL']] = TA.VW_MACD(stock_df)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_22072\\1998282740.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['RSI'] = TA.RSI(stock_df)\n",
      "C:\\Users\\12364\\AppData\\Local\\Temp\\ipykernel_22072\\1998282740.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  stock_df['CMO'] = TA.CMO(stock_df)\n"
     ]
    }
   ],
   "source": [
    "stock_df['SMA42'] = TA.SMA(stock_df, 42)\n",
    "stock_df['SMA5'] = TA.SMA(stock_df, 5)\n",
    "stock_df['SMA15'] = TA.SMA(stock_df, 15)\n",
    "stock_df['AO'] = TA.AO(stock_df)\n",
    "stock_df['OVB'] = TA.OBV(stock_df)\n",
    "stock_df[['VW_MACD','MACD_SIGNAL']] = TA.VW_MACD(stock_df)\n",
    "stock_df['RSI'] = TA.RSI(stock_df)\n",
    "stock_df['CMO'] = TA.CMO(stock_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'open', 'high', 'low', 'close', 'volume', 'positive', 'neutral',\n",
       "       'negative', 'tic', 'pctchange', 'SMA42', 'SMA5', 'SMA15', 'AO', 'OVB',\n",
       "       'VW_MACD', 'MACD_SIGNAL', 'RSI', 'CMO'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df_train = stock_df[stock_df['date']<='2023-03-31']\n",
    "stock_df_test = stock_df[stock_df['date']>'2023-03-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([0], maxlen=5)\n",
      "deque([0, 1], maxlen=5)\n",
      "deque([0, 1, 2], maxlen=5)\n",
      "deque([0, 1, 2, 3], maxlen=5)\n",
      "deque([0, 1, 2, 3, 4], maxlen=5)\n",
      "deque([1, 2, 3, 4, 5], maxlen=5)\n",
      "deque([2, 3, 4, 5, 6], maxlen=5)\n",
      "deque([3, 4, 5, 6, 7], maxlen=5)\n",
      "deque([4, 5, 6, 7, 8], maxlen=5)\n",
      "deque([5, 6, 7, 8, 9], maxlen=5)\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "test = deque(maxlen=5)\n",
    "for i in range(10):\n",
    "    test.append(i)\n",
    "    print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, state_size, action_size, bs, lr, tau, gamma, N, kappa, device, visual=False):\n",
    "        '''\n",
    "        When dealing with visual inputs, state_size should work as num_of_frame\n",
    "        '''\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.bs = bs\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "        self.N = N\n",
    "        self.tau = torch.linspace(0, 1, N+1)\n",
    "        self.tau = (self.tau[:-1] + self.tau[1:]) / 2\n",
    "        self.tau_hat = self.tau.to(device).unsqueeze(0) #(1, N)\n",
    "        self.tau = tau\n",
    "        self.kappa = kappa\n",
    "\n",
    "        self.Q_local = Q_Network(self.state_size, self.action_size, N).to(self.device)\n",
    "        self.Q_target = Q_Network(self.state_size, self.action_size, N).to(self.device)\n",
    "        self.soft_update(1)\n",
    "        self.optimizer = optim.Adam(self.Q_local.parameters(), self.lr)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "\n",
    "    def act(self, state, eps=0):\n",
    "        if random.random() > eps:\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.Q_local(state)\n",
    "                action_values = action_values.mean(dim=-1, keepdim=False).view(-1)\n",
    "            return np.argmax(action_values.cpu().numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def learn(self):\n",
    "        experiences = random.sample(self.memory, self.bs)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in experiences])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences]).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        quantiles_local = self.Q_local(states) #(batch_size, action_size, N)\n",
    "        quantiles_local = torch.gather(input=quantiles_local, dim=1, index=actions.unsqueeze(1).repeat(1, 1, self.N)) #(batch_size, 1, N)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            quantiles_target = self.Q_target(next_states) #(batch_size, action_size, N)\n",
    "            next_actions = torch.max(quantiles_target.sum(dim=2, keepdim=True), dim=1, keepdim=True)[1] #(batch_size, 1, 1)\n",
    "            quantiles_target = torch.gather(input=quantiles_target, index=next_actions.repeat(1,1,self.N), dim=1) #(batch_size, 1, N)\n",
    "            quantiles_target = rewards.unsqueeze(1).repeat(1, 1, self.N) + self.gamma * (1 - dones.unsqueeze(1).repeat(1, 1, self.N)) * quantiles_target #(batch_size, 1, N)\n",
    "            quantiles_target = quantiles_target.permute(0, 2, 1) #(batch_size, N, 1)\n",
    "\n",
    "        diff = quantiles_target - quantiles_local #(batch_size, N, N)\n",
    "        tau = self.tau_hat.unsqueeze(0).repeat(diff.size(0), 1, 1) #(batch_size, 1, N)\n",
    "        loss = (tau - (diff<0).float()).abs() * self.huber(diff) #(batch_size, N, N)\n",
    "        loss = loss.mean(dim=2, keepdim=False).sum(dim=1, keepdim=False) #(batch_size,)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.soft_update(self.tau)\n",
    "\n",
    "    def huber(self, u):\n",
    "        if self.kappa > 0:\n",
    "            flag = (u.abs()<self.kappa).float()\n",
    "            loss = 0.5*u.pow(2) * flag + self.kappa*(u.abs()-0.5*self.kappa) * (1-flag)\n",
    "        else:\n",
    "            loss = u.abs()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def soft_update(self, tau):\n",
    "        for target_param, local_param in zip(self.Q_target.parameters(), self.Q_local.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicators = ['open', 'high', 'low', 'close', 'volume', 'positive', 'neutral', 'negative','SMA42', 'SMA5', 'SMA15', 'AO', 'OVB','VW_MACD',\n",
    "#        'MACD_SIGNAL', 'RSI', 'CMO']\n",
    "\n",
    "indicators = ['pctchange', 'volume', 'positive', 'neutral', 'negative','SMA42', 'SMA5', 'SMA15', 'AO', 'OVB','VW_MACD',\n",
    "       'MACD_SIGNAL', 'RSI', 'CMO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stock_Env:\n",
    "    def __init__(self, initial_asset, data, cost):\n",
    "        self.asset = initial_asset\n",
    "        self.cash = initial_asset\n",
    "        self.stock = 0\n",
    "        self.data = data\n",
    "        self.time = data.iloc[time_period]['date']\n",
    "        self.cost = cost\n",
    "        self.history=[]\n",
    "        self.total_cost = 0\n",
    "        self.initial_asset = initial_asset\n",
    "        self.rowid = time_period\n",
    "        self.action_space = np.array(list(range(11)))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.asset = self.initial_asset\n",
    "        self.cash = self.initial_asset\n",
    "        self.stock = 0\n",
    "        self.time = self.data.iloc[100]['date']\n",
    "        self.history=[]\n",
    "        self.total_cost = 0    \n",
    "        self.rowid = time_period\n",
    "        return self.data[:time_period][indicators].values\n",
    "    \n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        states = self.data.iloc[self.rowid]        \n",
    "        self.rowid +=1\n",
    "        if self.rowid == len(self.data)-1:\n",
    "            done = True\n",
    "        next_state = self.data.iloc[self.rowid]\n",
    "        last_asset = self.asset\n",
    "        price = next_state['open']\n",
    "        old_asset = self.cash + self.stock*price\n",
    "        self.asset = old_asset\n",
    "        target_value = action*0.1*self.asset\n",
    "        distance = target_value - self.stock*price\n",
    "        stock_distance = int(distance/(price*(1+self.cost)))\n",
    "        self.stock += stock_distance\n",
    "        self.cash = self.cash - distance - np.abs(stock_distance*self.cost*price)\n",
    "        self.asset = self.cash+self.stock*price\n",
    "        market_value = self.stock * next_state['close']\n",
    "        self.asset = market_value + self.cash\n",
    "        reward = (self.asset - last_asset)/last_asset\n",
    "        self.time = next_state['date']\n",
    "        # self.stock = stock\n",
    "        return (self.data[self.rowid-time_period:self.rowid][indicators].values, reward, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make()\n",
    "env = Stock_Env(1000000, stock_df_train, 0.002)\n",
    "env_test = Stock_Env(1000000, stock_df_test, 0.002)\n",
    "num_episode = 5\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "\n",
    "for _ in range(num_episode):\n",
    "    \n",
    "    # initialize\n",
    "    env.reset()\n",
    "    t = 0\n",
    "    episodic_reward = 0\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        \n",
    "        #env.render()\n",
    "        action = np.random.randint(11) # random action\n",
    "        _, reward, done = env.step(action)\n",
    "        episodic_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    reward_log.append(episodic_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(1, len(env.action_space), 64, 0.001, 0.001, 0.99, 51, 1, 'cuda', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#env = gym.make()\n",
    "num_episode = 20000\n",
    "max_t = 1000\n",
    "reward_log = []\n",
    "average_log = [] # monitor training process\n",
    "eps = 1\n",
    "eps_decay = 0.995\n",
    "eps_min = 0.01\n",
    "C = 4 # update weights every C steps\n",
    "\n",
    "def validation(env, agent):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    episodic_reward = 0\n",
    "    done = False\n",
    "    t = 0\n",
    "    state = env.reset()\n",
    "    while not done and t < max_t:\n",
    "        t += 1\n",
    "        action = agent.act(state, eps)\n",
    "        frame, reward, done = env.step(action)\n",
    "        next_state = frame\n",
    "        state = next_state.copy()\n",
    "        episodic_reward += reward\n",
    "    return env.asset\n",
    "\n",
    "\n",
    "def train(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame=1, constant=0):\n",
    "    rewards_log = []\n",
    "    average_log = []\n",
    "    eps = eps_init\n",
    "\n",
    "    for i in range(1, 1 + num_episode):\n",
    "\n",
    "        episodic_reward = 0\n",
    "        done = False\n",
    "        frame = env.reset()\n",
    "        state_deque = deque(maxlen=num_frame)\n",
    "        for _ in range(num_frame):\n",
    "            state_deque.append(frame)\n",
    "        state = np.stack(state_deque, axis=0)\n",
    "        state = np.expand_dims(state, axis=0)\n",
    "        t = 0\n",
    "\n",
    "        while not done and t < max_t:\n",
    "\n",
    "            t += 1\n",
    "            action = agent.act(state, eps)\n",
    "            frame, reward, done = env.step(action)\n",
    "            state_deque.append(frame)\n",
    "            next_state = np.stack(state_deque, axis=0)\n",
    "            next_state = np.expand_dims(next_state, axis=0)\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "                agent.learn()\n",
    "                agent.soft_update(agent.tau)\n",
    "\n",
    "            state = next_state.copy()\n",
    "            episodic_reward += reward\n",
    "        \n",
    "        val_asset = validation(env_test, agent)\n",
    "\n",
    "        rewards_log.append(episodic_reward)\n",
    "        average_log.append(np.mean(rewards_log[-100:]))\n",
    "        print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}, Asset {:.2f}, Validation Asset {:.2f}'.format(i, episodic_reward, average_log[-1], env.asset, val_asset), end='')\n",
    "        if i % 100 == 0:\n",
    "            print()\n",
    "\n",
    "        eps = max(eps * eps_decay, eps_min)\n",
    "\n",
    "    return rewards_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Reward -0.209, Average Reward -0.198, Asset 796622.07, Validation Asset 923438.31\n",
      "Episode 200, Reward -0.205, Average Reward -0.154, Asset 801698.41, Validation Asset 892236.310\n",
      "Episode 300, Reward -0.268, Average Reward -0.116, Asset 746634.72, Validation Asset 975563.627\n",
      "Episode 400, Reward -0.042, Average Reward -0.085, Asset 939428.02, Validation Asset 939991.534\n",
      "Episode 500, Reward -0.091, Average Reward -0.104, Asset 890522.68, Validation Asset 939189.915\n",
      "Episode 600, Reward -0.065, Average Reward -0.086, Asset 924254.38, Validation Asset 905469.983\n",
      "Episode 700, Reward 0.105, Average Reward -0.071, Asset 1097075.53, Validation Asset 977096.363\n",
      "Episode 800, Reward 0.014, Average Reward -0.085, Asset 996283.92, Validation Asset 1020814.374\n",
      "Episode 900, Reward 0.008, Average Reward -0.086, Asset 984810.98, Validation Asset 924618.6906\n",
      "Episode 1000, Reward -0.147, Average Reward -0.102, Asset 850923.08, Validation Asset 1026252.38\n",
      "Episode 1100, Reward -0.073, Average Reward -0.121, Asset 915396.69, Validation Asset 1076082.70\n",
      "Episode 1200, Reward -0.302, Average Reward -0.111, Asset 710225.85, Validation Asset 924765.757\n",
      "Episode 1300, Reward -0.246, Average Reward -0.100, Asset 760367.78, Validation Asset 915863.254\n",
      "Episode 1400, Reward -0.204, Average Reward -0.066, Asset 802524.65, Validation Asset 889878.656\n",
      "Episode 1500, Reward -0.124, Average Reward -0.091, Asset 868693.17, Validation Asset 975959.027\n",
      "Episode 1600, Reward -0.265, Average Reward -0.047, Asset 754031.60, Validation Asset 962313.469\n",
      "Episode 1700, Reward -0.041, Average Reward -0.039, Asset 939241.78, Validation Asset 932142.190\n",
      "Episode 1800, Reward -0.004, Average Reward -0.045, Asset 990712.74, Validation Asset 894913.331\n",
      "Episode 1900, Reward -0.006, Average Reward -0.063, Asset 963852.96, Validation Asset 982735.065\n",
      "Episode 2000, Reward -0.112, Average Reward -0.042, Asset 865736.73, Validation Asset 1030250.60\n",
      "Episode 2100, Reward -0.015, Average Reward -0.037, Asset 962505.90, Validation Asset 939319.659\n",
      "Episode 2200, Reward 0.024, Average Reward -0.042, Asset 1000868.96, Validation Asset 967476.669\n",
      "Episode 2300, Reward -0.034, Average Reward -0.014, Asset 949564.27, Validation Asset 1004657.32\n",
      "Episode 2400, Reward 0.071, Average Reward -0.036, Asset 1047757.76, Validation Asset 900762.396\n",
      "Episode 2500, Reward -0.145, Average Reward -0.027, Asset 844541.91, Validation Asset 877942.281\n",
      "Episode 2600, Reward -0.007, Average Reward -0.027, Asset 979284.95, Validation Asset 1029320.09\n",
      "Episode 2700, Reward -0.143, Average Reward -0.021, Asset 839690.60, Validation Asset 954392.120\n",
      "Episode 2800, Reward -0.069, Average Reward -0.041, Asset 916184.68, Validation Asset 978975.073\n",
      "Episode 2900, Reward -0.144, Average Reward -0.032, Asset 844205.27, Validation Asset 1022686.59\n",
      "Episode 3000, Reward 0.125, Average Reward -0.017, Asset 1098398.86, Validation Asset 959472.588\n",
      "Episode 3100, Reward -0.258, Average Reward -0.006, Asset 757727.09, Validation Asset 916861.962\n",
      "Episode 3200, Reward 0.024, Average Reward -0.014, Asset 988065.78, Validation Asset 923257.7778\n",
      "Episode 3300, Reward -0.001, Average Reward -0.004, Asset 973586.98, Validation Asset 942467.311\n",
      "Episode 3400, Reward 0.028, Average Reward -0.002, Asset 996444.97, Validation Asset 967167.0630\n",
      "Episode 3500, Reward -0.060, Average Reward -0.013, Asset 919301.91, Validation Asset 934487.034\n",
      "Episode 3600, Reward -0.029, Average Reward -0.015, Asset 970560.49, Validation Asset 1097048.95\n",
      "Episode 3700, Reward -0.032, Average Reward -0.013, Asset 931682.48, Validation Asset 998529.820\n",
      "Episode 3800, Reward 0.060, Average Reward -0.001, Asset 1037229.71, Validation Asset 975512.062\n",
      "Episode 3900, Reward 0.005, Average Reward 0.001, Asset 970271.56, Validation Asset 972397.82936\n",
      "Episode 4000, Reward 0.032, Average Reward 0.001, Asset 1001834.02, Validation Asset 896176.4222\n",
      "Episode 4100, Reward -0.077, Average Reward -0.015, Asset 904641.37, Validation Asset 900257.392\n",
      "Episode 4200, Reward -0.137, Average Reward -0.012, Asset 845411.82, Validation Asset 1016541.19\n",
      "Episode 4300, Reward 0.036, Average Reward -0.004, Asset 1009398.66, Validation Asset 1009151.46\n",
      "Episode 4400, Reward -0.095, Average Reward -0.013, Asset 879874.45, Validation Asset 939794.490\n",
      "Episode 4500, Reward -0.037, Average Reward -0.017, Asset 934444.16, Validation Asset 884352.105\n",
      "Episode 4600, Reward 0.015, Average Reward -0.010, Asset 993112.43, Validation Asset 838370.4806\n",
      "Episode 4700, Reward -0.025, Average Reward -0.001, Asset 958337.73, Validation Asset 898439.575\n",
      "Episode 4800, Reward -0.126, Average Reward 0.001, Asset 860334.61, Validation Asset 934355.4476\n",
      "Episode 4900, Reward -0.018, Average Reward -0.008, Asset 946417.72, Validation Asset 980514.957\n",
      "Episode 5000, Reward -0.025, Average Reward 0.000, Asset 942929.16, Validation Asset 879052.1238\n",
      "Episode 5100, Reward 0.116, Average Reward 0.005, Asset 1092235.41, Validation Asset 914909.1422\n",
      "Episode 5200, Reward -0.026, Average Reward -0.004, Asset 935312.14, Validation Asset 998459.644\n",
      "Episode 5300, Reward 0.035, Average Reward 0.006, Asset 985837.81, Validation Asset 961457.39486\n",
      "Episode 5400, Reward -0.029, Average Reward -0.014, Asset 949253.38, Validation Asset 962193.330\n",
      "Episode 5500, Reward 0.025, Average Reward -0.001, Asset 995336.36, Validation Asset 1007819.323\n",
      "Episode 5600, Reward 0.084, Average Reward -0.003, Asset 1064389.23, Validation Asset 962280.639\n",
      "Episode 5700, Reward -0.069, Average Reward -0.009, Asset 922444.85, Validation Asset 997038.131\n",
      "Episode 5800, Reward -0.081, Average Reward -0.020, Asset 903202.78, Validation Asset 1009760.41\n",
      "Episode 5900, Reward 0.065, Average Reward -0.005, Asset 1044497.06, Validation Asset 862816.462\n",
      "Episode 6000, Reward 0.113, Average Reward -0.011, Asset 1087214.12, Validation Asset 961464.297\n",
      "Episode 6100, Reward 0.019, Average Reward 0.009, Asset 997382.20, Validation Asset 976652.43023\n",
      "Episode 6200, Reward -0.008, Average Reward 0.004, Asset 957259.77, Validation Asset 984230.525\n",
      "Episode 6300, Reward 0.088, Average Reward -0.004, Asset 1049500.25, Validation Asset 992853.416\n",
      "Episode 6400, Reward -0.019, Average Reward -0.012, Asset 962142.03, Validation Asset 878277.930\n",
      "Episode 6500, Reward -0.104, Average Reward -0.010, Asset 872255.95, Validation Asset 1025391.58\n",
      "Episode 6600, Reward -0.149, Average Reward 0.001, Asset 835805.21, Validation Asset 911553.6355\n",
      "Episode 6700, Reward -0.028, Average Reward 0.007, Asset 951784.93, Validation Asset 977589.672\n",
      "Episode 6800, Reward 0.095, Average Reward -0.013, Asset 1073394.99, Validation Asset 888216.718\n",
      "Episode 6900, Reward 0.033, Average Reward -0.003, Asset 998304.56, Validation Asset 933616.3960\n",
      "Episode 7000, Reward 0.089, Average Reward -0.001, Asset 1069046.66, Validation Asset 931070.790\n",
      "Episode 7100, Reward 0.113, Average Reward -0.009, Asset 1081169.17, Validation Asset 942625.219\n",
      "Episode 7200, Reward 0.037, Average Reward -0.006, Asset 1001221.66, Validation Asset 958474.458\n",
      "Episode 7300, Reward -0.064, Average Reward 0.002, Asset 920840.25, Validation Asset 970439.5725\n",
      "Episode 7400, Reward -0.018, Average Reward 0.000, Asset 948174.96, Validation Asset 933539.2718\n",
      "Episode 7500, Reward 0.132, Average Reward -0.003, Asset 1124604.79, Validation Asset 958351.589\n",
      "Episode 7600, Reward -0.069, Average Reward -0.015, Asset 913545.97, Validation Asset 1003173.57\n",
      "Episode 7700, Reward 0.067, Average Reward -0.005, Asset 1041339.26, Validation Asset 994728.768\n",
      "Episode 7800, Reward -0.019, Average Reward -0.014, Asset 951967.66, Validation Asset 958261.036\n",
      "Episode 7900, Reward -0.130, Average Reward -0.008, Asset 840920.65, Validation Asset 882191.409\n",
      "Episode 8000, Reward 0.033, Average Reward -0.008, Asset 1000896.79, Validation Asset 920536.123\n",
      "Episode 8100, Reward 0.039, Average Reward -0.018, Asset 993226.88, Validation Asset 948455.0537\n",
      "Episode 8200, Reward 0.069, Average Reward -0.015, Asset 1031284.56, Validation Asset 962268.691\n",
      "Episode 8300, Reward -0.058, Average Reward -0.016, Asset 924423.38, Validation Asset 939445.714\n",
      "Episode 8400, Reward -0.036, Average Reward 0.001, Asset 942918.67, Validation Asset 897549.7330\n",
      "Episode 8500, Reward 0.047, Average Reward -0.002, Asset 1017246.05, Validation Asset 961495.98\n",
      "Episode 8600, Reward -0.014, Average Reward -0.024, Asset 952134.38, Validation Asset 947672.128\n",
      "Episode 8700, Reward 0.122, Average Reward 0.002, Asset 1101196.60, Validation Asset 994040.0560\n",
      "Episode 8800, Reward 0.032, Average Reward -0.003, Asset 1020484.38, Validation Asset 1026798.64\n",
      "Episode 8900, Reward 0.042, Average Reward -0.004, Asset 1010209.43, Validation Asset 986303.181\n",
      "Episode 9000, Reward -0.041, Average Reward -0.007, Asset 938221.29, Validation Asset 885639.766\n",
      "Episode 9100, Reward -0.015, Average Reward -0.006, Asset 954507.67, Validation Asset 995792.104\n",
      "Episode 9200, Reward 0.091, Average Reward 0.003, Asset 1081558.32, Validation Asset 878989.6402\n",
      "Episode 9300, Reward -0.103, Average Reward -0.015, Asset 864340.90, Validation Asset 895401.016\n",
      "Episode 9400, Reward -0.168, Average Reward -0.022, Asset 824431.20, Validation Asset 895812.672\n",
      "Episode 9500, Reward 0.026, Average Reward -0.010, Asset 1013791.11, Validation Asset 896735.714\n",
      "Episode 9600, Reward 0.097, Average Reward -0.011, Asset 1086890.39, Validation Asset 929772.132\n",
      "Episode 9700, Reward 0.104, Average Reward -0.004, Asset 1091308.16, Validation Asset 964451.772\n",
      "Episode 9800, Reward -0.014, Average Reward -0.009, Asset 952941.75, Validation Asset 950930.226\n",
      "Episode 9900, Reward -0.141, Average Reward -0.009, Asset 844708.90, Validation Asset 907249.459\n",
      "Episode 10000, Reward -0.057, Average Reward -0.016, Asset 927788.66, Validation Asset 899147.92\n",
      "Episode 10100, Reward 0.096, Average Reward 0.009, Asset 1079639.61, Validation Asset 937689.7876\n",
      "Episode 10200, Reward -0.041, Average Reward 0.005, Asset 939935.34, Validation Asset 946350.952\n",
      "Episode 10300, Reward -0.031, Average Reward -0.001, Asset 945973.34, Validation Asset 950941.126\n",
      "Episode 10400, Reward -0.251, Average Reward 0.012, Asset 747156.25, Validation Asset 860717.662\n",
      "Episode 10500, Reward 0.034, Average Reward -0.018, Asset 1015397.08, Validation Asset 879260.623\n",
      "Episode 10600, Reward 0.018, Average Reward -0.002, Asset 987915.86, Validation Asset 947330.0141\n",
      "Episode 10700, Reward 0.041, Average Reward -0.003, Asset 1028470.46, Validation Asset 998330.725\n",
      "Episode 10800, Reward 0.027, Average Reward -0.016, Asset 994841.66, Validation Asset 995348.7942\n",
      "Episode 10900, Reward -0.027, Average Reward 0.005, Asset 957536.60, Validation Asset 993059.7055\n",
      "Episode 11000, Reward -0.042, Average Reward -0.013, Asset 916008.31, Validation Asset 874768.289\n",
      "Episode 11100, Reward -0.002, Average Reward -0.014, Asset 985688.01, Validation Asset 971911.663\n",
      "Episode 11200, Reward 0.100, Average Reward -0.008, Asset 1069469.67, Validation Asset 939650.454\n",
      "Episode 11300, Reward -0.200, Average Reward -0.008, Asset 789403.75, Validation Asset 1017959.70\n",
      "Episode 11400, Reward 0.039, Average Reward -0.002, Asset 1027723.53, Validation Asset 997450.226\n",
      "Episode 11500, Reward -0.000, Average Reward 0.002, Asset 977494.36, Validation Asset 974132.1998\n",
      "Episode 11600, Reward -0.046, Average Reward -0.010, Asset 926582.98, Validation Asset 928807.218\n",
      "Episode 11700, Reward -0.134, Average Reward -0.013, Asset 854803.71, Validation Asset 904745.703\n",
      "Episode 11743, Reward -0.064, Average Reward -0.017, Asset 913881.51, Validation Asset 945097.023"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22072\\4124558685.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_episode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_min\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_frame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22072\\963838167.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(env, agent, num_episode, eps_init, eps_decay, eps_min, max_t, num_frame, constant)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m             \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mstate_deque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22072\\141632333.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, eps)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m                 \u001b[0maction_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQ_local\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m                 \u001b[0maction_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programing\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22072\\1744997487.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m576\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programing\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programing\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\programing\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(env, agent, num_episode, eps, eps_decay, eps_min, max_t, num_frame=1, constant=C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps_init = eps\n",
    "# constant = C\n",
    "# num_frame =1\n",
    "\n",
    "# rewards_log = []\n",
    "# average_log = []\n",
    "# eps = eps_init\n",
    "\n",
    "# for i in range(1, 1 + num_episode):\n",
    "#     episodic_reward = 0\n",
    "#     done = False\n",
    "#     frame = env.reset()\n",
    "#     state_deque = deque(maxlen=num_frame)\n",
    "#     for _ in range(num_frame):\n",
    "#         state_deque.append(frame)\n",
    "#     state = np.stack(state_deque, axis=0)\n",
    "#     state = np.expand_dims(state, axis=0)\n",
    "#     t = 0\n",
    "\n",
    "#     while not done and t < max_t:\n",
    "\n",
    "#         t += 1\n",
    "#         action = agent.act(state, eps)\n",
    "#         frame, reward, done = env.step(action)\n",
    "#         state_deque.append(frame)\n",
    "#         next_state = np.stack(state_deque, axis=0)\n",
    "#         next_state = np.expand_dims(next_state, axis=0)\n",
    "#         agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "#         if t % 5 == 0 and len(agent.memory) >= agent.bs:\n",
    "#             agent.learn()\n",
    "#             agent.soft_update(agent.tau)\n",
    "\n",
    "#         state = next_state.copy()\n",
    "#         episodic_reward += reward\n",
    "\n",
    "#     rewards_log.append(episodic_reward)\n",
    "#     average_log.append(np.mean(rewards_log[-100:]))\n",
    "#     print('\\rEpisode {}, Reward {:.3f}, Average Reward {:.3f}'.format(i, episodic_reward, average_log[-1]), end='')\n",
    "#     if i % 100 == 0:\n",
    "#         print()\n",
    "\n",
    "#     eps = max(eps * eps_decay, eps_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMaJPtlMHur6DonwEyZLw5h",
   "collapsed_sections": [],
   "name": "data process and load.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
